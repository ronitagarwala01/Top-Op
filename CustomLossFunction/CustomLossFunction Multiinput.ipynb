{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model will take a decimal number and return the binary(ones and zeros) for that number.\n",
    "</br>\n",
    "ex:\n",
    "</br>\n",
    "    [0,1,0,1] -> 10\n",
    "</br>\n",
    "    [1,0,1,0] -> 5\n",
    "</br>\n",
    "    [0,1,1,0] -> 6\n",
    "</br>\n",
    "    [0,0,0,0] -> 0\n",
    "\n",
    "The loss function will take the model input and the model output and give a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinArray(num,bitDepth):\n",
    "    trueBinArray = np.zeros(bitDepth)\n",
    "    for i in range(1,bitDepth+1):\n",
    "        trueBinArray[bitDepth-i] = num%2\n",
    "        num = num//2\n",
    "    return trueBinArray\n",
    "\n",
    "def binaryLossFunction(binaryArray_pred,number_true):\n",
    "\n",
    "    l = len(binaryArray_pred)\n",
    "    trueBinArray = getBinArray(number_true,l)\n",
    "\n",
    "    loss = 0\n",
    "    gradient = np.ones(l)\n",
    "    for i in range(l): \n",
    "        y_hat = binaryArray_pred[i]\n",
    "        y = trueBinArray[i]\n",
    "        if(y == y_hat):\n",
    "            loss += 0\n",
    "        else:\n",
    "            y_hat = np.clip(y_hat,0.01,.99)\n",
    "            loss += -(y * np.log(y_hat)) - ((1-y) * (np.log(1-y_hat)))\n",
    "            gradient[i] = (-2*y + 1)/(y_hat)\n",
    "    \n",
    "    #print(\"\\tLossRaw:\",loss)\n",
    "\n",
    "    loss = abs((1/l) * loss)\n",
    "    gradient = (1/l) * gradient\n",
    "\n",
    "    #print(\"\\tloss:\",loss)\n",
    "\n",
    "\n",
    "\n",
    "    return loss,gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29710664 0.12555679 0.03463331 0.53428973] 2\n",
      "loss: 1.1534622326937687\n",
      "Gradient: [ 0.84144871  1.99113086 -7.21848411  0.46791092]\n",
      "\n",
      "[0.         0.         1.         0.06637881] 2\n",
      "loss: 0.017171124389289208\n",
      "diff:  1.1362911083044795\n",
      "Gradient: [0.25       0.25       0.25       3.76626235]\n"
     ]
    }
   ],
   "source": [
    "bitDepth = 4\n",
    "ar = np.random.random(size = bitDepth)\n",
    "n = np.random.randint(0,2**bitDepth)\n",
    "print(ar,n)\n",
    "l1,g = binaryLossFunction(ar,n)\n",
    "\n",
    "print(\"loss:\",l1)\n",
    "print(\"Gradient:\",g)\n",
    "print()\n",
    "\n",
    "ar -= g\n",
    "ar = np.clip(ar,0,1)\n",
    "print(ar,n)\n",
    "l2,g = binaryLossFunction(ar,n)\n",
    "\n",
    "print(\"loss:\",l2)\n",
    "print(\"diff: \",l1-l2)\n",
    "print(\"Gradient:\",g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "def BuildModel(bitDepth):\n",
    "    weightInitializer = tf.keras.initializers.RandomNormal(-2,3,seed=0)\n",
    "    Input_x = tf.keras.layers.Input(shape=(1),name=\"x\")\n",
    "\n",
    "    denseLayer = tf.keras.layers.Dense(bitDepth,kernel_initializer=weightInitializer)(Input_x)\n",
    "    #denseLayer = tf.keras.layers.Dense(degree,kernel_initializer=weightInitializer)(denseLayer)\n",
    "    #denseLayer = tf.keras.layers.Dense(degree)(denseLayer)\n",
    "    #denseLayer = tf.keras.layers.Dense(degree)(denseLayer)\n",
    "    outputLayer = tf.keras.layers.Dense(bitDepth,activation='sigmoid',kernel_initializer=weightInitializer)(denseLayer)\n",
    "    \n",
    "    return tf.keras.Model(inputs= Input_x,outputs=outputLayer)\n",
    "\n",
    "class Model_m9(tf.keras.Model):\n",
    "    def __init__(self,degree):\n",
    "        super(Model_m9, self).__init__()\n",
    "        self.model = BuildModel(degree)\n",
    "\n",
    "    def call(self,data,training = False):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPoints = 10\n",
    "data_x = np.random.randint(0,2**bitDepth,size=numPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x (InputLayer)              [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 8         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28\n",
      "Trainable params: 28\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "[<tf.Variable 'dense_6/kernel:0' shape=(1, 4) dtype=float32, numpy=array([[-7.091102 , -3.4441302, -3.9867108, -1.7389665]], dtype=float32)>, <tf.Variable 'dense_6/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_7/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
      "array([[-7.091102  , -3.4441302 , -3.9867108 , -1.7389665 ],\n",
      "       [-5.000487  ,  0.83042216, -0.1499157 , -1.3593047 ],\n",
      "       [-2.4283085 , -3.6605248 ,  1.853868  ,  2.5878382 ],\n",
      "       [ 0.16673112, -1.2561197 , -2.5471163 , -6.2665596 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_7/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model = Model_m9(bitDepth)#add one for the constant\n",
    "model.compile()#, loss= CustomTensorFlowLoss())\n",
    "print(model.model.summary())\n",
    "print()\n",
    "print(model.trainable_weights)\n",
    "lossArray = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decaySteps = len(data_x)\n",
    "print(decaySteps)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=.01,decay_steps=decaySteps,decay_rate=0.8,staircase=False)\n",
    "#lr_schedule = 0.000001\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)#lr_schedule)\n",
    "optimizer = tf.keras.optimizers.SGD(1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "[[14]]\n",
      "[[1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(0,len(data_x))\n",
    "x = np.reshape(data_x[i],(1,1))\n",
    "y_pred = model.predict(x)\n",
    "print(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def loss_fn(modelIn_num,modelOut_bin):\n",
    "    #create the vectors to hold the values for loss and gradient\n",
    "\n",
    "    loss_array = np.zeros(modelIn_num.shape)\n",
    "    grad_array = np.zeros(modelOut_bin.shape)\n",
    "    for i in range(len(modelIn_num)):\n",
    "        trueNum = float(modelIn_num[i])\n",
    "        predictedBin = np.array(modelOut_bin[i])\n",
    "        lossVal, gradVal = binaryLossFunction(predictedBin,trueNum)\n",
    "\n",
    "        loss_array[i] = lossVal\n",
    "        grad_array[i] = gradVal\n",
    "    \n",
    "\n",
    "    lossTensor = tf.convert_to_tensor(loss_array,dtype='float32')\n",
    "    gradTensor = tf.convert_to_tensor(grad_array,dtype='float32')\n",
    "    \n",
    "    def grad(upstream):\n",
    "        pred_grad = (upstream * gradTensor)\n",
    "        return pred_grad,pred_grad\n",
    "    \n",
    "    #print(\"lossVal = \",lossVal)\n",
    "    return lossTensor, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ypred: tf.Tensor([[1. 1. 1. 1.]], shape=(1, 4), dtype=float32)\n",
      "\n",
      "abs loss: tf.Tensor([[1.1512926]], shape=(1, 1), dtype=float32)\n",
      "\n",
      "mseLoss: tf.Tensor([61.576153], shape=(1,), dtype=float32)\n",
      "\n",
      "Loss gradient: tf.Tensor([[0.25       0.25       0.25       0.25252524]], shape=(1, 4), dtype=float32)\n",
      "\n",
      "\n",
      "Loss gradient MSE: tf.Tensor([[0. 0. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "\n",
      "Gradients: [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[-0.,  0.,  0.,  0.]], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.,  0.,  0.,  0.], dtype=float32)>, <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
      "array([[-0., -0., -0., -0.],\n",
      "       [-0., -0., -0., -0.],\n",
      "       [-0., -0., -0., -0.],\n",
      "       [-0., -0., -0., -0.]], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "\n",
      "Gradients MSE: [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
      "array([[ -6.086383 ,  -4.7575665,   9.057434 , -21.932959 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-0.43474162, -0.33982617,  0.64695954, -1.5666399 ], dtype=float32)>, <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
      "array([[ -0.      ,  -0.      ,  -0.      , -24.818857],\n",
      "       [ -0.      ,  -0.      ,  -0.      , -12.054456],\n",
      "       [ -0.      ,  -0.      ,  -0.      , -13.953487],\n",
      "       [ -0.      ,  -0.      ,  -0.      ,  -6.086383]], dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.  , 0.  , 0.  , 0.25], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Open a GradientTape to record the operations run\n",
    "# during the forward pass, which enables auto-differentiation.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "    # Run the forward pass of the layer.\n",
    "    # The operations that the layer applies\n",
    "    # to its inputs are going to be recorded\n",
    "    # on the GradientTape.\n",
    "    i = np.random.randint(0,len(data_x))\n",
    "    x = np.reshape(data_x[i],(1,1))\n",
    "\n",
    "    y_pred = model.model(x, training=True)  # Logits for this minibatch\n",
    "    print(\"\\nypred:\",y_pred)\n",
    "\n",
    "    loss_value = loss_fn(x,y_pred)\n",
    "    print(\"\\nabs loss:\",loss_value)\n",
    "\n",
    "    y_true = np.reshape(getBinArray(x[0],bitDepth),y_pred.shape)\n",
    "    loss_value_mse = tf.keras.losses.binary_crossentropy(y_true,y_pred)\n",
    "    print(\"\\nmseLoss:\",loss_value_mse)\n",
    "\n",
    "layerGrad = tape.gradient(loss_value,y_pred,unconnected_gradients='zero')\n",
    "layerGrad2 = tape.gradient(loss_value_mse,y_pred,unconnected_gradients='zero')\n",
    "\n",
    "print(\"\\nLoss gradient:\",layerGrad)\n",
    "print()\n",
    "print(\"\\nLoss gradient MSE:\",layerGrad2)\n",
    "\n",
    "# Use the gradient tape to automatically retrieve\n",
    "# the gradients of the trainable variables with respect to the loss.\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)#,output_gradients=loss_gradient,unconnected_gradients='zero')\n",
    "print(\"\\nGradients:\",grads)\n",
    "\n",
    "grads = tape.gradient(loss_value_mse, model.trainable_weights)#,output_gradients=loss_gradient,unconnected_gradients='zero')\n",
    "print(\"\\nGradients MSE:\",grads)\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorflowLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(CustomTensorflowLoss,self).__init__()\n",
    "    \n",
    "    def call(self,y_true,y_pred):\n",
    "        return loss_fn(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 500 \n",
    "batchSize = numPoints//epochs\n",
    "#loss_fn = CustomTensorFlowLoss()\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    batchLoss = []\n",
    "    batchGradients = []\n",
    "    for batchNum in range(batchSize):\n",
    "        dataPoint_x = np.reshape(x_forModel[epoch*batchSize + batchNum,:],(1,PolyDegree+1))\n",
    "        dataPoint_y = y_forModel[epoch*batchSize + batchNum]\n",
    "\n",
    "        #print(dataPoint_x)\n",
    "        #print(dataPoint_y)\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            y_pred = model.model(dataPoint_x, training=True)  # Logits for this minibatch\n",
    "            \n",
    "            #with tape.stop_recording(): \n",
    "            #print(\"\\nypred:\",y_pred)\n",
    "            loss_value = loss_fn(dataPoint_y,y_pred)\n",
    "            #loss_value = custom_operation(y_forModel,y_pred)\n",
    "            #loss_value = foo_custom_grad(y_pred)\n",
    "            #print(\"\\nabs loss:\",loss_value)\n",
    "            #loss_value = tf.keras.losses.MSE(y_forModel,y_pred)\n",
    "            batchLoss.append(float(np.mean(loss_value)))\n",
    "                \n",
    "            #print(\"\\nmseLoss:\",loss_value)\n",
    "            #print(loss_gradient)\n",
    "\n",
    "        #layerGrad = tape.gradient(loss_value,y_pred,unconnected_gradients='zero')\n",
    "        #layerGrad_var = tf.Variable(layerGrad)\n",
    "        #layerGrad_var.assign(loss_gradient)\n",
    "        #layerGrad[0] = loss_gradient\n",
    "        #print(layerGrad_var)\n",
    "        #print(layerGrad)\n",
    "        \n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        #grads = tape.gradient(loss_value, model.trainable_weights)#,output_gradients=loss_gradient,unconnected_gradients='zero')\n",
    "        if(batchNum == 0):\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights,unconnected_gradients='zero')\n",
    "        else:\n",
    "            grads += tape.gradient(loss_value, model.trainable_weights,unconnected_gradients='zero')\n",
    "        #print()\n",
    "        #print(\"\\nGradients:\",grads)\n",
    "        #print()\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        \n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "    #print(model.trainable_weights)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    lossArray.append(float(np.mean(batchLoss)))\n",
    "    print(\n",
    "        \"Training loss at epoch {}: {:.4f}\".format(epoch, lossArray[-1]))  \n",
    "    \n",
    "    #print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-8),loss=tf.keras.losses.MSE)\n",
    "hist = model.fit(x_forModel,y_forModel,\n",
    "                 epochs=1,\n",
    "                 #validation_split=.1,\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(hist):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_title(\"\")\n",
    "    keys = hist.history.keys()\n",
    "    y = np.arange(1,hist.params['epochs']+1)\n",
    "    for key in keys:\n",
    "        if('output' in key):\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            if(minVal != maxVal):\n",
    "                print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "                #ax.plot(y,hist.history[key],linewidth=0.5,label=key)\n",
    "        else:\n",
    "            ax.plot(y,hist.history[key],label=key)\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(lossArray)),lossArray)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.plot(np.arange(len(lossOfAllTrainings)),lossOfAllTrainings)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizeOutputs\n",
    "\n",
    "y_pred = model.predict(np.polynomial.polynomial.polyvander(x,PolyDegree))\n",
    "print(y_pred.shape)\n",
    "plt.plot(x,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91d6363c0adb958ed116842d9c2fc7faebb1fa3beaff0888078e0808098095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
