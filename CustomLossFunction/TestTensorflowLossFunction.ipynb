{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absLoss(true:float,pred:float):\n",
    "    #print(true)\n",
    "    #print(pred)\n",
    "    #set up a loss function beased on absolute value, gradient is sign of pred-true\n",
    "    #trueProto = tf.make_tensor_proto(true)\n",
    "    #predProto = tf.make_tensor_proto(pred)\n",
    "    #trueValues = tf.make_ndarray(trueProto)\n",
    "    #predictedValues =  tf.make_ndarray(predProto)\n",
    "    #print(predictedValues)\n",
    "    #print(trueValues)\n",
    "\n",
    "    #predictedValues = np.reshape(predictedValues,trueValues.shape)\n",
    "    #trueValues = np.reshape(trueValues,predictedValues.shape)\n",
    "\n",
    "    loss = pred-true\n",
    "    #print(loss)\n",
    "    #print()\n",
    "    \n",
    "    #gradient = np.zeros(loss.shape)\n",
    "    gradient = 2*loss\n",
    "    loss = loss**2\n",
    "\n",
    "    #loss_tensor = tf.convert_to_tensor(loss,dtype='float32')\n",
    "    #gradient_tensor = tf.convert_to_tensor(gradient,dtype='float32')\n",
    "\n",
    "    return loss,gradient\n",
    "\n",
    "size = 1\n",
    "v1 = tf.convert_to_tensor(np.random.normal(0,1,size=(size,1)))\n",
    "v2 = tf.convert_to_tensor(np.random.normal(0,1,size=(size)))\n",
    "\n",
    "print(v1)\n",
    "print()\n",
    "print(v2)\n",
    "print()\n",
    "l,g = absLoss(v1,v2)\n",
    "l2 = tf.keras.losses.MSE(v1,v2)\n",
    "print(l)\n",
    "print(l2)\n",
    "print()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(-10,10,50)\n",
    "y_array = []\n",
    "grad_array = []\n",
    "for num in x1:\n",
    "    loss,grad = absLoss(0,num)\n",
    "    y_array.append(loss)\n",
    "    grad_array.append(grad)\n",
    "\n",
    "\n",
    "plt.plot(x1,y_array)\n",
    "plt.plot(x1,grad_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "def BuildModel(degree):\n",
    "    weightInitializer = tf.keras.initializers.RandomNormal(-2,3)\n",
    "    Input_x = tf.keras.layers.Input(shape=(degree),name=\"x\")\n",
    "\n",
    "    #denseLayer = tf.keras.layers.Dense(degree,kernel_initializer=weightInitializer)(Input_x)\n",
    "    #denseLayer = tf.keras.layers.Dense(degree,kernel_initializer=weightInitializer)(denseLayer)\n",
    "    #denseLayer = tf.keras.layers.Dense(degree)(denseLayer)\n",
    "    #denseLayer = tf.keras.layers.Dense(degree)(denseLayer)\n",
    "    outputLayer = tf.keras.layers.Dense(1,kernel_initializer=weightInitializer)(Input_x)\n",
    "    \n",
    "    return tf.keras.Model(inputs= Input_x,outputs=outputLayer)\n",
    "\n",
    "class Model_m9(tf.keras.Model):\n",
    "    def __init__(self,degree):\n",
    "        super(Model_m9, self).__init__()\n",
    "        self.model = BuildModel(degree)\n",
    "\n",
    "    def call(self,data,training = False):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for trainin on\n",
    "# f(x) = sin(x) * e**x\n",
    "\n",
    "def f(x,noise:float=0):\n",
    "    return np.add(np.add(np.sin(1.5*np.pi*x-0.5), np.exp(x)), np.random.normal(0,noise,size=x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minVal = -2\n",
    "maxVal = 2\n",
    "\n",
    "\n",
    "x = np.linspace(minVal,maxVal,num=1000)\n",
    "y1 = f(x,.1)\n",
    "y2 = f(x)\n",
    "\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolyDegree = 1\n",
    "numPoints = 10\n",
    "x_forModel = np.polynomial.polynomial.polyvander(np.random.uniform(minVal,maxVal,size=numPoints),PolyDegree)\n",
    "y_forModel = f(x_forModel[:,1],noise=.1)\n",
    "print(x_forModel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_m9(PolyDegree+1)#add one for the constant\n",
    "model.compile()#, loss= CustomTensorFlowLoss())\n",
    "print(model.model.summary())\n",
    "print()\n",
    "print(model.trainable_weights)\n",
    "lossArray = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(minVal,maxVal,num=1000)\n",
    "y_pred = model.predict(np.polynomial.polynomial.polyvander(x,PolyDegree))\n",
    "print(y_pred.shape)\n",
    "plt.plot(x,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decaySteps = len(y_forModel)\n",
    "print(decaySteps)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=.01,decay_steps=decaySteps,decay_rate=0.8,staircase=False)\n",
    "#lr_schedule = 0.000001\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)#lr_schedule)\n",
    "optimizer = tf.keras.optimizers.SGD(1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ep = np.arange(decaySteps,dtype='int32')\n",
    "lrs = []\n",
    "for i in ep:\n",
    "    lrs.append(lr_schedule(i))\n",
    "\n",
    "# the learning rate schedule\n",
    "plt.plot(ep, lrs)\n",
    "#plt.yscale('log')\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_forModel[0])\n",
    "print(f(x_forModel[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.model(np.reshape(x_forModel[0,:],(1,PolyDegree+1)), training=True)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def loss_fn(y_true,y_pred):\n",
    "    #create the vectors to hold the values for loss and gradient\n",
    "    #print(\"input1:{}\\n\\nInput2:{}\".format(y_true,y_pred))\n",
    "    loss_array = np.zeros(y_pred.shape)\n",
    "    grad_array = np.zeros(y_pred.shape)\n",
    "    mean_normalizer = 1/len(y_true)\n",
    "    for i in range(len(y_true)):\n",
    "        t = float(y_true[i])\n",
    "        p = float(y_pred[i])\n",
    "        lossVal, gradVal = absLoss(t,p)\n",
    "        #print(\"\\tloss {} ({},{}) has val: {} and gradient {}\".format(i,t,p,lossVal,gradVal))\n",
    "        loss_array[i] = lossVal#*mean_normalizer\n",
    "        grad_array[i] = gradVal#*mean_normalizer\n",
    "    \n",
    "    #print(\"Converting to tensor\")\n",
    "    lossTensor = tf.convert_to_tensor(loss_array,dtype='float32')\n",
    "    #lossTensor = tf.squeeze(lossTensor)\n",
    "    gradTensor = tf.convert_to_tensor(grad_array,dtype='float32')\n",
    "    #gradTensor = tf.squeeze(gradTensor)\n",
    "    #print(\"loss: {}\\nGrad: {}\".format(lossTensor,gradTensor))\n",
    "    #print(\"lossval and grad=\",lossVal,gradVal)\n",
    "    def grad(upstream):\n",
    "        #print(\"\\tGradient parameters:\\n\",upstream)\n",
    "        pred_grad = (upstream * gradTensor)\n",
    "        #var_grad = upstream * tf.constant(1)\n",
    "        #print(\"\\tPredGrad = \",pred_grad)\n",
    "        return pred_grad,pred_grad\n",
    "    \n",
    "    #print(\"lossVal = \",lossVal)\n",
    "    return lossTensor, grad\n",
    "\n",
    "\n",
    "@tf.custom_gradient\n",
    "def foo_custom_grad(x):\n",
    "    y = np.square(x)\n",
    "    def grad_fn(dy):\n",
    "        grad = 2 * np.array(x)\n",
    "        return grad * dy\n",
    "    return y, grad_fn\n",
    "\n",
    "@tf.custom_gradient\n",
    "def custom_operation(x, y):\n",
    "    print(\"custom operation:\",x,y)\n",
    "    # define the gradient\n",
    "    def grad(g):\n",
    "        \n",
    "        print(\"\\tcustomGrad:\",g)\n",
    "        return g, g\n",
    "    \n",
    "    forward_pass = x * y\n",
    "    print(\"Forward pass:\",forward_pass)\n",
    "\n",
    "    return forward_pass, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "dataPointNumber = 0\n",
    "dataPoint_x = x_forModel#np.reshape(x_forModel[dataPointNumber,:],(1,PolyDegree+1))\n",
    "dataPoint_y = y_forModel#[dataPointNumber]\n",
    "\n",
    "print(dataPoint_x)\n",
    "print(dataPoint_y)\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "\n",
    "# Open a GradientTape to record the operations run\n",
    "# during the forward pass, which enables auto-differentiation.\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "    # Run the forward pass of the layer.\n",
    "    # The operations that the layer applies\n",
    "    # to its inputs are going to be recorded\n",
    "    # on the GradientTape.\n",
    "    y_pred = model.model(dataPoint_x, training=True)  # Logits for this minibatch\n",
    "    \n",
    "    #with tape.stop_recording(): \n",
    "    print(\"\\nypred:\",y_pred)\n",
    "    loss_value = loss_fn(dataPoint_y,y_pred)\n",
    "    #loss_value = custom_operation(y_forModel,y_pred)\n",
    "    #loss_value = foo_custom_grad(y_pred)\n",
    "    print(\"\\nabs loss:\",loss_value)\n",
    "    loss_value_mse = tf.keras.losses.MSE(y_forModel,y_pred)\n",
    "        \n",
    "    print(\"\\nmseLoss:\",loss_value_mse)\n",
    "    #print(loss_gradient)\n",
    "\n",
    "layerGrad = tape.gradient(loss_value,y_pred,unconnected_gradients='zero')\n",
    "layerGrad2 = tape.gradient(loss_value_mse,y_pred,unconnected_gradients='zero')\n",
    "#layerGrad_var = tf.Variable(layerGrad)\n",
    "#layerGrad_var.assign(loss_gradient)\n",
    "#layerGrad[0] = loss_gradient\n",
    "#print(layerGrad_var)\n",
    "print(\"Loss gradient:\",layerGrad)\n",
    "print()\n",
    "print(\"Loss gradient MSE:\",layerGrad2)\n",
    "\n",
    "# Use the gradient tape to automatically retrieve\n",
    "# the gradients of the trainable variables with respect to the loss.\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)#,output_gradients=loss_gradient,unconnected_gradients='zero')\n",
    "\n",
    "#grads = tape.gradient(loss_value, model.trainable_weights,unconnected_gradients='zero')\n",
    "\n",
    "#print()\n",
    "print(\"\\nGradients:\",grads)\n",
    "\n",
    "grads = tape.gradient(loss_value_mse, model.trainable_weights)#,output_gradients=loss_gradient,unconnected_gradients='zero')\n",
    "print(\"\\nGradients MSE:\",grads)\n",
    "#print()\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorflowLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(CustomTensorflowLoss,self).__init__()\n",
    "    \n",
    "    def call(self,y_true,y_pred):\n",
    "        return loss_fn(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 500 \n",
    "batchSize = numPoints//epochs\n",
    "#loss_fn = CustomTensorFlowLoss()\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    batchLoss = []\n",
    "    batchGradients = []\n",
    "    for batchNum in range(batchSize):\n",
    "        dataPoint_x = np.reshape(x_forModel[epoch*batchSize + batchNum,:],(1,PolyDegree+1))\n",
    "        dataPoint_y = y_forModel[epoch*batchSize + batchNum]\n",
    "\n",
    "        #print(dataPoint_x)\n",
    "        #print(dataPoint_y)\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            y_pred = model.model(dataPoint_x, training=True)  # Logits for this minibatch\n",
    "            \n",
    "            #with tape.stop_recording(): \n",
    "            #print(\"\\nypred:\",y_pred)\n",
    "            loss_value = loss_fn(dataPoint_y,y_pred)\n",
    "            #loss_value = custom_operation(y_forModel,y_pred)\n",
    "            #loss_value = foo_custom_grad(y_pred)\n",
    "            #print(\"\\nabs loss:\",loss_value)\n",
    "            #loss_value = tf.keras.losses.MSE(y_forModel,y_pred)\n",
    "            batchLoss.append(float(np.mean(loss_value)))\n",
    "                \n",
    "            #print(\"\\nmseLoss:\",loss_value)\n",
    "            #print(loss_gradient)\n",
    "\n",
    "        #layerGrad = tape.gradient(loss_value,y_pred,unconnected_gradients='zero')\n",
    "        #layerGrad_var = tf.Variable(layerGrad)\n",
    "        #layerGrad_var.assign(loss_gradient)\n",
    "        #layerGrad[0] = loss_gradient\n",
    "        #print(layerGrad_var)\n",
    "        #print(layerGrad)\n",
    "        \n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        #grads = tape.gradient(loss_value, model.trainable_weights)#,output_gradients=loss_gradient,unconnected_gradients='zero')\n",
    "        if(batchNum == 0):\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights,unconnected_gradients='zero')\n",
    "        else:\n",
    "            grads += tape.gradient(loss_value, model.trainable_weights,unconnected_gradients='zero')\n",
    "        #print()\n",
    "        #print(\"\\nGradients:\",grads)\n",
    "        #print()\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        \n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "    #print(model.trainable_weights)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    lossArray.append(float(np.mean(batchLoss)))\n",
    "    print(\n",
    "        \"Training loss at epoch {}: {:.4f}\".format(epoch, lossArray[-1]))  \n",
    "    \n",
    "    #print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-8),loss=tf.keras.losses.MSE)\n",
    "hist = model.fit(x_forModel,y_forModel,\n",
    "                 epochs=1,\n",
    "                 #validation_split=.1,\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(hist):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_title(\"\")\n",
    "    keys = hist.history.keys()\n",
    "    y = np.arange(1,hist.params['epochs']+1)\n",
    "    for key in keys:\n",
    "        if('output' in key):\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            if(minVal != maxVal):\n",
    "                print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "                #ax.plot(y,hist.history[key],linewidth=0.5,label=key)\n",
    "        else:\n",
    "            ax.plot(y,hist.history[key],label=key)\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(lossArray)),lossArray)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.plot(np.arange(len(lossOfAllTrainings)),lossOfAllTrainings)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizeOutputs\n",
    "\n",
    "y_pred = model.predict(np.polynomial.polynomial.polyvander(x,PolyDegree))\n",
    "print(y_pred.shape)\n",
    "plt.plot(x,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91d6363c0adb958ed116842d9c2fc7faebb1fa3beaff0888078e0808098095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
