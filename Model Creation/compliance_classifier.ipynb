{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These sets of models all calcuate the compliacne of a part.\n",
    "Uses:\n",
    "    - The part data, a 2D binary array representing where mass is present.\n",
    "    - The Force vectors, 2 2D arrays representing the forces baing applied to the part \n",
    "        - One array holds the forces in the x direction\n",
    "        - The other array holds the forces in the y direction\n",
    "    - The supports, The circle where the part is anchored and doesn't move\n",
    "    - The maxCompliance. \n",
    "        - Used only by the classifier model(discontinued)\n",
    "Returns:\n",
    "    - The classifier would return a boolean value if the compliance of the element was witin the max compliance constraint\n",
    "    - The reggressor model would retunr the log(natural) of the compliance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data management libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for gathering data\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below gets the data path loaded into memmory.\n",
    "It then counts the number of avalible data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH= os.path.join(os.path.dirname(os.getcwd()),r\"DataGathering\\Data\\30_30\")\n",
    "print(DATA_FILE_PATH)\n",
    "\n",
    "dir_list = os.listdir(DATA_FILE_PATH)\n",
    "max_data_points = len(dir_list)\n",
    "print(\"Number of data points: {}\".format(len(dir_list)))\n",
    "print(dir_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAgent(path):\n",
    "    \"\"\"\n",
    "    Returns the uncompressed numpy arrays inside a file pointed to by path\n",
    "    The path must point to a file FOLDER the has one or more npz files in it.\n",
    "\n",
    "    This will only properly return values for npz files saved with the following data(see the data creation algorithms).\n",
    "        - xPhys\n",
    "        - forces\n",
    "        - degreesOfFreedom\n",
    "        - formating_array\n",
    "    \"\"\"\n",
    "    filesToPull = os.listdir(path)\n",
    "\n",
    "    dataPoints = []\n",
    "\n",
    "    for fileName in filesToPull:\n",
    "        try:\n",
    "\n",
    "            data = np.load(os.path.join(path,fileName),allow_pickle=True)\n",
    "\n",
    "            xPhys = data['a']\n",
    "            forces = data['b']\n",
    "            degreesOfFreedom = data['c']\n",
    "            formating_array = data['d']\n",
    "\n",
    "            maxDof = forces.shape[0]\n",
    "            toPad = maxDof - len(degreesOfFreedom)\n",
    "            degreesOfFreedom = np.pad(degreesOfFreedom,[0,toPad])\n",
    "\n",
    "            compliance = formating_array[0]\n",
    "            compliance_max = formating_array[1]\n",
    "            shape = formating_array[2:]\n",
    "\n",
    "            #forces = np.reshape(forces,(shape[0],shape[1],4))\n",
    "\n",
    "            dataPoints.append([xPhys,forces,degreesOfFreedom,compliance_max,shape,compliance])\n",
    "\n",
    "        except:\n",
    "            print(\"Error in reading file couldn't get file: {}\".format(fileName))\n",
    "    \n",
    "    return dataPoints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some basic information about the shapes of the data\n",
    "d1 = getAgent(os.path.join(DATA_FILE_PATH,dir_list[0]))\n",
    "\n",
    "print(\"xPhys.shape:\",d1[0][0].shape)\n",
    "print(\"forces.shape:\",d1[0][1].shape)\n",
    "print(\"dofs.shape:\",d1[0][2].shape)\n",
    "print(\"c-Max:\",d1[0][3])\n",
    "print(\"compliance:\",d1[0][5])\n",
    "print(\"shape:\",d1[0][4])\n",
    "\n",
    "shape_of_Inputs = d1[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatData(xPhys,forces,degreesOfFreedom,compliance_max,shape):\n",
    "    \"\"\"\n",
    "    Given the data from the compressed NPZ file return an array of the data formatted as a series of 2D arrays(images) all of the same size\n",
    "    \"\"\"\n",
    "    #since xphys is only nelx by nely get the size difference between it and thepropper shape (should be 1)\n",
    "    finalShape = (int(shape[0]+1),int(shape[1]+1))\n",
    "    xPhysShape = xPhys.shape\n",
    "    difference = (int(finalShape[0]-xPhysShape[0]),int(finalShape[1]-xPhysShape[1]))\n",
    "    # pad xPhys to be the porpper size of the rest of the images\n",
    "    xPhys2 = np.pad(xPhys,[[0,difference[0]],[0,difference[1]]])\n",
    "\n",
    "    # the 2D forces are 2 arrays of size 2*(nelx+1)*(nely+1)\n",
    "    # The two arrays represent the two individual forces being applied to the part\n",
    "    # sumt the forces along the second axis (not 0) in order to get the two sets of forces on a single array.\n",
    "    forces2 = forces.sum(1)\n",
    "\n",
    "    #reshape the forces array into 2 arrays of propper size (one for x, one for y)\n",
    "    forces2 = np.reshape(forces2,(finalShape[0],finalShape[1],2))\n",
    "\n",
    "    # degrees of freedom is a list of indexes represening all the elements of the part that can move(have displacement thus be compliant)\n",
    "    # the following code creates and array where all indexes where the part can move are 0 and all anchored elements(supports) are 1\n",
    "    d2 = np.ones(2*finalShape[0]*finalShape[1])\n",
    "    for index in degreesOfFreedom:\n",
    "        d2[index] = 0\n",
    "\n",
    "    # since this follows the same rule as the forces with respect to the x and y direction it must be two arrays\n",
    "    d3 = np.reshape(d2,(finalShape[0],finalShape[1],2))\n",
    "    # since it will be anchored fully(in both x and y directions) we will sum it to compress the two arrays into one\n",
    "    degreesOfFreedom2 = d3.sum(2)\n",
    "\n",
    "    #a 2D array of the compliance as an image\n",
    "    c_max = compliance_max * np.ones((finalShape[0],finalShape[1]))\n",
    "\n",
    "    #previously, each of the inputs were stacked together into a (nelx+1) by (nely+1) by 5 matrix that would be passed to a convolution\n",
    "    # now each entry is returned in a list to allow easy seperation\n",
    "    arraysToStack = [xPhys2,forces2[:,:,0],forces2[:,:,1],degreesOfFreedom2]\n",
    "    fullPart = np.stack(arraysToStack,axis=-1).astype('float32')\n",
    "\n",
    "    return [xPhys2,forces2,degreesOfFreedom2,compliance_max]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the shape of the formatted data\n",
    "for xPhys,forces,degreesOfFreedom,compliance_max,shape,compliance in getAgent(os.path.join(DATA_FILE_PATH,dir_list[0])):\n",
    "    data = formatData(xPhys,forces,degreesOfFreedom,compliance_max,shape)\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the data is brought from each of the files and loaded into memory.\n",
    "\n",
    "You can define you own number of points to use as an integer > 0. This will then be limited to the max allowable number of points defeined above.\n",
    "\n",
    "Data is formated as follows\n",
    "- Data_x is a list holding the formated data(see above)\n",
    "- Data_y is a numpy array holding the coresponding compliance values for each part\n",
    "    - Since compliance can range from +0 to +inf we take the natural log of it to allow the model to reuturn a smaller number from -inf to +inf.\n",
    "\n",
    "\n",
    "More data is needed to train more accurate models.\n",
    "This data could be gathered by rotating/fliping the formated data. Care must be taken to ensure all points still line up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the data\n",
    "points_to_Use = 100000\n",
    "\"\"\"\n",
    "1000    = 11 seconds\n",
    "10000   = 4 minutes\n",
    "\"\"\"\n",
    "points_to_Use = min(points_to_Use,max_data_points)\n",
    "\n",
    "# can use an np.arrange and np.random.shuffle to get random data points every time\n",
    "pointsShuffeled = np.arange(max_data_points).astype(\"int32\")\n",
    "np.random.shuffle(pointsShuffeled)\n",
    "\n",
    "badData = 0\n",
    "\n",
    "\n",
    "data_x = []\n",
    "data_y = []\n",
    "for i in range(points_to_Use):\n",
    "    data1 = getAgent(os.path.join(DATA_FILE_PATH,dir_list[pointsShuffeled[i]]))\n",
    "    for xPhys,forces,degreesOfFreedom,compliance_max,shape,compliance in data1: \n",
    "        x = formatData(xPhys,forces,degreesOfFreedom,compliance_max,shape)\n",
    "        data_x.append(x)\n",
    "        y_val = np.log(compliance).astype('float32')\n",
    "        data_y.append(y_val)\n",
    "        if(y_val <= 0):\n",
    "            #print(i,compliance)\n",
    "            badData += 1\n",
    "            data_x.pop()\n",
    "            data_y.pop()\n",
    "\n",
    "#data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "\n",
    "print(len(data_x))\n",
    "print(len(data_y))\n",
    "print(\"Bad data:\",badData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some statistics on the classification data(deprecieated)\n",
    "numTrue = 0\n",
    "numFalse = 0\n",
    "\n",
    "numTrue = int(data_y.sum())\n",
    "numFalse = len(data_y) - numTrue\n",
    "\n",
    "print(numTrue)\n",
    "print(numFalse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some statistics on the regression data\n",
    "\n",
    "yMax = np.max(data_y)\n",
    "yMin = np.min(data_y)\n",
    "\n",
    "print(yMax)\n",
    "print(yMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform a test train split\n",
    "X_train, X_test, Y_train, Y_test  = train_test_split(data_x,data_y, test_size=0.25, random_state=42)\n",
    "print(\"X_train: {}\\nY_train: {}\".format(len(X_train), len(Y_train)))\n",
    "print(\"X_test: {}\\nY_test: {}\".format(len(X_test), len(Y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code seperates the test train split data into groups of the seperate part images\n",
    "\n",
    "A seperate array is formed for each of the images\n",
    "A for loop extracts the images and puts them into seperate arrays\n",
    "The arrays are kept in the order that is defined by the test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_part = []\n",
    "X_train_compliance = []\n",
    "X_train_forces=[]\n",
    "X_train_supports = []\n",
    "\n",
    "X_test_part = []\n",
    "X_test_compliance = []\n",
    "X_test_forces=[]\n",
    "X_test_supports = []\n",
    "\n",
    "for x,f,dof,compliance_max in X_train:\n",
    "    X_train_part.append(x)\n",
    "    X_train_forces.append(f)\n",
    "    X_train_supports.append(dof)\n",
    "    X_train_compliance.append(compliance_max)\n",
    "\n",
    "for x,f,dof,compliance_max in X_test:\n",
    "    X_test_part.append(x)\n",
    "    X_test_forces.append(f)\n",
    "    X_test_supports.append(dof)\n",
    "    X_test_compliance.append(compliance_max)\n",
    "\n",
    "X_train_part = np.array(X_train_part)\n",
    "X_train_forces = np.array(X_train_forces)\n",
    "X_train_supports = np.array(X_train_supports)\n",
    "X_train_compliance = np.array(X_train_compliance)\n",
    "\n",
    "X_test_part = np.array(X_test_part)\n",
    "X_test_forces = np.array(X_test_forces)\n",
    "X_test_supports = np.array(X_test_supports)\n",
    "X_test_compliance = np.array(X_test_compliance)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the model constants as well as the path where the model will be saved.\n",
    "Once the model completes an epoch it will be saved to the defined path\n",
    "\n",
    "The input shape will be the shape of the stacked part images(unused in later iteration of the model)\n",
    "activation is defined as 'relu' since it is easy and fast\n",
    "the uniform initalizer is unused after little progress was show when using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToSavePath = os.path.join(os.path.dirname(os.getcwd()),r\"Model Creation\\models\\complianceRegressor4\")\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=str(ToSavePath),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "InputShape = (31,31,4)\n",
    "activation = 'relu'\n",
    "uniformRandomInitalizer = tf.random_uniform_initializer(minval=-0.1, maxval=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model will have a generic input of a 31 by 31 image with 4 channels representing the density grid, the forces in both the x and y direction, and the supports. A 5th channel, holding a matrix of compliance max values, will be stacked on the layers as they pass through each convolution. The same value will be concatenated in each dense layer once we flatten it.\n",
    "\n",
    "The reutnr of the models is a boolean value retpresenting if the given part is within compliance.\n",
    "\n",
    "The actual inputs differ by model some take the stacked part images, some take the part images seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m1_classification():\n",
    "    partInput = keras.Input(shape=InputShape,name=\"partImage\")\n",
    "    complianceMaxInput = keras.Input(shape=(1),name=\"complianceMax\")\n",
    "\n",
    "\n",
    "    x1 = layers.Conv2D(filters= 16,kernel_size=(3,3))(partInput)\n",
    "    cScaling = layers.Reshape((31,31,1))(layers.RepeatVector(31*31)(complianceMaxInput))\n",
    "\n",
    "    x1 = layers.Concatenate()([x1,cScaling])\n",
    "    x1 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x1 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2,2))(x1)\n",
    "\n",
    "    x2 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x3 = layers.MaxPooling2D(pool_size=(2,2))(x2)\n",
    "\n",
    "    x_flat = layers.Flatten()(x3)\n",
    "    x4 = layers.concatenate([x_flat,complianceMaxInput])\n",
    "    x4 = layers.Dense(31,activation=activation)(x4)\n",
    "\n",
    "    x4 = layers.concatenate([x4,complianceMaxInput])\n",
    "    outputs = layers.Dense(1)(x4)\n",
    "    return keras.Model(inputs= [partInput,complianceMaxInput],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2_classification():\n",
    "    partInput = keras.Input(shape=InputShape,name=\"partImage\")\n",
    "    complianceMaxInput = keras.Input(shape=(1),name=\"complianceMax\")\n",
    "\n",
    "\n",
    "    x1 = layers.Conv2D(filters= 16,kernel_size=(3,3))(partInput)\n",
    "    cScaling = layers.Reshape((31,31,1))(layers.RepeatVector(31*31)(complianceMaxInput))\n",
    "\n",
    "    x1 = layers.Concatenate()([x1,cScaling])\n",
    "    x1 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x1 = layers.Concatenate()([x1,cScaling])\n",
    "    x1 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "\n",
    "    newShape = (x1.shape[1],x1.shape[2])\n",
    "    #print(x1.shape)\n",
    "    #cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    x2 = layers.Concatenate()([x1,cScaling])\n",
    "    x2 = layers.Conv2D(filters= 25, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2,2))(x2)\n",
    "\n",
    "    x_flat = layers.Flatten()(x2)\n",
    "    x3 = layers.concatenate([x_flat,complianceMaxInput])\n",
    "    x3 = layers.Dense(31,activation=activation)(x3)\n",
    "\n",
    "    x4 = layers.concatenate([x3,complianceMaxInput])\n",
    "    outputs = layers.Dense(1)(x4)\n",
    "    return keras.Model(inputs= [partInput,complianceMaxInput],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m3_classification():\n",
    "    partInput = keras.Input(shape=(31,31,1),name=\"partImage\")\n",
    "    forceInput = keras.Input(shape=(31,31,2),name=\"Forces\")\n",
    "    supportInput = keras.Input(shape=(31,31,1),name=\"Supports\")\n",
    "    complianceMaxInput = keras.Input(shape=(1),name=\"complianceMax\")\n",
    "\n",
    "\n",
    "    partConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(partInput)\n",
    "    forceConv = layers.Conv2D(filters= 10,kernel_size=(3,3),padding='same',activation='tanh')(forceInput)\n",
    "    supportConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(supportInput)\n",
    "    cScaling = layers.Reshape((supportConv.shape[1],supportConv.shape[2],1))(layers.RepeatVector(supportConv.shape[1]*supportConv.shape[2])(complianceMaxInput))\n",
    "\n",
    "\n",
    "    concatenatedConvolution = layers.Concatenate()([partConv,forceConv,supportConv,cScaling])\n",
    "    x1 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(concatenatedConvolution)\n",
    "    x1 = layers.Dropout(0.1)(x1)\n",
    "    x1 = layers.Concatenate()([x1,cScaling])\n",
    "    x1 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x1 = layers.Dropout(0.2)(x1)\n",
    "    x1 = layers.MaxPool2D(pool_size=(2,2),strides=(2,2),padding='same')(x1)\n",
    "\n",
    "    newShape = (x1.shape[1],x1.shape[2])\n",
    "    #print(x1.shape)\n",
    "    cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    #partScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(concatenatedConvolution)\n",
    "    x2 = layers.Concatenate()([x1,cScaling])\n",
    "    x2 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(x2)\n",
    "\n",
    "    newShape = (x2.shape[1],x2.shape[2])\n",
    "    #print(x1.shape)\n",
    "    cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    x3 = layers.Concatenate()([x2,cScaling])\n",
    "    x3 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(x3)\n",
    "    x3 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(x3)\n",
    "    x3 = layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(x3)\n",
    "\n",
    "    x_flat = layers.Flatten()(x3)\n",
    "    #x3 = layers.concatenate([x_flat,complianceMaxInput])\n",
    "    x4 = layers.Dense(32,activation=activation)(x_flat)\n",
    "    #x3 = layers.Dropout(0.2)(x3)\n",
    "\n",
    "    #x4 = layers.concatenate([x3,complianceMaxInput])\n",
    "    #outputs = layers.GlobalAveragePooling1D()(x4)\n",
    "    outputs = layers.Dense(1)(x4)\n",
    "    return keras.Model(inputs= [partInput,forceInput,supportInput,complianceMaxInput],outputs=outputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When little progress was made with classification model I moved to regression.\n",
    "\n",
    "The regession models take the sperate part images as seperate inputs and return the natural log of the compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m4_regression():\n",
    "    partInput = keras.Input(shape=(31,31,1),name=\"partImage\")\n",
    "    forceInput = keras.Input(shape=(31,31,2),name=\"Forces\")\n",
    "    supportInput = keras.Input(shape=(31,31,1),name=\"Supports\")\n",
    "    #complianceMaxInput = keras.Input(shape=(1),name=\"complianceMax\")\n",
    "\n",
    "\n",
    "    partConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(partInput)\n",
    "    forceConv = layers.Conv2D(filters= 10,kernel_size=(3,3),padding='same',activation='tanh')(forceInput)\n",
    "    supportConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(supportInput)\n",
    "    #cScaling = layers.Reshape((supportConv.shape[1],supportConv.shape[2],1))(layers.RepeatVector(supportConv.shape[1]*supportConv.shape[2])(complianceMaxInput))\n",
    "\n",
    "\n",
    "    concatenatedConvolution = layers.Concatenate()([partConv,forceConv,supportConv])\n",
    "    x1 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(concatenatedConvolution)\n",
    "    x1 = layers.Dropout(0.1)(x1)\n",
    "    #x1 = layers.Concatenate()([x1,cScaling])\n",
    "    x1 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x1 = layers.Dropout(0.2)(x1)\n",
    "    x1 = layers.MaxPool2D(pool_size=(2,2),strides=(2,2),padding='same')(x1)\n",
    "\n",
    "    newShape = (x1.shape[1],x1.shape[2])\n",
    "    #print(x1.shape)\n",
    "    #cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    partScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(concatenatedConvolution)\n",
    "    x2 = layers.Concatenate()([x1,partScaling])\n",
    "    x2 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(x2)\n",
    "\n",
    "    newShape = (x2.shape[1],x2.shape[2])\n",
    "    #print(x1.shape)\n",
    "    #cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    #x3 = layers.Concatenate()([x2,cScaling])\n",
    "    x3 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x3 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(x3)\n",
    "    x3 = layers.Dropout(0.3)(x3)\n",
    "    x3 = layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(x3)\n",
    "\n",
    "    x_flat = layers.Flatten()(x3)\n",
    "    #x3 = layers.concatenate([x_flat,complianceMaxInput])\n",
    "    x4 = layers.Dense(32,activation=activation)(x_flat)\n",
    "    #x3 = layers.Dropout(0.2)(x3)\n",
    "\n",
    "    #x4 = layers.concatenate([x3,complianceMaxInput])\n",
    "    #outputs = layers.GlobalAveragePooling1D()(x4)\n",
    "    outputs = layers.Dense(1)(x4)\n",
    "    return keras.Model(inputs= [partInput,forceInput,supportInput],outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m5_regression():\n",
    "    partInput = keras.Input(shape=(31,31,1),name=\"partImage\")\n",
    "    forceInput = keras.Input(shape=(31,31,2),name=\"Forces\")\n",
    "    supportInput = keras.Input(shape=(31,31,1),name=\"Supports\")\n",
    "    #complianceMaxInput = keras.Input(shape=(1),name=\"complianceMax\")\n",
    "\n",
    "\n",
    "    partConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(partInput)\n",
    "    forceConv = layers.Conv2D(filters= 10,kernel_size=(3,3),padding='same',activation='tanh')(forceInput)\n",
    "    supportConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(supportInput)\n",
    "    #cScaling = layers.Reshape((supportConv.shape[1],supportConv.shape[2],1))(layers.RepeatVector(supportConv.shape[1]*supportConv.shape[2])(complianceMaxInput))\n",
    "\n",
    "\n",
    "    concatenatedConvolution = layers.Concatenate()([partConv,forceConv,supportConv])\n",
    "    x1 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(concatenatedConvolution)\n",
    "    x1 = layers.GaussianNoise(stddev=1)(x1)\n",
    "    x1 = layers.ReLU()(x1)\n",
    "    x1 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "    x1 = layers.MaxPool2D(pool_size=(2,2),strides=(2,2),padding='same')(x1)\n",
    "\n",
    "    newShape = (x1.shape[1],x1.shape[2])\n",
    "    #print(x1.shape)\n",
    "    #cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    partScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(concatenatedConvolution)\n",
    "    x2 = layers.Concatenate()([x1,partScaling])\n",
    "    x2 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.GaussianNoise(stddev=1)(x2)\n",
    "    x2 = layers.ReLU()(x2)\n",
    "    x2 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(x2)\n",
    "\n",
    "    newShape = (x2.shape[1],x2.shape[2])\n",
    "    #print(x1.shape)\n",
    "    #cScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(cScaling)\n",
    "    #x3 = layers.Concatenate()([x2,cScaling])\n",
    "    x3 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x3 = layers.GaussianNoise(stddev=1)(x3)\n",
    "    x3 = layers.ReLU()(x3)\n",
    "    x3 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(x3)\n",
    "    x3 = layers.Dropout(0.3)(x3)\n",
    "    x3 = layers.MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(x3)\n",
    "\n",
    "    x_flat = layers.Flatten()(x3)\n",
    "    #x3 = layers.concatenate([x_flat,complianceMaxInput])\n",
    "    x4 = layers.Dense(32,activation=activation)(x_flat)\n",
    "    #x3 = layers.Dropout(0.2)(x3)\n",
    "\n",
    "    #x4 = layers.concatenate([x3,complianceMaxInput])\n",
    "    #outputs = layers.GlobalAveragePooling1D()(x4)\n",
    "    outputs = layers.Dense(1)(x4)\n",
    "    return keras.Model(inputs= [partInput,forceInput,supportInput],outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m6_regression():\n",
    "    partInput = keras.Input(shape=(31,31,1),name=\"partImage\")\n",
    "    forceInput = keras.Input(shape=(31,31,2),name=\"Forces\")\n",
    "    supportInput = keras.Input(shape=(31,31,1),name=\"Supports\")\n",
    "    #complianceMaxInput = keras.Input(shape=(1),name=\"complianceMax\")\n",
    "\n",
    "\n",
    "    partConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(partInput)\n",
    "    forceConv = layers.Conv2D(filters= 10,kernel_size=(3,3),padding='same',activation='tanh')(forceInput)\n",
    "    supportConv = layers.Conv2D(filters= 5,kernel_size=(3,3),padding='same')(supportInput)\n",
    "    #cScaling = layers.Reshape((supportConv.shape[1],supportConv.shape[2],1))(layers.RepeatVector(supportConv.shape[1]*supportConv.shape[2])(complianceMaxInput))\n",
    "\n",
    "\n",
    "    concatenatedConvolution = layers.Concatenate()([partConv,forceConv,supportConv])\n",
    "    x1 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(concatenatedConvolution)\n",
    "    x1 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(x1)\n",
    "    x1 = layers.MaxPool2D(pool_size=(3,3),strides=(3,3),padding='same')(x1)\n",
    "\n",
    "    newShape = (x1.shape[1],x1.shape[2])\n",
    "    partScaling = layers.Resizing(height=newShape[0],width=newShape[1],interpolation='nearest')(concatenatedConvolution)\n",
    "    x2 = layers.Concatenate()([x1,partScaling])\n",
    "    x2 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=(3,3),strides=(3,3),padding='same')(x2)\n",
    "\n",
    "    x_flat = layers.Flatten()(x2)\n",
    "    x4 = layers.Dense(x_flat.shape[1],activation=activation)(x_flat)\n",
    "    x4 = layers.Dense(31,activation=activation)(x4)\n",
    "\n",
    "    outputs = layers.Dense(1)(x4)\n",
    "    return keras.Model(inputs= [partInput,forceInput,supportInput],outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we build and create a summary of the model we wish to train.\n",
    "\n",
    "comp_tester = m6_regression()\n",
    "comp_tester.summary()\n",
    "#keras.utils.plot_model(comp_tester, os.path.join(os.getcwd(),\"model1.png\"), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Little testing has gon into the difference between optimizers.\n",
    "\n",
    "The loss is the standard loss function for regression\n",
    "\n",
    "If you wish to load a model then use the load weights function otherwise comment it out\n",
    "Note that models are not named so inf you save a model you may want to actually name it (see above with the mode save path).\n",
    "\"\"\"\n",
    "\n",
    "selfMadOptomizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "comp_tester.compile(optimizer=selfMadOptomizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "#comp_tester.load_weights(str(ToSavePath))\n",
    "# comp_tester.compile(optimizer=selfMadOptomizer,\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training data.\n",
    "comp_tester.fit(\n",
    "                {\"partImage\":X_train_part, \"Forces\":X_train_forces, \"Supports\":X_train_supports},#, \"complianceMax\":X_train_compliance},\n",
    "                Y_train,\n",
    "                epochs=5,\n",
    "                shuffle=True,\n",
    "                validation_data=(\n",
    "                    {\"partImage\":X_test_part, \"Forces\":X_test_forces, \"Supports\":X_test_supports},#,\"complianceMax\":X_test_compliance}, \n",
    "                        Y_test),\n",
    "                callbacks=[cp_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained we can run some stats on it.\n",
    "The following cells do just that however they use testing data, which while the model theoretically has \"never seen\" the data it is still the data it was tested on\n",
    "\n",
    "More data is needed to create a set of random data that can be used solely for this stats run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsToTest = 100\n",
    "dataSample_part = X_test_part[pointsToTest:]\n",
    "dataSample_forces = X_test_forces[pointsToTest:]\n",
    "dataSample_supports = X_test_supports[pointsToTest:]\n",
    "\n",
    "dataSample_Y_True = Y_test[pointsToTest:]\n",
    "\n",
    "dataSample_Y_pred = comp_tester.predict({\"partImage\":dataSample_part, \"Forces\":dataSample_forces, \"Supports\":dataSample_supports})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorList = []\n",
    "\n",
    "for i in range(pointsToTest):\n",
    "    #print(dataSample_Y_True[i],dataSample_Y_pred[i][0])\n",
    "    a = np.exp(dataSample_Y_True[i])\n",
    "    b = np.exp(dataSample_Y_pred[i][0])\n",
    "    #standard relative error\n",
    "    if(a != 0):\n",
    "        errorList.append((abs(a-b))/a)\n",
    "\n",
    "error = np.array(errorList)\n",
    "\n",
    "print(\"mean:\", np.mean(error))\n",
    "print(\"standard deviation:\", np.std(error))\n",
    "print(\"min: {}\\tmax: {}\".format(np.min(error),np.max(error)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    #print(dataSample_Y_True[i],dataSample_Y_pred[i][0])\n",
    "    a = np.exp(dataSample_Y_True[i])\n",
    "    b = np.exp(dataSample_Y_pred[i][0])\n",
    "    print(\"True: {:.3f}\\t Predicted: {:.3f}\\t error: {:.3f}\".format(a,b,((a-b))/a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts\n",
    "\n",
    "The regression model works well however it is prone to random cases where it is very incorrect. If adding more data to the mix fails to reduce this we can boost the model. Ideally we could use multiple smaller moddels to perform the same calculations and the we can average their return values to hopefully get accurate data without the outlining random error spikes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91d6363c0adb958ed116842d9c2fc7faebb1fa3beaff0888078e0808098095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
