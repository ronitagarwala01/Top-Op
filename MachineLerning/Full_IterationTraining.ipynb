{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetAgentData import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3129\n",
      "Agent1_485135\n"
     ]
    }
   ],
   "source": [
    "#overview of the data\n",
    "dataDirectory = r\"E:\\TopoptGAfileSaves\\ComplianceMinimization\\Agents\"\n",
    "DATA_FILE_PATH = os.path.join(dataDirectory,'100_50')\n",
    "\n",
    "dir_list = os.listdir(DATA_FILE_PATH)\n",
    "max_data_points = len(dir_list)\n",
    "print(\"Number of data points: {}\".format(len(dir_list)))\n",
    "print(dir_list[0])\n",
    "\n",
    "data_x_columns = ['forces','supports','filled','x']\n",
    "data_y_columns = ['x','finished']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset will be the iterative model.\n",
    "\n",
    "This model will attempt to mimic the proccess of the TopOpt where a 'part' and load conditions will be inputed and then the optimal part will then be outputed. This new optimal part should in theory be able to be put back into the model untill it is fully optimized.\n",
    "\n",
    "The model data will be formated as follows:\n",
    "<h1>Inputs</h1>\n",
    "<ul>\n",
    "    <li>xphys:the part to be optimized</li>\n",
    "    <li>forces: as an image for xforces and y forces</li>\n",
    "    <li>Supports: as an image</li>\n",
    "</ul>\n",
    "\n",
    "<h1>Outputs</h1>\n",
    "<ul>\n",
    "    <li>x: the optimal part</li>\n",
    "    <li> A boolean representing if the part has been fully optimized, i.e. last iteration</li>\n",
    "</ul>\n",
    "\n",
    "The data will be built with the following factors in mind.\n",
    "<ol>\n",
    "    <li>The output x will be an image of values between 0 and 1</li>\n",
    "    <li>The boolean can be gotten using a cross entropy error and the wieght on the total accuracy will be low</li>\n",
    "    <li>Since mass will need to remain constant, we can sum the first xphys input layer and use that to normalize the output x</li>\n",
    "    <li>Forces will be an image of nelx+1 by nely+1 with two channels, supports will be the same size as forces but with one channel. xPhys will only be nelx by nely so some form of resizing may be needed</li>\n",
    "    <li>It will be important for the model to know when the part is fully optimized, thus extra data of a fully optimized part as input and itself as output will be used to enforce the idea that there is an optimal end point</li>\n",
    "    <li>A step/jump in iterations may be needed, this will make it so that instead of model predicting an iteration, it will predict 2 iterations in one go.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopOptSequence:\n",
    "    def __init__(self,ID,forces,dof,passive,x,numIterations):\n",
    "        self.ID = ID\n",
    "        self.forceImage = forces\n",
    "        self.anchorImage = dof\n",
    "        self.filledAreaImage = passive\n",
    "        self.xPhys_array = x\n",
    "        self.numIterations = numIterations\n",
    "    \n",
    "    def dispenceData(self,iterationJump:int=5):\n",
    "        \"\"\"\n",
    "        When called creates list of numpy arrays filled with the data needed to train the model\n",
    "\n",
    "        returns:\n",
    "            forces_array\n",
    "            support_array\n",
    "            filled_array\n",
    "            x_array\n",
    "            x_optimized_array\n",
    "            finished_array\n",
    "        \"\"\"\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for j in range(self.numIterations-iterationJump):\n",
    "                dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,j]])\n",
    "                v = 0.0\n",
    "                f= 'unfinished'\n",
    "                if(j+iterationJump >= self.numIterations - 1):\n",
    "                    v = 1.0\n",
    "                    f = 'finished'\n",
    "                dataY.append([self.xPhys_array[:,:,j+iterationJump],np.array([v])])\n",
    "\n",
    "                #print(\"Adding itter: {} -> {}:{}\".format(j,j+iterationJump,f))\n",
    "\n",
    "        for j in range(1,min(iterationJump,self.numIterations)):\n",
    "            # add the last iterations(dataY has True)\n",
    "            dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,-j -1]])\n",
    "            dataY.append([self.xPhys_array[:,:,self.numIterations-1],np.array([1.])])\n",
    "\n",
    "            #print(\"Adding itter: {} -> {}:finished\".format(numIterations-j-1,numIterations-1))\n",
    "\n",
    "        # add the optimal Stoping point data, input = output\n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,self.numIterations-1]])\n",
    "        dataY.append([self.xPhys_array[:,:,self.numIterations-1],np.array([1.])])\n",
    "\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for x,finished in dataY:\n",
    "            x_optimized_array.append(x)\n",
    "            finished_array.append(finished)\n",
    "\n",
    "\n",
    "        forces_array = np.array(forces_array)\n",
    "        support_array = np.array(support_array)\n",
    "        filled_array = np.array(filled_array)\n",
    "        x_array = np.array(x_array)\n",
    "        x_optimized_array = np.array(x_optimized_array)\n",
    "        finished_array = np.array(finished_array)\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "    \n",
    "    def dispenceDataSingle(self,iterationJump:int=5):\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(dataPointsToGrab:int):\n",
    "\n",
    "    # Constants of interest\n",
    "    # DATA_FILE_PATH = path to agent files\n",
    "    # dir_List = all agent files\n",
    "    # max_data_points = total number of datapoints\n",
    "\n",
    "    dataPointsToGrab = min(dataPointsToGrab,max_data_points)\n",
    "\n",
    "    #randomize the data grabed so that the first thee datapoints aren't always in the data.\n",
    "    indexList = np.arange(max_data_points,dtype='int32')\n",
    "    np.random.shuffle(indexList)\n",
    "\n",
    "    sequenceData = []\n",
    "    print(\"Retreiving {} Datapoints.\".format(dataPointsToGrab))\n",
    "\n",
    "    for i in range(dataPointsToGrab):\n",
    "        print(\"{}%\\t\\t\".format(int(100*(i/dataPointsToGrab))),end='\\r')\n",
    "        try:\n",
    "            #join the data file path to a random sorted member within the data directory\n",
    "            pathToAgent = os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])\n",
    "            forces,dof,passive,x,numIterations = formatIterativeModelDataSet(pathToAgent)\n",
    "        except:\n",
    "            #if an exception occurs list it and move forward\n",
    "            print(\"Exception Occured at file '{}'.\".format(os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])))\n",
    "            continue\n",
    "        else:\n",
    "            #if no error occured append that data to the data list\n",
    "            sequenceData.append(TopOptSequence(i,forces,dof,passive,x,numIterations))\n",
    "\n",
    "    print(\"100%\\t\\t\")\n",
    "    return sequenceData\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retreiving 20 Datapoints.\n",
      "100%\t\t\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "Data = buildDataSet(20)\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 16\n",
      "\n",
      "Test: 3\n",
      "\n",
      "Score: 1\n"
     ]
    }
   ],
   "source": [
    "#Test Train Split\n",
    "\"\"\"\n",
    "By performing the test train split we can get a training data set and a testing dataset to get the metrics for out model\n",
    "By performing the split a second time we can get a validataion dataset that the model will never see that we can use to get out own accuracy score out of\n",
    "\"\"\"\n",
    "Data_train, Data_test = train_test_split(Data, test_size=0.2)\n",
    "Data_test, Data_score= train_test_split(Data_test, test_size=0.05)\n",
    "print(\"Train: {}\".format(len(Data_train)))\n",
    "print(\"\\nTest: {}\".format(len(Data_test)))\n",
    "print(\"\\nScore: {}\".format(len(Data_score)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Information</h1>\n",
    "\n",
    "Below are the models that will be used to attempt to learn the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universal parameters\n",
    "activation = 'relu'\n",
    "uniformRandomInitalizer = tf.random_uniform_initializer(minval=-0.5, maxval=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model_m4(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    \"\"\"\n",
    "    A U-net architecture model to be used to predict the first iteration of the part.\n",
    "    all input images are upscalled to the nearest power of two\n",
    "    \"\"\"\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    \n",
    "    concatenatedConvolution = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    convolution1 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(concatenatedConvolution)\n",
    "    convolution1 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution1)\n",
    "\n",
    "    convolution2 = layers.MaxPool2D(pool_size=(2,2))(convolution1)\n",
    "    convolution2 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution2)\n",
    "    convolution2 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution2)\n",
    "\n",
    "    convolution3 = layers.MaxPool2D(pool_size=(2,2))(convolution2)\n",
    "    convolution3 = layers.Conv2D(filters= 128, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution3)\n",
    "    convolution3 = layers.Conv2D(filters= 128, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution3)\n",
    "    \n",
    "    convolution4 = layers.Conv2DTranspose(filters=64, kernel_size=(3,3),strides=2,padding='same',activation=activation)(convolution3)\n",
    "    convolution4 = layers.Concatenate()([convolution4,convolution2])\n",
    "    convolution4 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution4)\n",
    "    convolution4 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution4)\n",
    "\n",
    "    convolution5 = layers.Conv2DTranspose(filters=32, kernel_size=(3,3),strides=2,padding='same',activation=activation)(convolution4)\n",
    "    convolution5 = layers.Concatenate()([convolution5,convolution1])\n",
    "    convolution5 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution5)\n",
    "    convolution5 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution5)\n",
    "\n",
    "    outputConvolution = layers.Resizing(height=100,width=50)(convolution5)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),strides=1,padding='same',activation=activation, name=\"x_out\")(outputConvolution)\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_m5(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    \"\"\"\n",
    "    Modle based off the m3 but with noise reduction embedded into it.\n",
    "    \"\"\"\n",
    "\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    forcesInput_resize = layers.Activation(activation='tanh')(forcesInput_resize)# normaize the force input\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_128_64)\n",
    "    conv_64_32 = layers.Dropout(rate=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_64_32)\n",
    "    conv_32_16 = layers.Dropout(rate=0.2)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_32_16)\n",
    "    conv_16_8 = layers.Dropout(rate=0.3)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #Dense 2D layer\n",
    "    newShape=conv_16_8.shape[1:]\n",
    "    shapeFlat = np.prod(newShape)\n",
    "    print(\"x2.Shape:{}={}\".format(newShape,shapeFlat))\n",
    "    denseLayer = layers.Flatten()(conv_16_8)\n",
    "    denseLayer = layers.Dense(shapeFlat,activation=activation)(denseLayer)\n",
    "    denseLayer = layers.Reshape(newShape)(denseLayer)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(denseLayer)\n",
    "    print(\"32_16:\",convUpscale_32_16.shape)\n",
    "    convUpscale_32_16 = layers.Dropout(rate=0.3)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Concatenate()([convUpscale_32_16,conv_32_16])\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    print(\"64_32:\",convUpscale_32_16.shape)\n",
    "    convUpscale_64_32 = layers.Dropout(rate=0.2)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Concatenate()([convUpscale_64_32,conv_64_32])\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    print(\"128_64:\",convUpscale_32_16.shape)\n",
    "    convUpscale_128_64 = layers.Dropout(rate=0.1)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Concatenate()([convUpscale_128_64,conv_128_64])\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(convUpscale_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='sigmoid', name=\"x_out\")(output_resize)\n",
    "\n",
    "\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])\n",
    "\n",
    "# m = model_m5()\n",
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_m6(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    \"\"\"\n",
    "    Modle based off the m3 but with noise reduction embedded into it.\n",
    "    \"\"\"\n",
    "\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    forcesInput_resize = layers.Activation(activation='tanh')(forcesInput_resize)# normaize the force input\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_128_64)\n",
    "    conv_64_32 = layers.GaussianNoise(stddev=0.1)(conv_64_32)\n",
    "    conv_64_32 = layers.Dropout(rate=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_64_32)\n",
    "    conv_32_16 = layers.GaussianNoise(stddev=0.1)(conv_32_16)\n",
    "    conv_32_16 = layers.Dropout(rate=0.2)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_32_16)\n",
    "    conv_16_8 = layers.Dropout(rate=0.3)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #Dense 2D layer\n",
    "    newShape=conv_16_8.shape[1:]\n",
    "    shapeFlat = np.prod(newShape)\n",
    "    print(\"x2.Shape:{}={}\".format(newShape,shapeFlat))\n",
    "    denseLayer = layers.Flatten()(conv_16_8)\n",
    "    denseLayer = layers.Dense(shapeFlat,activation=activation)(denseLayer)\n",
    "    denseLayer = layers.Reshape(newShape)(denseLayer)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(denseLayer)\n",
    "    print(\"32_16:\",convUpscale_32_16.shape)\n",
    "    convUpscale_32_16 = layers.Dropout(rate=0.3)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.GaussianNoise(stddev=0.1)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Concatenate()([convUpscale_32_16,conv_32_16])\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    print(\"64_32:\",convUpscale_32_16.shape)\n",
    "    convUpscale_64_32 = layers.Dropout(rate=0.2)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Concatenate()([convUpscale_64_32,conv_64_32])\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    print(\"128_64:\",convUpscale_32_16.shape)\n",
    "    convUpscale_128_64 = layers.Dropout(rate=0.1)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Concatenate()([convUpscale_128_64,conv_128_64])\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(convUpscale_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='hard_sigmoid', name=\"x_out\")(output_resize)\n",
    "    \"\"\"\n",
    "    The hard sigmoid activation, defined as:\n",
    "        if x < -2.5: return 0\n",
    "        if x > 2.5: return 1\n",
    "        if -2.5 <= x <= 2.5: return 0.2 * x + 0.5\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])\n",
    "\n",
    "# m = model_m5()\n",
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetUpOptimizer(variant):\n",
    "    \"\"\"\n",
    "    Builds a keras optmizer based of default parameters\n",
    "    \n",
    "    Accepts:\n",
    "        1:adam\n",
    "        2:adadelta\n",
    "        3:adafactor\n",
    "        4:adagrad\n",
    "        5:adamax\n",
    "        6:ftrl\n",
    "        7:nadam\n",
    "        8:rmsprop\n",
    "    \"\"\"\n",
    "    if(variant == 1 or variant == 'adam'):\n",
    "        print(\"Optimizer: Adam\")\n",
    "        return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam') \n",
    "    elif(variant == 2 or variant == 'adadelta'):\n",
    "        print(\"Optimizer: AdaDelta\")\n",
    "        return keras.optimizers.experimental.Adadelta(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        rho=0.95,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adadelta'\n",
    "                                                    )\n",
    "    elif(variant == 3 or variant == 'adafactor'):\n",
    "        print(\"Optimizer: AdaFactor\")\n",
    "        return keras.optimizers.experimental.Adafactor(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        beta_2_decay=-0.8,\n",
    "                                                        epsilon_1=1e-30,\n",
    "                                                        epsilon_2=0.001,\n",
    "                                                        clip_threshold=1.0,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adafactor'\n",
    "                                                    )\n",
    "    elif(variant == 4 or variant == 'adagrad'):\n",
    "        print(\"Optimizer: AdaGrad\")\n",
    "        return keras.optimizers.experimental.Adagrad(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        initial_accumulator_value=0.1,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adagrad'\n",
    "                                                    )\n",
    "    elif(variant == 5 or variant == 'adamax'):\n",
    "        print(\"Optimizer: AdaMax\")\n",
    "        return keras.optimizers.experimental.Adamax(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        beta_1=0.9,\n",
    "                                                        beta_2=0.999,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adamax'\n",
    "                                                    )\n",
    "    elif(variant == 6 or variant == 'ftrl'):\n",
    "        print(\"Optimizer: FTRL\")\n",
    "        return keras.optimizers.experimental.Ftrl(\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    learning_rate_power=-0.5,\n",
    "                                                    initial_accumulator_value=0.1,\n",
    "                                                    l1_regularization_strength=0.0,\n",
    "                                                    l2_regularization_strength=0.0,\n",
    "                                                    l2_shrinkage_regularization_strength=0.0,\n",
    "                                                    beta=0.0,\n",
    "                                                    ema_momentum=0.99,\n",
    "                                                    name='Ftrl'\n",
    "                                                )\n",
    "    elif(variant == 7 or variant == 'nadam'):\n",
    "        print(\"Optimizer: Nadam\")\n",
    "        return keras.optimizers.experimental.Nadam(\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    beta_1=0.9,\n",
    "                                                    beta_2=0.999,\n",
    "                                                    epsilon=1e-07,\n",
    "                                                    ema_momentum=0.99,\n",
    "                                                    name='Nadam'\n",
    "                                                )\n",
    "    elif(variant == 8 or variant == 'rmsprop'):\n",
    "        print(\"Optimizer: RMSprop\")\n",
    "        return keras.optimizers.experimental.RMSprop(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        rho=0.9,\n",
    "                                                        momentum=0.0,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        ema_overwrite_frequency=100,\n",
    "                                                        name='RMSprop'\n",
    "                                                    )\n",
    "    else:\n",
    "        print(\"Optimizer: Adam\")\n",
    "        return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setUp modelSaving\n",
    "\n",
    "def getModel(modelNumber,optimizerVarient:int = 1):\n",
    "    if(modelNumber == 4):\n",
    "        model = unet_model_m4()\n",
    "        fileSaveName = \"Model_m4\"\n",
    "    \n",
    "    elif(modelNumber == 5):\n",
    "        model = model_m5()\n",
    "        fileSaveName = \"Model_m5\"\n",
    "    elif(modelNumber == 6):\n",
    "        model = model_m6()\n",
    "        fileSaveName = \"Model_m6\"\n",
    "    else:\n",
    "        raise Exception(\"No model identified, model {} DNE.\".format(modelNumber))\n",
    "    \n",
    "\n",
    "    modelPath = os.path.join(os.getcwd(),'ModelSave',fileSaveName)\n",
    "    \n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=os.path.join(modelPath,fileSaveName),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    if(os.path.isdir(modelPath)):\n",
    "        try:\n",
    "            model.load_weights(os.path.join(modelPath,fileSaveName))\n",
    "        except:\n",
    "            print(\"Model weights could not be loaded.\")\n",
    "    else:\n",
    "        os.mkdir(modelPath)\n",
    "\n",
    "    if(modelNumber == 1 or modelNumber == 2):\n",
    "        model.compile(optimizer=SetUpOptimizer(1),\n",
    "                        loss={\n",
    "                            'x_out':keras.losses.MeanSquaredLogarithmicError(), #logrithmic error for the 0-1 output of the image\n",
    "                            'finished':keras.losses.BinaryCrossentropy(from_logits=True) #binary entropy error for the bool output\n",
    "                        },\n",
    "                        loss_weights={'x_out':1.0,'finished':0.01})\n",
    "    elif(modelNumber == 4 or modelNumber == 5 or modelNumber == 6):\n",
    "        model.compile(  optimizer=SetUpOptimizer(optimizerVarient),\n",
    "                        loss= keras.losses.BinaryCrossentropy()\n",
    "                        )\n",
    "    return model,cp_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentModelNumber = 6 #change this one\n",
    "model,callBack = getModel(currentModelNumber)\n",
    "print()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data spliting\n",
    "def splitX(data):\n",
    "    forces_array = []\n",
    "    support_array = []\n",
    "    filled_array = []\n",
    "    x_array = []\n",
    "    for forces,support,filled,x in data:\n",
    "        forces_array.append(forces)\n",
    "        support_array.append(support)\n",
    "        filled_array.append(filled)\n",
    "        x_array.append(x)\n",
    "    return x_array,filled_array,forces_array, support_array,  \n",
    "\n",
    "def splitY(data):\n",
    "    x_array = []\n",
    "    finished_array = []\n",
    "    for x,finished in data:\n",
    "        x_array.append(x)\n",
    "        finished_array.append(finished)\n",
    "    return x_array,finished_array\n",
    "\n",
    "X_train_part, X_train_filled, X_train_forces, X_train_supports = splitX(X_train)\n",
    "Y_train_x, Y_train_finished = splitY(Y_train)\n",
    "X_test_part, X_test_filled, X_test_forces, X_test_supports = splitX(X_test)\n",
    "Y_test_x, Y_test_finished = splitY(Y_test)\n",
    "\n",
    "\n",
    "X_train_part = np.array(X_train_part)\n",
    "X_train_forces = np.array(X_train_forces)\n",
    "X_train_supports = np.array(X_train_supports)\n",
    "X_train_filled = np.array(X_train_filled)\n",
    "Y_train_x = np.array(Y_train_x)\n",
    "Y_train_finished = np.array(Y_train_finished)\n",
    "\n",
    "X_test_part = np.array(X_test_part)\n",
    "X_test_forces = np.array(X_test_forces)\n",
    "X_test_supports = np.array(X_test_supports)\n",
    "X_test_filled = np.array(X_test_filled)\n",
    "Y_test_x = np.array(Y_test_x)\n",
    "Y_test_finished = np.array(Y_test_finished)\n",
    "\n",
    "print(len(X_train_part))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 1\n",
    "BatchSize = 32 # default tensorflow batchsize\n",
    "BatchesPerEpoch = len(X_train_part) // (BatchSize*numEpochs)\n",
    "BatchesPerEpoch = BatchesPerEpoch\n",
    "\n",
    "print(BatchesPerEpoch)\n",
    "def trainModel(modelNumber,batchTrain:bool=False):\n",
    "\n",
    "    if(batchTrain):\n",
    "        batch_size = BatchSize\n",
    "        steps_per_epoch = BatchesPerEpoch\n",
    "    else:\n",
    "        batch_size = None\n",
    "        steps_per_epoch = None\n",
    "\n",
    "    if(modelNumber == 1 or modelNumber == 2):\n",
    "        history = model.fit(\n",
    "                            {'x':X_train_part,'forces':X_train_forces,'supports':X_train_supports,'filled':X_train_filled},\n",
    "                            {'x_out':Y_train_x,'finished':Y_train_finished},\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=numEpochs,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(\n",
    "                                            {'x':X_test_part,'forces':X_test_forces,'supports':X_test_supports,'filled':X_test_filled},\n",
    "                                            {'x_out':Y_test_x,'finished':Y_test_finished}),\n",
    "                            callbacks=[callBack],\n",
    "                            steps_per_epoch = steps_per_epoch)\n",
    "    elif(modelNumber == 3 or modelNumber == 4 or modelNumber == 5):\n",
    "        history = model.fit(\n",
    "                            {'x':X_train_part,'forces':X_train_forces,'supports':X_train_supports,'filled':X_train_filled},\n",
    "                            {'x_out':Y_train_x},\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=numEpochs,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(\n",
    "                                            {'x':X_test_part,'forces':X_test_forces,'supports':X_test_supports,'filled':X_test_filled},\n",
    "                                            {'x_out':Y_test_x}),\n",
    "                            callbacks=[callBack],\n",
    "                            steps_per_epoch = steps_per_epoch)\n",
    "    else:\n",
    "        raise Exception(\"No model identified, model {} DNE.\".format(modelNumber))\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainModel(currentModelNumber,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build some statistics on the data\n",
    "X_score_part, X_score_filled, X_score_forces, X_score_supports = splitX(X_score)\n",
    "Y_score_x, Y_score_finished = splitY(Y_score)\n",
    "\n",
    "X_score_part = np.array(X_score_part)\n",
    "X_score_forces = np.array(X_score_forces)\n",
    "X_score_supports = np.array(X_score_supports)\n",
    "X_score_filled = np.array(X_score_filled)\n",
    "Y_score_x = np.array(Y_score_x)\n",
    "Y_score_finished = np.array(Y_score_finished)\n",
    "\n",
    "output = model.predict({'x':X_score_part,'forces':X_score_forces,'supports':X_score_supports,'filled':X_score_filled})\n",
    "Y_pred_part = output#[0]\n",
    "#Y_pred_finished = output[1]\n",
    "print(len(Y_pred_part))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalBit(a):\n",
    "    if(a[0] > 0):\n",
    "        return 'fin'\n",
    "    else:\n",
    "        return 'it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the true and predicted Values\n",
    "ncol = 5\n",
    "nelx = 100\n",
    "nely = 50\n",
    "\n",
    "fig,ax = plt.subplots(3,ncol)\n",
    "\n",
    "rnd = np.arange(len(X_score_part),dtype='int32')\n",
    "np.random.shuffle(rnd)\n",
    "\n",
    "for i in range(ncol):\n",
    "    ax[0,i].set_title(\"Input\")\n",
    "    ax[0,i].imshow(np.reshape(X_score_part[rnd[i]],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[0,i].get_xaxis().set_visible(False)\n",
    "    ax[0,i].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[1,i].set_title(\"True\")#:{}\".format(finalBit(Y_score_finished[rnd[i]])))\n",
    "    ax[1,i].imshow(np.reshape(Y_score_x[rnd[i]],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[1,i].get_xaxis().set_visible(False)\n",
    "    ax[1,i].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[2,i].set_title(\"Predicted\")#:{}\".format(finalBit(Y_pred_finished[rnd[i]])))\n",
    "    ax[2,i].imshow(np.reshape(Y_pred_part[rnd[i]],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[2,i].get_xaxis().set_visible(False)\n",
    "    ax[2,i].get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thoughts</h1>\n",
    "\n",
    "m1: The model is not learning anything. It is applying a blur filter to what it is given and adding the circles around that. I think that a new aproach it needed to fully train the model.\n",
    "\n",
    "In addition to various different loss functions and optimizers that could be changed, and can easily be changed. I think that training the model on only the first itteration of each data point may be a good indicator of how things should end up.\n",
    "\n",
    "By looking only at the first itteration, the model can learn how to form shapes between the given points instead of just adding a blur. The first iteration being the solid volume filled with the volfrac amount and the optimal part from that being the first signs of a shape forming\n",
    "\n",
    "Since the x data is a float between 0 and 1 I think that it may be nessesary to cosider a different loss function. logrithmic error is good for this type of loss but there may be a better loss function for the part data.\n",
    "\n",
    "m2 Looking at the model outputs for the first iteration dataset, we need to introduce the circles eariler as well as ensure taht the convolutions allow for a full grasp of what the part is. As much as I didn't want to a fully dense or perhaps a few fully dense layers are needed.\n",
    "\n",
    "m3Affter designing the third model and looking at the outputs of the model I can see that it is infact learning the needed shapes to start building the full model. I think that the resolution of the image may be negativly impacting the result. Returned images are very noisy and while considering that we only want to start the iterations, I think that downsampleing that input images would help the model learn faster. A downsampled input as well as some form of denoised output could be what we need.\n",
    "\n",
    "m5. I can increase the data variety by including the outputs of the training as input for data in the next iteration. Improve the activation function for the output layer to easily map between 0 and 1(spline ideal). Possibly add attention layers.\n",
    "\n",
    "m5: When I applied the m5 model to the full dataset I was worries that it too would blur the input part. While looking at the plots of the data aren't enough to fully convince me, I do believe that the model has learned how to create propper shapes and is not just copying the input.\n",
    "\n",
    "I have a multi step plan for fully trainingh the itterative model now.\n",
    "<ol>\n",
    "    <li>The model needs to be pretrained on the first sets of iterations. This will force the model to learn how to propperly make shapes</li>\n",
    "    <li>Once pretraining is complete, a mulit step final training is needed. </li>\n",
    "</ol>\n",
    "The multi step training will be commenced as follows:\n",
    "<ol>\n",
    "    <li>Perform a small batch of training. Some 10% of the full dataset</li>\n",
    "    <li>Allow the model to predict the some fraction(25%) of the remaining training data.</li>\n",
    "    <li>The model predictions will be compiled along side what would be the next itteration as a new data point</li>\n",
    "    <li>This new data point holds as input all the same load conditions but the x_phys will be the predicted x_phys</li>\n",
    "    <li>The new data points are appended to the model training data and we repeat from step 1 untill all data has be passed through</li>\n",
    "</ol>\n",
    "Some method is needed to keep track of all iteration indexes so that the propper next iteration can be found. Also, the datasets need to be kept clean and seperate i.e. all predicted data needs to be removed after each batch of training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91d6363c0adb958ed116842d9c2fc7faebb1fa3beaff0888078e0808098095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
