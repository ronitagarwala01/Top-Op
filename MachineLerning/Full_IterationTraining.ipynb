{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetAgentData import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overview of the data\n",
    "#dataDirectory = os.path.join(os.getcwd(),'data')\n",
    "dataDirectory = r\"E:\\TopoptGAfileSaves\\ComplianceMinimization\\Agents\"\n",
    "DATA_FILE_PATH = os.path.join(dataDirectory,'100_50')\n",
    "\n",
    "dir_list = os.listdir(DATA_FILE_PATH)\n",
    "max_data_points = len(dir_list)\n",
    "print(\"Number of data points: {}\".format(len(dir_list)))\n",
    "print(dir_list[0])\n",
    "\n",
    "data_x_columns = ['forces','supports','filled','x']\n",
    "data_y_columns = ['x','finished']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence class will hold each problem statement as a sequence of all the iterations.\n",
    "It will hold the load conditions as well as each iteration.\n",
    "<ol>\n",
    "<li>When training the model, some fraction of the sequences will be called and the model will train on them.</li>\n",
    "<li>The model will then be given some other fraction of seqences to predict.</li>\n",
    "<li>These predictions will be maped to their original inputs and will be re-outputed as new problem statment iterations.</li>\n",
    "<li>The new(Model predicted) datapoints can then be inputed back into the model for training.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopOptSequence:\n",
    "    def __init__(self,ID,forces,dof,passive,x,numIterations):\n",
    "        self.ID = ID\n",
    "        self.forceImage = forces\n",
    "        self.anchorImage = dof\n",
    "        self.filledAreaImage = passive\n",
    "        self.xPhys_array = x\n",
    "        self.numIterations = numIterations\n",
    "        self.iterationJumpTracker = []\n",
    "    \n",
    "    def dispenceData(self,iterationJump:int=5):\n",
    "        \"\"\"\n",
    "        When called creates list of numpy arrays filled with the data needed to train the model\n",
    "\n",
    "        returns:\n",
    "            forces_array\n",
    "            support_array\n",
    "            filled_array\n",
    "            x_array\n",
    "            x_optimized_array\n",
    "            finished_array\n",
    "        \"\"\"\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        self.iterationJumpTracker = []\n",
    "        for j in range(self.numIterations-iterationJump):\n",
    "                dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,j]])\n",
    "                v = 0.0\n",
    "                f= 'unfinished'\n",
    "                if(j+iterationJump >= self.numIterations - 1):\n",
    "                    v = 1.0\n",
    "                    f = 'finished'\n",
    "                dataY.append([self.xPhys_array[:,:,j+iterationJump],np.array([v])])\n",
    "                self.iterationJumpTracker.append([j,j+iterationJump])\n",
    "\n",
    "                #print(\"Adding itter: {} -> {}:{}\".format(j,j+iterationJump,f))\n",
    "\n",
    "        for j in range(1,min(iterationJump,self.numIterations)):\n",
    "            # add the last iterations(dataY has True)\n",
    "            dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,-j -1]])\n",
    "            dataY.append([self.xPhys_array[:,:,self.numIterations-1],np.array([1.])])\n",
    "            self.iterationJumpTracker.append([-j,self.numIterations-1])\n",
    "\n",
    "            #print(\"Adding itter: {} -> {}:finished\".format(numIterations-j-1,numIterations-1))\n",
    "\n",
    "        # add the optimal Stoping point data, input = output\n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,self.numIterations-1]])\n",
    "        dataY.append([self.xPhys_array[:,:,self.numIterations-1],np.array([1.])])\n",
    "        self.iterationJumpTracker.append([self.numIterations-1,self.numIterations-1])\n",
    "\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for x,finished in dataY:\n",
    "            x_optimized_array.append(x)\n",
    "            finished_array.append(finished)\n",
    "\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "\n",
    "    def findNextIterations(self,i):\n",
    "        \"\"\"\n",
    "        Given and index for a predicted image, find it's next optimized output.\n",
    "        each output image will have a order that matches the order of it's input.\n",
    "        By knowing the input image we can find what image the prediction was supposed to be.\n",
    "        By knowing what the image was supposed to be, we can find next step in the iteration.\n",
    "\n",
    "\n",
    "        Process works like this:\n",
    "            - iterationTracker stores the input and output image,\n",
    "            - We know the input image so we use this to find the correct output image\n",
    "            - We then search the tracker for where the output image was used as input to get the correct next iteration.\n",
    "        \"\"\"\n",
    "        def searchTracker(index):\n",
    "            \"\"\"given an input index find the matching output index\"\"\"\n",
    "            for inputIndex,outputIndex in self.iterationJumpTracker:\n",
    "                if(inputIndex == index):\n",
    "                    return outputIndex\n",
    "            return -1\n",
    "        \n",
    "        correctOutput = searchTracker(i)\n",
    "        nextIteration = searchTracker(correctOutput)\n",
    "        return nextIteration\n",
    "\n",
    "    def formatPredictedData(self,predicted_x_array):\n",
    "        \"\"\"\n",
    "        Given an array of images and the finished array, reformate the images with their respective inputs to create a new dataset that has as input the predicted x and as output the correct next iteration.\n",
    "\n",
    "        The iteration Jump tracker has stored what iterations were jumped so by following the tracker list we can find the correct next output\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(predicted_x_array)\n",
    "        if(n ==  self.numIterations):#check if correct number of iterations has been recieved\n",
    "            forces_array = []\n",
    "            support_array = []\n",
    "            filled_array = []\n",
    "            x_array = []\n",
    "            x_optimized_array = []\n",
    "            finished_array = []\n",
    "            for i in range(n):\n",
    "                correctOutput = self.findNextIterations(i)\n",
    "                if(correctOutput <= 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    forces_array.append(self.forceImage.copy())\n",
    "                    support_array.append(self.anchorImage.copy())\n",
    "                    filled_array.append(self.filledAreaImage.copy())\n",
    "\n",
    "                    x_array.append(predicted_x_array[i].copy())\n",
    "                    v = 0.0\n",
    "                    if(correctOutput >= n):\n",
    "                        correctOutput = n-1\n",
    "                        v = 1.0\n",
    "                    x_optimized_array.append(self.xPhys_array[:,:,correctOutput])\n",
    "                    finished_array.append([v])\n",
    "                    \n",
    "            return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Data given is not of the correct format. required iterations: {}. iterations recieved: {}\".format(self.numIterations,n))\n",
    "\n",
    "    def dispenceFirstIterationData(self):\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        \n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,0]])\n",
    "        dataY.append([self.xPhys_array[:,:,1],np.array([0])])\n",
    "\n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,0]])\n",
    "        dataY.append([self.xPhys_array[:,:,2],np.array([0])])\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for x,finished in dataY:\n",
    "            x_optimized_array.append(x)\n",
    "            finished_array.append(finished)\n",
    "\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "        \n",
    "    def dispenceM7modelData(self,iterationJump:int=5,predictionDepth:int=5):\n",
    "        \"\"\"\n",
    "        When called creates list of numpy arrays filled with the data needed to train the model\n",
    "        Designed to return the inital input but with an array of output representing the folowing iterations.\n",
    "\n",
    "        This will allow the model to train on its own predictions\n",
    "        \"\"\"\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for j in range(self.numIterations-(iterationJump//2)):\n",
    "            dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,j]])\n",
    "            \n",
    "            dataY.append([])\n",
    "            for i in range(predictionDepth):\n",
    "                currentIteration = min(j + i*iterationJump,self.numIterations-1)\n",
    "                v = 0.0\n",
    "                if(currentIteration >= self.numIterations - 1):\n",
    "                    v = 1.0\n",
    "                dataY[j].append([self.xPhys_array[:,:,currentIteration],np.array([v])])\n",
    "\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for i in range(len(dataY)):\n",
    "            x_optimized_array.append([])\n",
    "            finished_array.append([])\n",
    "            for x,finished in dataY[i]:\n",
    "                x_optimized_array[i].append(x)\n",
    "                finished_array[i].append(finished)\n",
    "\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(dataPointsToGrab:int):\n",
    "\n",
    "    # Constants of interest\n",
    "    # DATA_FILE_PATH = path to agent files\n",
    "    # dir_List = all agent files\n",
    "    # max_data_points = total number of datapoints\n",
    "\n",
    "    dataPointsToGrab = min(dataPointsToGrab,max_data_points)\n",
    "\n",
    "    #randomize the data grabed so that the first thee datapoints aren't always in the data.\n",
    "    indexList = np.arange(max_data_points,dtype='int32')\n",
    "    np.random.shuffle(indexList)\n",
    "\n",
    "    sequenceData = []\n",
    "    print(\"Retreiving {} Datapoints.\".format(dataPointsToGrab))\n",
    "\n",
    "    for i in range(dataPointsToGrab):\n",
    "        print(\"{:.2f}%\\t\\t\".format((100*(i/dataPointsToGrab))),end='\\r')\n",
    "        try:\n",
    "            #join the data file path to a random sorted member within the data directory\n",
    "            pathToAgent = os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])\n",
    "            forces,dof,passive,x,numIterations = formatIterativeModelDataSet(pathToAgent)\n",
    "        except:\n",
    "            #if an exception occurs list it and move forward\n",
    "            print(\"Exception Occured at file '{}'.\".format(os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])))\n",
    "            continue\n",
    "        else:\n",
    "            #if no error occured append that data to the data list\n",
    "            sequenceData.append(TopOptSequence(i,forces,dof,passive,x,numIterations))\n",
    "\n",
    "    print(\"100%\\t\\t\")\n",
    "    return sequenceData\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = buildDataSet(50)\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the data output is correct\n",
    "def checkArrays(arraysOfValues):\n",
    "    correct = True\n",
    "    for i in range(1,len(arraysOfValues)):\n",
    "        a = np.equal(arraysOfValues[i-1],arraysOfValues[i]).sum()\n",
    "        numberofValues = np.prod(arraysOfValues[i-1].shape)\n",
    "        #print(a,numberofValues)\n",
    "        if(a != numberofValues):\n",
    "            print(\"iteration {} is not the same as iteration {}, {} != {}.\".format(i-1,i,a,numberofValues))\n",
    "            correct = False\n",
    "        #print(a,forceValues)\n",
    "\n",
    "    return correct\n",
    "            \n",
    "def plotIteration(input_array,output_array,finished_array):\n",
    "    numIterations = len(input_array)\n",
    "    for i in range(numIterations):\n",
    "        fig,ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(input_array[i].T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[0].get_xaxis().set_visible(False)\n",
    "        ax[0].get_yaxis().set_visible(False)\n",
    "        ax[0].set_title(\"Input\")\n",
    "\n",
    "        ax[1].imshow(output_array[i].T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[1].get_xaxis().set_visible(False)\n",
    "        ax[1].get_yaxis().set_visible(False)\n",
    "        ax[1].set_title(\"Output, Finished:{}\".format(finished_array[i]))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def checkData(indexToUse):\n",
    "    currentSum = 0\n",
    "    for sequence in Data:\n",
    "        currentSum += sequence.numIterations\n",
    "    \n",
    "    print(\"With {} problem statements there are {} sample datapoints.\".format(len(Data),currentSum))\n",
    "\n",
    "    forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array = Data[indexToUse].dispenceFirstIterationData()\n",
    "    #check forces\n",
    "    print(\"check Forces:\")\n",
    "    if(checkArrays(forces_array)):\n",
    "        print(\"\\tOk.\")\n",
    "\n",
    "    print(\"check Supports:\")\n",
    "    if(checkArrays(support_array)):\n",
    "        print(\"\\tOk.\")\n",
    "    \n",
    "    print(\"check Filled area:\")\n",
    "    if(checkArrays(filled_array)):\n",
    "        print(\"\\tOk.\")\n",
    "    \n",
    "    numIterations = Data[indexToUse].numIterations\n",
    "    print(\"Iterations:\",numIterations )\n",
    "    plotIteration(x_array,x_optimized_array,finished_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array = Data[0].dispenceData()\n",
    "\n",
    "#print(Data[0].numIterations)\n",
    "#print(len(x_array))\n",
    "checkData(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Train Split\n",
    "\"\"\"\n",
    "By performing the test train split we can get a training data set and a testing dataset to get the metrics for out model\n",
    "By performing the split a second time we can get a validataion dataset that the model will never see that we can use to get out own accuracy score out of\n",
    "\"\"\"\n",
    "Data_train, Data_test = train_test_split(Data, test_size=0.85)\n",
    "Data_test, Data_score= train_test_split(Data_test, test_size=0.7)\n",
    "print(\"Train: {}\".format(len(Data_train)))\n",
    "print(\"\\nTest: {}\".format(len(Data_test)))\n",
    "print(\"\\nScore: {}\".format(len(Data_score)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Information</h1>\n",
    "\n",
    "Below are the models that will be used to attempt to learn the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universal parameters\n",
    "activation = 'relu'\n",
    "uniformRandomInitalizer = tf.random_uniform_initializer(minval=-0.5, maxval=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model_m4(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    \"\"\"\n",
    "    A U-net architecture model to be used to predict the first iteration of the part.\n",
    "    all input images are upscalled to the nearest power of two\n",
    "    \"\"\"\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    \n",
    "    concatenatedConvolution = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    convolution1 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(concatenatedConvolution)\n",
    "    convolution1 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution1)\n",
    "\n",
    "    convolution2 = layers.MaxPool2D(pool_size=(2,2))(convolution1)\n",
    "    convolution2 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution2)\n",
    "    convolution2 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution2)\n",
    "\n",
    "    convolution3 = layers.MaxPool2D(pool_size=(2,2))(convolution2)\n",
    "    convolution3 = layers.Conv2D(filters= 128, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution3)\n",
    "    convolution3 = layers.Conv2D(filters= 128, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution3)\n",
    "    \n",
    "    convolution4 = layers.Conv2DTranspose(filters=64, kernel_size=(3,3),strides=2,padding='same',activation=activation)(convolution3)\n",
    "    convolution4 = layers.Concatenate()([convolution4,convolution2])\n",
    "    convolution4 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution4)\n",
    "    convolution4 = layers.Conv2D(filters= 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution4)\n",
    "\n",
    "    convolution5 = layers.Conv2DTranspose(filters=32, kernel_size=(3,3),strides=2,padding='same',activation=activation)(convolution4)\n",
    "    convolution5 = layers.Concatenate()([convolution5,convolution1])\n",
    "    convolution5 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution5)\n",
    "    convolution5 = layers.Conv2D(filters= 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convolution5)\n",
    "\n",
    "    outputConvolution = layers.Resizing(height=100,width=50)(convolution5)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),strides=1,padding='same',activation=activation, name=\"x_out\")(outputConvolution)\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_m5(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    \"\"\"\n",
    "    Modle based off the m3 but with noise reduction embedded into it.\n",
    "    \"\"\"\n",
    "\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    forcesInput_resize = layers.Activation(activation='tanh')(forcesInput_resize)# normaize the force input\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_128_64)\n",
    "    conv_64_32 = layers.Dropout(rate=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_64_32)\n",
    "    conv_32_16 = layers.Dropout(rate=0.2)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_32_16)\n",
    "    conv_16_8 = layers.Dropout(rate=0.3)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #Dense 2D layer\n",
    "    newShape=conv_16_8.shape[1:]\n",
    "    shapeFlat = np.prod(newShape)\n",
    "    print(\"x2.Shape:{}={}\".format(newShape,shapeFlat))\n",
    "    denseLayer = layers.Flatten()(conv_16_8)\n",
    "    denseLayer = layers.Dense(shapeFlat,activation=activation)(denseLayer)\n",
    "    denseLayer = layers.Reshape(newShape)(denseLayer)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(denseLayer)\n",
    "    print(\"32_16:\",convUpscale_32_16.shape)\n",
    "    convUpscale_32_16 = layers.Dropout(rate=0.3)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Concatenate()([convUpscale_32_16,conv_32_16])\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    print(\"64_32:\",convUpscale_32_16.shape)\n",
    "    convUpscale_64_32 = layers.Dropout(rate=0.2)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Concatenate()([convUpscale_64_32,conv_64_32])\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    print(\"128_64:\",convUpscale_32_16.shape)\n",
    "    convUpscale_128_64 = layers.Dropout(rate=0.1)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Concatenate()([convUpscale_128_64,conv_128_64])\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(convUpscale_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='sigmoid', name=\"x_out\")(output_resize)\n",
    "\n",
    "\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])\n",
    "\n",
    "# m = model_m5()\n",
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_m6(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    \"\"\"\n",
    "    Modle based off the m3 but with noise reduction embedded into it.\n",
    "    \"\"\"\n",
    "\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    forcesInput_resize = layers.Activation(activation='tanh')(forcesInput_resize)# normaize the force input\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_128_64)\n",
    "    conv_64_32 = layers.GaussianNoise(stddev=0.1)(conv_64_32)\n",
    "    conv_64_32 = layers.Dropout(rate=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_64_32)\n",
    "    conv_32_16 = layers.GaussianNoise(stddev=0.1)(conv_32_16)\n",
    "    conv_32_16 = layers.Dropout(rate=0.2)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_32_16)\n",
    "    conv_16_8 = layers.Dropout(rate=0.3)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #Dense 2D layer\n",
    "    newShape=conv_16_8.shape[1:]\n",
    "    shapeFlat = np.prod(newShape)\n",
    "    #print(\"x2.Shape:{}={}\".format(newShape,shapeFlat))\n",
    "    denseLayer = layers.Flatten()(conv_16_8)\n",
    "    denseLayer = layers.Dense(shapeFlat,activation=activation)(denseLayer)\n",
    "    denseLayer_16_8 = layers.Reshape(newShape)(denseLayer)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(denseLayer_16_8)\n",
    "    #print(\"32_16:\",convUpscale_32_16.shape)\n",
    "    convUpscale_32_16 = layers.Dropout(rate=0.3)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.GaussianNoise(stddev=0.1)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Concatenate()([convUpscale_32_16,conv_32_16])\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    #print(\"64_32:\",convUpscale_32_16.shape)\n",
    "    convUpscale_64_32 = layers.Dropout(rate=0.2)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Concatenate()([convUpscale_64_32,conv_64_32])\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    #print(\"128_64:\",convUpscale_32_16.shape)\n",
    "    convUpscale_128_64 = layers.Dropout(rate=0.1)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Concatenate()([convUpscale_128_64,conv_128_64])\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(convUpscale_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='hard_sigmoid', name=\"x_out\")(output_resize)\n",
    "    \"\"\"\n",
    "    The hard sigmoid activation, defined as:\n",
    "        if x < -2.5: return 0\n",
    "        if x > 2.5: return 1\n",
    "        if -2.5 <= x <= 2.5: return 0.2 * x + 0.5\n",
    "    \"\"\"\n",
    "    finishedCheckLayer = layers.Dense(10)(denseLayer)\n",
    "    finishedCheckLayer = layers.Dense(10)(finishedCheckLayer)\n",
    "    finishedOutput = layers.Dense(1,name='finished')(finishedCheckLayer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part,finishedOutput])\n",
    "\n",
    "# m = model_m5()\n",
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel_m7(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    forcesInput_resize = layers.Activation(activation='tanh')(forcesInput_resize)# normaize the force input\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_128_64)\n",
    "    conv_64_32 = layers.GaussianNoise(stddev=0.1)(conv_64_32)\n",
    "    conv_64_32 = layers.Dropout(rate=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_64_32)\n",
    "    conv_32_16 = layers.GaussianNoise(stddev=0.1)(conv_32_16)\n",
    "    conv_32_16 = layers.Dropout(rate=0.2)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_32_16)\n",
    "    conv_16_8 = layers.Dropout(rate=0.3)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #Dense 2D layer\n",
    "    newShape=conv_16_8.shape[1:]\n",
    "    shapeFlat = np.prod(newShape)\n",
    "    denseLayer = layers.Flatten()(conv_16_8)\n",
    "    denseLayer = layers.Dense(shapeFlat,activation=activation)(denseLayer)\n",
    "    denseLayer_16_8 = layers.Reshape(newShape)(denseLayer)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(denseLayer_16_8)\n",
    "    convUpscale_32_16 = layers.Dropout(rate=0.3)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.GaussianNoise(stddev=0.1)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Concatenate()([convUpscale_32_16,conv_32_16])\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_64_32 = layers.Dropout(rate=0.2)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Concatenate()([convUpscale_64_32,conv_64_32])\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_128_64 = layers.Dropout(rate=0.1)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Concatenate()([convUpscale_128_64,conv_128_64])\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(convUpscale_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='hard_sigmoid', name=\"x_out\")(output_resize)\n",
    "    \"\"\"\n",
    "    The hard sigmoid activation, defined as:\n",
    "        if x < -2.5: return 0\n",
    "        if x > 2.5: return 1\n",
    "        if -2.5 <= x <= 2.5: return 0.2 * x + 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])#,finishedOutput])\n",
    "\n",
    "class Model_m7(keras.Model):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Model_m7, self).__init__()\n",
    "        self.model = buildModel_m7()\n",
    "        \n",
    "\n",
    "    def call(self,data,training = False):\n",
    "        #part = data['x']\n",
    "        #forces = data['forces']\n",
    "        #supports = data['supports']\n",
    "        #filled = data['filled']\n",
    "        #print(1)\n",
    "        if(training):\n",
    "            \n",
    "            out1 = self.model(data)\n",
    "            data['x'] = out1\n",
    "            \n",
    "            #print(2)\n",
    "            out2 = self.model(data)\n",
    "            data['x'] = out2\n",
    "\n",
    "            out3 = self.model(data)\n",
    "            data['x'] = out3\n",
    "        \n",
    "            out4 = self.model(data)\n",
    "            data['x'] = out4\n",
    "\n",
    "            out5 = self.model(data)\n",
    "            #print(6)\n",
    "            return out1,out2,out3,out4,out5\n",
    "        else:\n",
    "            return self.model(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel_m8(x_inputShape = (100,50,1),forces_inputShape = (101,51,2),supports_inputShape = (101,51,1),filled_inputShape = (100,50,1)):\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    forcesInput = keras.Input(shape=forces_inputShape,name=\"forces\")\n",
    "    supportsInput = keras.Input(shape=supports_inputShape,name=\"supports\")\n",
    "    #since filled input is solely the solid area it will be passed into the model at the very end\n",
    "    filledInput = keras.Input(shape=filled_inputShape,name=\"filled\")\n",
    "\n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    forcesInput_resize = layers.Resizing(height=128,width=64)(forcesInput)\n",
    "    forcesInput_resize = layers.Activation(activation='tanh')(forcesInput_resize)# normaize the force input\n",
    "    supportsInput_resize = layers.Resizing(height=128,width=64)(supportsInput)\n",
    "    filledInput_resize = layers.Resizing(height=128,width=64)(filledInput)\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput_resize,forcesInput_resize,supportsInput_resize,filledInput_resize])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2))(conv_128_64)\n",
    "    conv_64_32 = layers.GaussianNoise(stddev=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2))(conv_64_32)\n",
    "    conv_32_16 = layers.GaussianNoise(stddev=0.1)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2))(conv_32_16)\n",
    "    conv_16_8 = layers.GaussianNoise(stddev=0.1)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(conv_16_8)\n",
    "    convUpscale_32_16 = layers.GaussianNoise(stddev=0.1)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Concatenate()([convUpscale_32_16,conv_32_16])\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Concatenate()([convUpscale_64_32,conv_64_32])\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_128_64 = layers.Concatenate()([convUpscale_128_64,conv_128_64])\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 16, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 16, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(convUpscale_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='hard_sigmoid', name=\"x_out\")(output_resize)\n",
    "    \"\"\"\n",
    "    The hard sigmoid activation, defined as:\n",
    "        if x < -2.5: return 0\n",
    "        if x > 2.5: return 1\n",
    "        if -2.5 <= x <= 2.5: return 0.2 * x + 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    return keras.Model(inputs= [partInput,forcesInput,supportsInput,filledInput],outputs=[output_part])#,finishedOutput])\n",
    "\n",
    "class Model_m8(keras.Model):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Model_m8, self).__init__()\n",
    "        self.model = buildModel_m8()\n",
    "        \n",
    "\n",
    "    def call(self,data,training = False):\n",
    "        #part = data['x']\n",
    "        #forces = data['forces']\n",
    "        #supports = data['supports']\n",
    "        #filled = data['filled']\n",
    "        #print(1)\n",
    "        if(training):\n",
    "            \n",
    "            out1 = self.model(data)\n",
    "            data['x'] = out1\n",
    "            \n",
    "            #print(2)\n",
    "            out2 = self.model(data)\n",
    "            data['x'] = out2\n",
    "\n",
    "            out3 = self.model(data)\n",
    "            data['x'] = out3\n",
    "        \n",
    "            out4 = self.model(data)\n",
    "            data['x'] = out4\n",
    "\n",
    "            out5 = self.model(data)\n",
    "            #print(6)\n",
    "            return out1,out2,out3,out4,out5\n",
    "        else:\n",
    "            return self.model(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatAndCrop(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Takes the images from the upscaled image and the image from the skip connection and concatedates the two by cropping the upscaled image to the size of the skip connection\n",
    "    \"\"\"\n",
    "    def __init__(self,upscale_shape,skipConnection_shape):\n",
    "        super(ConcatAndCrop, self).__init__()\n",
    "        #left crop the image by the difference of the two shapes\n",
    "        #the upscale shape will aways be the larger of the two\n",
    "\n",
    "        horizontalCrop = upscale_shape[1] - skipConnection_shape[1]\n",
    "        verticalCrop = upscale_shape[2] - skipConnection_shape[2]\n",
    "        print(f\"concat shapes {upscale_shape} and {skipConnection_shape} create padding {horizontalCrop} and {verticalCrop}\")\n",
    "\n",
    "        self.cropLayer = layers.Cropping2D(cropping=((horizontalCrop,0),(verticalCrop,0)))\n",
    "        self.concatLayer = layers.Concatenate()\n",
    "        #print(f\"cropped to {self.cropLayer.output_shape} and {skipConnection_shape} \")\n",
    "\n",
    "    def call(self,upscaledLayer,skipConnectionLayer):\n",
    "\n",
    "        crop = self.cropLayer(upscaledLayer)\n",
    "        concat = self.concatLayer([crop,skipConnectionLayer])\n",
    "        return concat\n",
    "\n",
    "\n",
    "def buildModel_m9(x_inputShape = (101,51,1),LoadConditionsImage = (101,51,5)):\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    loadsInput = keras.Input(shape=LoadConditionsImage,name=\"loadConditions\")\n",
    "\n",
    "    concatenatedStartLayer = layers.Concatenate()([partInput,loadsInput])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = layers.Conv2D(filters= 16, kernel_size=(3,3),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = layers.MaxPooling2D(pool_size=(2,2),padding='same')(conv_128_64)\n",
    "    conv_64_32 = layers.GaussianNoise(stddev=0.1)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(filters= 32, kernel_size=(3,3),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = layers.MaxPooling2D(pool_size=(2,2),padding='same')(conv_64_32)\n",
    "    conv_32_16 = layers.GaussianNoise(stddev=0.1)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = layers.Conv2D(filters= 64, kernel_size=(3,3),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = layers.MaxPooling2D(pool_size=(2,2),padding='same')(conv_32_16)\n",
    "    conv_16_8 = layers.GaussianNoise(stddev=0.1)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = layers.Conv2D(filters= 128, kernel_size=(3,3),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(conv_16_8)\n",
    "    convUpscale_32_16 = layers.GaussianNoise(stddev=0.1)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = ConcatAndCrop(convUpscale_32_16.shape,conv_32_16.shape)(convUpscale_32_16,conv_32_16)\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = layers.Conv2D(filters = 64, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = layers.Conv2DTranspose(filters= 32, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = ConcatAndCrop(convUpscale_64_32.shape,conv_64_32.shape)(convUpscale_64_32,conv_64_32)\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.Conv2D(filters = 32, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = layers.Conv2DTranspose(filters= 64, kernel_size=(5,5),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_128_64 = ConcatAndCrop(convUpscale_128_64.shape,conv_128_64.shape)(convUpscale_128_64,conv_128_64)\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 16, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = layers.Conv2D(filters = 16, kernel_size=(3,3),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='hard_sigmoid', name=\"x_out\")(convUpscale_128_64)\n",
    "    \"\"\"\n",
    "    The hard sigmoid activation, defined as:\n",
    "        if x < -2.5: return 0\n",
    "        if x > 2.5: return 1\n",
    "        if -2.5 <= x <= 2.5: return 0.2 * x + 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    return keras.Model(inputs= [partInput,loadsInput],outputs=[output_part])#,finishedOutput])\n",
    "\n",
    "class Model_m9(keras.Model):\n",
    "    def __init__(self,nelx:int=51,nely:int=51):\n",
    "\n",
    "        super(Model_m9, self).__init__()\n",
    "\n",
    "        self.model = buildModel_m9((nelx,nely,1),(nelx,nely,5))\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self,data,training = False):\n",
    "        #part = data['x']\n",
    "        #forces = data['forces']\n",
    "        #supports = data['supports']\n",
    "        #filled = data['filled']\n",
    "        #print(1)\n",
    "        if(training):\n",
    "            \n",
    "            out1 = self.model(data)\n",
    "            data['x'] = out1\n",
    "            \n",
    "            #print(2)\n",
    "            out2 = self.model(data)\n",
    "            data['x'] = out2\n",
    "\n",
    "            out3 = self.model(data)\n",
    "            data['x'] = out3\n",
    "        \n",
    "            out4 = self.model(data)\n",
    "            data['x'] = out4\n",
    "\n",
    "            out5 = self.model(data)\n",
    "            #print(6)\n",
    "            return out1,out2,out3,out4,out5\n",
    "        else:\n",
    "            return self.model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_m9()\n",
    "print(model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetUpOptimizer(variant):\n",
    "    \"\"\"\n",
    "    Builds a keras optmizer based of default parameters\n",
    "    \n",
    "    Accepts:\n",
    "        1:adam\n",
    "        2:adadelta\n",
    "        3:adafactor\n",
    "        4:adagrad\n",
    "        5:adamax\n",
    "        6:ftrl\n",
    "        7:nadam\n",
    "        8:rmsprop\n",
    "    \"\"\"\n",
    "    if(variant == 1 or variant == 'adam'):\n",
    "        print(\"Optimizer: Adam\")\n",
    "        return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam') \n",
    "    elif(variant == 2 or variant == 'adadelta'):\n",
    "        print(\"Optimizer: AdaDelta\")\n",
    "        return keras.optimizers.experimental.Adadelta(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        rho=0.95,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adadelta'\n",
    "                                                    )\n",
    "    elif(variant == 3 or variant == 'adafactor'):\n",
    "        print(\"Optimizer: AdaFactor\")\n",
    "        return keras.optimizers.experimental.Adafactor(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        beta_2_decay=-0.8,\n",
    "                                                        epsilon_1=1e-30,\n",
    "                                                        epsilon_2=0.001,\n",
    "                                                        clip_threshold=1.0,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adafactor'\n",
    "                                                    )\n",
    "    elif(variant == 4 or variant == 'adagrad'):\n",
    "        print(\"Optimizer: AdaGrad\")\n",
    "        return keras.optimizers.experimental.Adagrad(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        initial_accumulator_value=0.1,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adagrad'\n",
    "                                                    )\n",
    "    elif(variant == 5 or variant == 'adamax'):\n",
    "        print(\"Optimizer: AdaMax\")\n",
    "        return keras.optimizers.experimental.Adamax(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        beta_1=0.9,\n",
    "                                                        beta_2=0.999,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adamax'\n",
    "                                                    )\n",
    "    elif(variant == 6 or variant == 'ftrl'):\n",
    "        print(\"Optimizer: FTRL\")\n",
    "        return keras.optimizers.experimental.Ftrl(\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    learning_rate_power=-0.5,\n",
    "                                                    initial_accumulator_value=0.1,\n",
    "                                                    l1_regularization_strength=0.0,\n",
    "                                                    l2_regularization_strength=0.0,\n",
    "                                                    l2_shrinkage_regularization_strength=0.0,\n",
    "                                                    beta=0.0,\n",
    "                                                    ema_momentum=0.99,\n",
    "                                                    name='Ftrl'\n",
    "                                                )\n",
    "    elif(variant == 7 or variant == 'nadam'):\n",
    "        print(\"Optimizer: Nadam\")\n",
    "        return keras.optimizers.experimental.Nadam(\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    beta_1=0.9,\n",
    "                                                    beta_2=0.999,\n",
    "                                                    epsilon=1e-07,\n",
    "                                                    ema_momentum=0.99,\n",
    "                                                    name='Nadam'\n",
    "                                                )\n",
    "    elif(variant == 8 or variant == 'rmsprop'):\n",
    "        print(\"Optimizer: RMSprop\")\n",
    "        return keras.optimizers.experimental.RMSprop(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        rho=0.9,\n",
    "                                                        momentum=0.0,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        ema_overwrite_frequency=100,\n",
    "                                                        name='RMSprop'\n",
    "                                                    )\n",
    "    else:\n",
    "        print(\"Optimizer: Adam\")\n",
    "        return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setUp modelSaving\n",
    "\n",
    "def getModel(modelNumber,optimizerVarient:int = 1):\n",
    "    if(modelNumber == 4):\n",
    "        model = unet_model_m4()\n",
    "        fileSaveName = \"Model_m4\"\n",
    "    \n",
    "    elif(modelNumber == 5):\n",
    "        model = model_m5()\n",
    "        fileSaveName = \"Model_m5\"\n",
    "    elif(modelNumber == 6):\n",
    "        model = model_m6()\n",
    "        fileSaveName = \"Model_m6\"\n",
    "    elif(modelNumber == 7):\n",
    "        model = Model_m7()\n",
    "        fileSaveName = \"Model_m7\"\n",
    "    elif(modelNumber == 8):\n",
    "        model = Model_m8()\n",
    "        fileSaveName = \"Model_m8\"\n",
    "    else:\n",
    "        raise Exception(\"No model identified, model {} DNE.\".format(modelNumber))\n",
    "    \n",
    "\n",
    "    modelPath = os.path.join(os.getcwd(),'ModelSave',fileSaveName)\n",
    "    \n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=os.path.join(modelPath,fileSaveName),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    if(os.path.isdir(modelPath)):\n",
    "        try:\n",
    "            \n",
    "            model.load_weights(os.path.join(modelPath,fileSaveName))\n",
    "        except:\n",
    "            print(\"Model weights could not be loaded.\")\n",
    "        else:\n",
    "            print(\"Model weights Loaded\")\n",
    "    else:\n",
    "        os.mkdir(modelPath)\n",
    "        print(\"Model path created\")\n",
    "\n",
    "    if(modelNumber == 1 or modelNumber == 6 ):\n",
    "        model.compile(optimizer=SetUpOptimizer(1),\n",
    "                        loss={\n",
    "                            'x_out':keras.losses.BinaryCrossentropy(), #logrithmic error for the 0-1 output of the image\n",
    "                            'finished':keras.losses.BinaryCrossentropy(from_logits=True) #binary entropy error for the bool output\n",
    "                        },\n",
    "                        loss_weights={'x_out':1.0,'finished':0.01})\n",
    "    elif(modelNumber == 4 or modelNumber == 5 or modelNumber == 7 or modelNumber == 8):\n",
    "        model.compile(  optimizer=SetUpOptimizer(optimizerVarient),\n",
    "                        loss= keras.losses.BinaryCrossentropy()\n",
    "                        )\n",
    "    return model,cp_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentModelNumber = 8 #change this one\n",
    "model,callBack = getModel(currentModelNumber)\n",
    "print()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFirstIterationDataSet(sequenceData):\n",
    "    forces_array = []\n",
    "    support_array = []\n",
    "    filled_array = []\n",
    "    x_array = []\n",
    "    x_optimized_array =[]\n",
    "    finished_array = []\n",
    "    for i in range(len(sequenceData)):\n",
    "        force,support,filled,x_start,x_optimized,finished = sequenceData[i].dispenceFirstIterationData()\n",
    "        for f in force:\n",
    "            forces_array.append(f)\n",
    "        for s in support:\n",
    "            support_array.append(s)\n",
    "        for f in filled:\n",
    "            filled_array.append(f)\n",
    "        for x in x_start:\n",
    "            x_array.append(x)\n",
    "        for x in x_optimized:\n",
    "            x_optimized_array.append(x)\n",
    "        for f in finished:\n",
    "            finished_array.append(f)\n",
    "    \n",
    "    x_array = np.array(x_array)\n",
    "    forces_array = np.array(forces_array)\n",
    "    support_array = np.array(support_array)\n",
    "    filled_array = np.array(filled_array)\n",
    "    x_optimized_array = np.array(x_optimized_array)\n",
    "    finished_array = np.array(finished_array)\n",
    "\n",
    "    return x_array, forces_array, support_array, filled_array, x_optimized_array, finished_array\n",
    "\n",
    "\n",
    "def createFullIterationDataSet(sequenceData,indexesToUse,iterationJump:int=5):\n",
    "    forces_array = []\n",
    "    support_array = []\n",
    "    filled_array = []\n",
    "    x_array = []\n",
    "    x_optimized_array =[]\n",
    "    finished_array = []\n",
    "    for i in indexesToUse:\n",
    "        force,support,filled,x_start,x_optimized,finished = sequenceData[i].dispenceData(iterationJump)\n",
    "        for f in force:\n",
    "            forces_array.append(f)\n",
    "        for s in support:\n",
    "            support_array.append(s)\n",
    "        for f in filled:\n",
    "            filled_array.append(f)\n",
    "        for x in x_start:\n",
    "            x_array.append(x)\n",
    "        for x in x_optimized:\n",
    "            x_optimized_array.append(x)\n",
    "        for f in finished:\n",
    "            finished_array.append(f)\n",
    "    \n",
    "    x_array = np.array(x_array)\n",
    "    forces_array = np.array(forces_array)\n",
    "    support_array = np.array(support_array)\n",
    "    filled_array = np.array(filled_array)\n",
    "    x_optimized_array = np.array(x_optimized_array)\n",
    "    finished_array = np.array(finished_array)\n",
    "\n",
    "    return x_array, forces_array, support_array, filled_array, x_optimized_array, finished_array\n",
    "\n",
    "\n",
    "def CreateAugmentedFullIterationDataSet(sequenceData,indexesToUse_train,indexToUse_predict,iterationJump:int=5):\n",
    "    forces_array = []\n",
    "    support_array = []\n",
    "    filled_array = []\n",
    "    x_array = []\n",
    "    x_optimized_array =[]\n",
    "    finished_array = []\n",
    "\n",
    "\n",
    "    for index in indexToUse_predict:\n",
    "        part_pred,forces_pred,supports_pred,filled_pred,_,_ = createFullIterationDataSet(sequenceData,[index],iterationJump)\n",
    "\n",
    "        output = model.predict({'x':part_pred,'forces':forces_pred,'supports':supports_pred,'filled':filled_pred})\n",
    "        Y_pred_part = output[0]\n",
    "\n",
    "        force,support,filled,x_start,x_optimized,finished = sequenceData[index].formatPredictedData(Y_pred_part)\n",
    "\n",
    "        for f in force:\n",
    "            forces_array.append(f)\n",
    "        for s in support:\n",
    "            support_array.append(s)\n",
    "        for f in filled:\n",
    "            filled_array.append(f)\n",
    "        for x in x_start:\n",
    "            x_array.append(np.reshape(x,(100,50)))\n",
    "        for x in x_optimized:\n",
    "            x_optimized_array.append(x)\n",
    "        for f in finished:\n",
    "            finished_array.append(f)\n",
    "\n",
    "    for index in indexesToUse_train:\n",
    "        force,support,filled,x_start,x_optimized,finished = sequenceData[index].dispenceData(iterationJump)\n",
    "        for f in force:\n",
    "            forces_array.append(f)\n",
    "        for s in support:\n",
    "            support_array.append(s)\n",
    "        for f in filled:\n",
    "            filled_array.append(f)\n",
    "        for x in x_start:\n",
    "            x_array.append(x)\n",
    "        for x in x_optimized:\n",
    "            x_optimized_array.append(x)\n",
    "        for f in finished:\n",
    "            finished_array.append(f)\n",
    "\n",
    "    #print(x_array)\n",
    "    \n",
    "    x_array = np.array(x_array)\n",
    "    forces_array = np.array(forces_array)\n",
    "    support_array = np.array(support_array)\n",
    "    filled_array = np.array(filled_array)\n",
    "    x_optimized_array = np.array(x_optimized_array)\n",
    "    finished_array = np.array(finished_array)\n",
    "\n",
    "    return x_array, forces_array, support_array, filled_array, x_optimized_array, finished_array\n",
    "\n",
    "\n",
    "def TrainFirstIteration_2OutputModel(numEpochs,data_train,data_test,saveModelCallback):\n",
    "    \"\"\"\n",
    "    Train the model according the the predefined method\n",
    "\n",
    "        1. Model will be trained on all input data for 1 epoch with the first iterations only\n",
    "    \"\"\"\n",
    "\n",
    "    #build data for first Iteration\n",
    "    part,forces,supports,filled,x_true,finished = createFirstIterationDataSet(data_train)\n",
    "    X_test_part,X_test_forces,X_test_supports,X_test_filled,Y_test_x,Y_test_finished = createFirstIterationDataSet(data_test)\n",
    "\n",
    "    # print(f'part:{part.shape}')\n",
    "    # print(f'forces:{forces.shape}')\n",
    "    # print(f'supports:{supports.shape}')\n",
    "    # print(f'filled:{filled.shape}')\n",
    "    # print(f'x_true:{x_true.shape}')\n",
    "    # print(f'finished:{finished.shape}')\n",
    "\n",
    "    history = model.fit(\n",
    "                            {'x':part,'forces':forces,'supports':supports,'filled':filled},\n",
    "                            {'x_out':x_true,'finished':finished},\n",
    "                            epochs=numEpochs,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(\n",
    "                                            {'x':X_test_part,'forces':X_test_forces,'supports':X_test_supports,'filled':X_test_filled},\n",
    "                                            {'x_out':Y_test_x,'finished':Y_test_finished}),\n",
    "                            callbacks=[saveModelCallback])\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def TrainFullIteration_2OutputModel(numEpochs,data_train,data_test,saveModelCallback,iterationJump:int=5):\n",
    "    \"\"\"\n",
    "    Train the model according the the predefined method\n",
    "        The first iteration will train on 1/(numEpochs)% of the full data\n",
    "        The following iterations will predict data and add that into the training data.\n",
    "        As the epochs continue more predicted data will be added to the mix but the same real data fraction will remain\n",
    "        At the final epoch all predicted data will be trained on.\n",
    "        \n",
    "    \"\"\"\n",
    "    #setup global parameters\n",
    "    DataTrainingIndexes = np.arange(len(data_train),dtype='int32')\n",
    "    np.random.shuffle(DataTrainingIndexes)\n",
    "\n",
    "    DataPredictionIndexes = np.arange(len(data_train),dtype='int32')\n",
    "    np.random.shuffle(DataPredictionIndexes)\n",
    "    \n",
    "    DataTestingIndexes = np.arange(len(data_test),dtype='int32')\n",
    "    np.random.shuffle(DataTrainingIndexes)\n",
    "\n",
    "    indexsPerEpoch = (len(DataTrainingIndexes)-1)//numEpochs\n",
    "    #print('indexsPerEpoch:',indexsPerEpoch)\n",
    "\n",
    "    historyArray = []\n",
    "\n",
    "\n",
    "    #first training NO predictions\n",
    "    \n",
    "    #print('DataTrainingIndexes:',DataTrainingIndexes[:indexsPerEpoch])\n",
    "    part,forces,supports,filled,x_true,finished = createFullIterationDataSet(data_train,DataTrainingIndexes[:indexsPerEpoch],iterationJump)\n",
    "    #print('DataTestingIndexes:',DataTestingIndexes)\n",
    "    X_test_part,X_test_forces,X_test_supports,X_test_filled,Y_test_x,Y_test_finished = createFullIterationDataSet(data_test,DataTestingIndexes,iterationJump)\n",
    "\n",
    "    #set up epoch data\n",
    "    BatchSize = 32 # default tensorflow batchsize\n",
    "    BatchesPerEpoch = (len(part)-1) // (BatchSize*numEpochs)\n",
    "    print(\"Training model on first non-augmented data:\")\n",
    "    history = model.fit(\n",
    "                            {'x':part,'forces':forces,'supports':supports,'filled':filled},\n",
    "                            {'x_out':x_true,'finished':finished},\n",
    "                            #batch_size=BatchSize,\n",
    "                            epochs=1,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(\n",
    "                                            {'x':X_test_part,'forces':X_test_forces,'supports':X_test_supports,'filled':X_test_filled},\n",
    "                                            {'x_out':Y_test_x,'finished':Y_test_finished}),\n",
    "                            callbacks=[saveModelCallback])#,\n",
    "                            #steps_per_epoch = BatchesPerEpoch)\n",
    "    historyArray.append(history)\n",
    "\n",
    "    for i in range(1,numEpochs):\n",
    "\n",
    "        #build data for first Iteration\n",
    "        indexesToUse_train = DataTrainingIndexes[indexsPerEpoch*i:indexsPerEpoch*(i+1)]\n",
    "        \n",
    "        np.random.shuffle(DataPredictionIndexes)\n",
    "        indexesToUse_predict = DataPredictionIndexes[:indexsPerEpoch*(i+1)]\n",
    "\n",
    "        part,forces,supports,filled,x_true,finished = CreateAugmentedFullIterationDataSet(data_train,indexesToUse_train,indexesToUse_predict,iterationJump)\n",
    "        X_test_part,X_test_forces,X_test_supports,X_test_filled,Y_test_x,Y_test_finished = createFullIterationDataSet(data_test,DataTestingIndexes,iterationJump)\n",
    "\n",
    "        # print(f'part:{part.shape}')\n",
    "        # print(f'forces:{forces.shape}')\n",
    "        # print(f'supports:{supports.shape}')\n",
    "        # print(f'filled:{filled.shape}')\n",
    "        # print(f'x_true:{x_true.shape}')\n",
    "        # print(f'finished:{finished.shape}')\n",
    "\n",
    "        #set up epoch data\n",
    "        BatchSize = 32 # default tensorflow batchsize\n",
    "        BatchesPerEpoch = (len(part)-1) // (BatchSize*i)\n",
    "        percentAugmented = len(indexesToUse_predict)/(len(indexesToUse_train) + len(indexesToUse_predict))\n",
    "        print(\"\\nTraining model on augmented data {} of {} with {} data points({}% augmented):\".format(i,numEpochs,len(part),percentAugmented))\n",
    "\n",
    "        history = model.fit(\n",
    "                                {'x':part,'forces':forces,'supports':supports,'filled':filled},\n",
    "                                {'x_out':x_true,'finished':finished},\n",
    "                                batch_size=BatchSize,\n",
    "                                epochs=i,\n",
    "                                shuffle=True,\n",
    "                                validation_data=(\n",
    "                                                {'x':X_test_part,'forces':X_test_forces,'supports':X_test_supports,'filled':X_test_filled},\n",
    "                                                {'x_out':Y_test_x,'finished':Y_test_finished}),\n",
    "                                callbacks=[saveModelCallback],\n",
    "                                steps_per_epoch = BatchesPerEpoch)\n",
    "        historyArray.append(history)\n",
    "    \n",
    "    return historyArray\n",
    "\n",
    "\n",
    "def Train2OutputModel(numEpochs,data_train,data_test,saveModelCallback):\n",
    "    \"\"\"\n",
    "    Train the model according the the predefined method\n",
    "\n",
    "        1. Model will be trained on all input data for 1 epoch with the first iterations only\n",
    "        2. Model will then begin 10 batch trainings where at the end of each batch predicted data is added into the training set.\n",
    "    \"\"\"\n",
    "    iterationJump = 1\n",
    "    print(\"Training model on first Iteration dataset\")\n",
    "    firstIterationHistory = TrainFirstIteration_2OutputModel(numEpochs//2,data_train,data_test,saveModelCallback)\n",
    "    print(\"Training model on full dataset\")\n",
    "    fullIterationHistory = TrainFullIteration_2OutputModel(numEpochs,data_train,data_test,saveModelCallback,iterationJump)\n",
    "\n",
    "    #firstIterationHistory = TrainFirstIteration_2OutputModel(1,data_train,data_test,saveModelCallback)\n",
    "    #firstIterationHistory = None\n",
    "    #fullIterationHistory = TrainFullIteration_2OutputModel(2,data_train,data_test,saveModelCallback,iterationJump)\n",
    "\n",
    "    return firstIterationHistory,fullIterationHistory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h1,h2 = Train2OutputModel(10,Data_train,Data_test,callBack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Pretrain_SubcalssModelTrainingSet(data,predictionDepth:int=5):\n",
    "    forces_array = []\n",
    "    support_array = []\n",
    "    filled_array = []\n",
    "    x_array = []\n",
    "    x_pred= []\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    for j in range(predictionDepth):\n",
    "        x_pred.append([])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        force,support,filled,x_start,x_optimized,finished = Data_train[i].dispenceM7modelData(predictionDepth)\n",
    "        forces_array.append(force[0])\n",
    "        support_array.append(support[0])\n",
    "        filled_array.append(filled[0])\n",
    "        x_array.append(x_start[0])\n",
    "\n",
    "        for j in range(predictionDepth):\n",
    "            x_pred[j].append(x_optimized[0][j])\n",
    "\n",
    "    \n",
    "    x_array = np.array(x_array)\n",
    "    forces_array = np.array(forces_array)\n",
    "    support_array = np.array(support_array)\n",
    "    filled_array = np.array(filled_array)\n",
    "    x_pred_return = []\n",
    "    for j in range(predictionDepth):\n",
    "        x_pred_return.append(np.array(x_pred[j]))\n",
    "\n",
    "    return x_array,forces_array,support_array,filled_array,x_pred_return\n",
    "\n",
    "\n",
    "\n",
    "def Pretrain_modelm7(run=False):\n",
    "\n",
    "    x_array,forces_array,support_array,filled_array,x_pred = create_Pretrain_SubcalssModelTrainingSet(Data_train,5)\n",
    "    x1=x_pred[0]\n",
    "    x2=x_pred[1]\n",
    "    x3=x_pred[2]\n",
    "    x4=x_pred[3]\n",
    "    x5=x_pred[4]\n",
    "\n",
    "    x_array_test,forces_array_test,support_array_test,filled_array_test,x_pred_test = create_Pretrain_SubcalssModelTrainingSet(Data_test,5)\n",
    "    x1_test=x_pred_test[0]\n",
    "    x2_test=x_pred_test[1]\n",
    "    x3_test=x_pred_test[2]\n",
    "    x4_test=x_pred_test[3]\n",
    "    x5_test=x_pred_test[4]\n",
    "\n",
    "    numEpochs = 5\n",
    "    BatchSize = 32 # default tensorflow batchsize\n",
    "    numBatches = len(x_array) // BatchSize\n",
    "    BatchesPerEpoch = numBatches// numEpochs\n",
    "    print(\"Pretraining model over {} epochs.\\n\\tnumSamples: {}\\n\\tnumBatches: {}\\n\\tBatches per Epoch:{}\\n\".format(numEpochs,len(x_array),numBatches,BatchesPerEpoch))\n",
    "    if(run):\n",
    "        history1 = model.fit(\n",
    "            x={'x':x_array,'forces':forces_array,'supports':support_array,'filled':filled_array},\n",
    "            y=(x1,x2,x3,x4,x5),\n",
    "            #validation_split = 0.1,\n",
    "            batch_size=BatchSize,\n",
    "            validation_data=(\n",
    "                    {'x':x_array_test,'forces':forces_array_test,'supports':support_array_test,'filled':filled_array_test},\n",
    "                    x1_test),\n",
    "            epochs=numEpochs,\n",
    "            shuffle=True,\n",
    "            steps_per_epoch = BatchesPerEpoch,\n",
    "            callbacks = [callBack])\n",
    "        return history1\n",
    "    else:\n",
    "        return len(x_array)\n",
    "    \n",
    "h2 = Pretrain_modelm7(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(hist):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_title(\"\")\n",
    "    keys = hist.history.keys()\n",
    "    y = np.arange(1,hist.params['epochs']+1)\n",
    "    for key in keys:\n",
    "        if('output' in key):\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            if(minVal != maxVal):\n",
    "                print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "                #ax.plot(y,hist.history[key],linewidth=0.5,label=key)\n",
    "        else:\n",
    "            ax.plot(y,hist.history[key],label=key)\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubcalssModelTrainingSet(data,predictionDepth:int=5):\n",
    "    forces_array = []\n",
    "    support_array = []\n",
    "    filled_array = []\n",
    "    x_array = []\n",
    "    x_pred= []\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    for j in range(predictionDepth):\n",
    "        x_pred.append([])\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        force,support,filled,x_start,x_optimized,finished = Data_train[i].dispenceM7modelData(predictionDepth)\n",
    "        for f in force:\n",
    "            forces_array.append(f)\n",
    "        for s in support:\n",
    "            support_array.append(s)\n",
    "        for f in filled:\n",
    "            filled_array.append(f)\n",
    "        for x in x_start:\n",
    "            x_array.append(x)\n",
    "\n",
    "        for x_optimized_array in x_optimized:\n",
    "            for j in range(predictionDepth):\n",
    "                x_pred[j].append(x_optimized_array[j])\n",
    "\n",
    "    \n",
    "    x_array = np.array(x_array)\n",
    "    forces_array = np.array(forces_array)\n",
    "    support_array = np.array(support_array)\n",
    "    filled_array = np.array(filled_array)\n",
    "    x_pred_return = []\n",
    "    for j in range(predictionDepth):\n",
    "        x_pred_return.append(np.array(x_pred[j]))\n",
    "\n",
    "    return x_array,forces_array,support_array,filled_array,x_pred_return\n",
    "\n",
    "def train_Test_modelm7(run=False):\n",
    "\n",
    "    x_array,forces_array,support_array,filled_array,x_pred = createSubcalssModelTrainingSet(Data_train,5)\n",
    "    x1=x_pred[0]\n",
    "    x2=x_pred[1]\n",
    "    x3=x_pred[2]\n",
    "    x4=x_pred[3]\n",
    "    x5=x_pred[4]\n",
    "\n",
    "    x_array_test,forces_array_test,support_array_test,filled_array_test,x_pred_test = createSubcalssModelTrainingSet(Data_test,5)\n",
    "    x1_test=x_pred_test[0]\n",
    "    x2_test=x_pred_test[1]\n",
    "    x3_test=x_pred_test[2]\n",
    "    x4_test=x_pred_test[3]\n",
    "    x5_test=x_pred_test[4]\n",
    "\n",
    "    numEpochs = 50\n",
    "    BatchSize = 32 # 32 is default tensorflow batchsize\n",
    "    numBatches = len(x_array) // BatchSize\n",
    "    BatchesPerEpoch = numBatches// numEpochs\n",
    "    print(\"Training model over {} epochs.\\n\\tnumSamples: {}\\n\\tnumBatches: {}\\n\\tBatches per Epoch:{}\\n\".format(numEpochs,len(x_array),numBatches,BatchesPerEpoch))\n",
    "    if(run):\n",
    "        history1 = model.fit(\n",
    "            x={'x':x_array,'forces':forces_array,'supports':support_array,'filled':filled_array},\n",
    "            y=(x1,x2,x3,x4,x5),\n",
    "            #validation_split = 0.1,\n",
    "            batch_size=BatchSize,\n",
    "            validation_data=(\n",
    "                    {'x':x_array_test,'forces':forces_array_test,'supports':support_array_test,'filled':filled_array_test},\n",
    "                    x1_test),\n",
    "            epochs=numEpochs,\n",
    "            shuffle=True,\n",
    "            steps_per_epoch = BatchesPerEpoch,\n",
    "            callbacks = [callBack])\n",
    "        \n",
    "        return history1\n",
    "    \n",
    "\n",
    "#train_Test_modelm7()\n",
    "h3 = train_Test_modelm7(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the true and predicted Values\n",
    "ncol = 5\n",
    "nelx = 100\n",
    "nely = 50\n",
    "\n",
    "fig,ax = plt.subplots(3,ncol)\n",
    "\n",
    "rnd = np.arange(len(part),dtype='int32')\n",
    "np.random.shuffle(rnd)\n",
    "\n",
    "for i in range(ncol):\n",
    "    print(rnd[i],end='\\t')\n",
    "    ax[0,i].set_title(\"Input\")\n",
    "    ax[0,i].imshow(np.reshape(part[rnd[i]],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[0,i].get_xaxis().set_visible(False)\n",
    "    ax[0,i].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[1,i].set_title(\"True\")#:{}\".format(finalBit(Y_score_finished[rnd[i]])))\n",
    "    ax[1,i].imshow(np.reshape(x_true[rnd[i]],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[1,i].get_xaxis().set_visible(False)\n",
    "    ax[1,i].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[2,i].set_title(\"Predicted\")#:{}\".format(finalBit(Y_pred_finished[rnd[i]])))\n",
    "    ax[2,i].imshow(np.reshape(Y_pred_part[rnd[i]],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[2,i].get_xaxis().set_visible(False)\n",
    "    ax[2,i].get_yaxis().set_visible(False)\n",
    "\n",
    "print()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EditImage(image):\n",
    "\n",
    "    #noise = np.random.normal(loc=0,scale=.05,size=image.shape)\n",
    "    #return image + noise\n",
    "\n",
    "    return np.flip(image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawModelIteratins(num):\n",
    "    sequenceToStart:TopOptSequence = Data_score[num]\n",
    "    numImages = sequenceToStart.numIterations\n",
    "    imagesToShow = 5\n",
    "    #print(numImages)\n",
    "    fig,ax = plt.subplots(2,imagesToShow)\n",
    "\n",
    "    sequenceToStart.dispenceData()\n",
    "\n",
    "    nelx = 100\n",
    "    nely = 50\n",
    "    force,support,filled,x_start,x_optimized,finished = sequenceToStart.dispenceData(1)\n",
    "\n",
    "    force = np.array([force[0]])\n",
    "    force = EditImage(force)\n",
    "    support = EditImage(np.array([support[0]]))\n",
    "    filled = EditImage(np.array([filled[0]]))\n",
    "    ImageToPredict = np.array([sequenceToStart.xPhys_array[:,:,0]])\n",
    "    PredictedImages = [ImageToPredict]\n",
    "\n",
    "    start = time()\n",
    "    for i in range(numImages):\n",
    "        \n",
    "        output = model.predict({'x':ImageToPredict,'forces':force,'supports':support,'filled':filled},verbose = 0)\n",
    "        ImageToPredict = output#[0]\n",
    "        PredictedImages.append(ImageToPredict)\n",
    "    end = time()\n",
    "    print(\"{} iterations took {:.2f} seconds or about {:.5f} seconds per iteration.\".format(numImages,end-start,(end-start)/numImages))\n",
    "    imagesToJump = numImages // imagesToShow\n",
    "\n",
    "    for i in range(0,imagesToShow-1):\n",
    "        ax[0,i].imshow(np.reshape(sequenceToStart.xPhys_array[:,:,i*imagesToJump],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[0,i].get_xaxis().set_visible(False)\n",
    "        ax[0,i].get_yaxis().set_visible(False)\n",
    "\n",
    "        #ax[1,i].set_title(\"Pred\")#:{}\".format(finalBit(Y_score_finished[rnd[i]])))\n",
    "        ax[1,i].imshow(np.reshape(PredictedImages[i*imagesToJump],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[1,i].get_xaxis().set_visible(False)\n",
    "        ax[1,i].get_yaxis().set_visible(False)\n",
    "    ax[0,-1].imshow(np.reshape(sequenceToStart.xPhys_array[:,:,-1],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[0,-1].get_xaxis().set_visible(False)\n",
    "    ax[0,-1].get_yaxis().set_visible(False)\n",
    "\n",
    "    #ax[1,i].set_title(\"Pred\")#:{}\".format(finalBit(Y_score_finished[rnd[i]])))\n",
    "    ax[1,-1].imshow(np.reshape(PredictedImages[-1],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[1,-1].get_xaxis().set_visible(False)\n",
    "    ax[1,-1].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[0,0].set_title(\"True\")\n",
    "    ax[1,0].set_title(\"Pred\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 28#np.random.randint(0,len(Data_score)-1)\n",
    "print(n)\n",
    "DrawModelIteratins(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = np.random.randint(0,len(Data_score)-1)\n",
    "n = np.arange(len(Data_score),dtype='int32')\n",
    "np.random.shuffle(n)\n",
    "print(len(n))\n",
    "for i in range(10):\n",
    "    DrawModelIteratins(n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverIterations(index,data):\n",
    "    sequenceToStart = data[index]\n",
    "    numImages = sequenceToStart.numIterations\n",
    "    imagesToShow = numImages\n",
    "    #print(numImages)\n",
    "\n",
    "    sequenceToStart.dispenceData()\n",
    "\n",
    "    nelx = 100\n",
    "    nely = 50\n",
    "    force,support,filled,x_start,x_optimized,finished = sequenceToStart.dispenceData(1)\n",
    "\n",
    "    force = np.array([force[0]])\n",
    "    support = np.array([support[0]])\n",
    "    filled = np.array([filled[0]])\n",
    "    ImageToPredict = np.array([sequenceToStart.xPhys_array[:,:,0]])\n",
    "    PredictedImages = [ImageToPredict]\n",
    "\n",
    "\n",
    "    start = time()\n",
    "    for i in range(numImages * 3):\n",
    "        \n",
    "        output = model.predict({'x':ImageToPredict,'forces':force,'supports':support,'filled':filled},verbose = 0)\n",
    "        ImageToPredict = output#[0]\n",
    "        PredictedImages.append(ImageToPredict)\n",
    "    end = time()\n",
    "    print(\"{} iterations took {:.2f} seconds or about {:.5f} seconds per iteration.\".format(numImages,end-start,(end-start)/numImages))\n",
    "    imagesToJump = 1\n",
    "    imageArray = []\n",
    "\n",
    "    for i,image in enumerate(PredictedImages):\n",
    "        fig,ax = plt.subplots(1,1)\n",
    "        \n",
    "        if(i == 0):\n",
    "            ax.set_title(\"Iteration: {}\".format(i))\n",
    "        else:\n",
    "            im1 = np.reshape(PredictedImages[i],(nelx*nely))\n",
    "            im2 = np.reshape(PredictedImages[i-1],(nelx*nely))\n",
    "            ax.set_title(\"Iteration: {}, Change: {:.5f}\".format(i,np.linalg.norm(im1-im2,ord=np.inf)))\n",
    "        ax.imshow(np.reshape(image,(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        img_buf = io.BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "\n",
    "        im = Image.open(img_buf)\n",
    "        imageArray.append(im)\n",
    "\n",
    "        #plt.show()\n",
    "    return imageArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesToSave = 10\n",
    "rnd = np.arange(imagesToSave,dtype='int32')\n",
    "np.random.shuffle(rnd)\n",
    "for i in range(10):\n",
    "    im_array = plotOverIterations(rnd[i],Data)\n",
    "    im = im_array[0]\n",
    "    im_array.pop(0)\n",
    "    im.save(\"out{}.gif\".format(i),save_all=True,append_images = im_array,optimize=False,loop=0)\n",
    "    im.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thoughts</h1>\n",
    "\n",
    "M6: Looking at the model at work shows progress when loads are grouped up. Failures seem to occur with spaced load conditions and low volfrac."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Unet, in order to be able to capture multiple resolutions we need to allow the downsampling to resolve issues with resolutions that are not multiples of 2.\n",
    "\n",
    "This can be solved with the following setup\n",
    "Take the resolution when a down sample occurs. Take the ceil of the down sample(5/2 = 3). Pass this to the next layer and continue untill upscaleing. When we upscale there is a possibility of the tensors that must be concatenated not being the same resolution. We take the resolution with the higher dimensions(this will always be the upsampled part, not the skip connection) and crop it by one along the nessesary dimension.\n",
    "\n",
    "Some times the cropping will be nessesary sometimes it will not be nessesary but it will always be a cropping by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = Model_m8()\n",
    "m2.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Model_m9\"\n",
    "\n",
    "modelPath = os.path.join(os.getcwd(),'ModelSave',modelName)\n",
    "model.load_weights(os.path.join(modelPath,modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(m2.model.layers):\n",
    "#layer = model.model.layers[1].output\n",
    "    name = layer.name\n",
    "    #if('conv2d' in name):\n",
    "    print(i,name)\n",
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.model.layers):\n",
    "#layer = model.model.layers[1].output\n",
    "    name = layer.name\n",
    "    #if('conv2d' in name):\n",
    "    print(i,name)\n",
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups = [[11,4],[14,7],[15,8],[18,11],[19,12],[22,15],[23,16],[24,17],[27,20],[28,21],[29,22],[32,25],[33,26],[34,27],[36,29],[37,30],[39,31]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m2_layer in enumerate(m2.model.layers):\n",
    "    for j,m1_layer in enumerate(model.model.layers):\n",
    "        for iMatch,jMatch in matchups:\n",
    "            if(i==iMatch and j == jMatch):\n",
    "                print(f\"Getting weights from {i}:{m2_layer.name} and putting them in {j}:{m1_layer.name}\")\n",
    "                m1_layer.set_weights(m2_layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Model_m9\"\n",
    "\n",
    "modelPath = os.path.join(os.getcwd(),'ModelSave',modelName)\n",
    "model.save_weights(modelPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91d6363c0adb958ed116842d9c2fc7faebb1fa3beaff0888078e0808098095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
