{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetMassData import *\n",
    "from ModelData import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overview of the data\n",
    "#dataDirectory = os.path.join(os.getcwd(),'data')\n",
    "dataDirectory = os.path.join(r\"E:\\TopoptGAfileSaves\",\"Mass minimization\",\"AlienWareData\",\"Augmented\",\"Set1\",\"Agents\")\n",
    "DATA_FILE_PATH = os.path.join(dataDirectory,'100_50')\n",
    "\n",
    "dir_list = os.listdir(DATA_FILE_PATH)\n",
    "max_data_points = len(dir_list)\n",
    "print(\"Number of data points: {}\".format(len(dir_list)))\n",
    "print(dir_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory = os.path.join(\"E:\\TopoptGAfileSaves\",\"Mass minimization\",\"JustMirrored\",\"Agents\",\"100_50\")\n",
    "path = dataDirectory#os.path.join(dataDirectory,'100_50')\n",
    "AgentsToGrab = os.listdir(path)\n",
    "numAgents = len(AgentsToGrab)\n",
    "print(\"There are {} files to explore.\".format(numAgents))\n",
    "\n",
    "fx_array = []\n",
    "fy_array = []\n",
    "Youngs_array = []\n",
    "compliance_array = []\n",
    "stress_array = []\n",
    "for i,agent in enumerate(AgentsToGrab):\n",
    "    print(\"{:.1f}%\\t\".format(100*(i/numAgents)),end='\\r')\n",
    "    agentFiles = os.listdir(os.path.join(path,agent))\n",
    "    for fileName in agentFiles:\n",
    "        if('loadConditions' in fileName):\n",
    "            loadConditions = np.load(os.path.join(path,agent,fileName))\n",
    "            #print('loadCondtions Exist')\n",
    "            formated = unpackLoadConditions(loadConditions)\n",
    "            circles = formated[0]\n",
    "            radii = formated[1]\n",
    "            forces = formated[2]\n",
    "            nelx, nely = formated[3], formated[4]\n",
    "            Youngs, C_max, S_max = formated[5], formated[6], formated[7]\n",
    "            fx_array.append(forces[0][0])\n",
    "            fx_array.append(forces[0][1])\n",
    "            fx_array.append(forces[0][2])\n",
    "            fy_array.append(forces[1][0])\n",
    "            fy_array.append(forces[1][1])\n",
    "            fy_array.append(forces[1][2])\n",
    "            Youngs_array.append(Youngs)\n",
    "            compliance_array.append(C_max)\n",
    "            stress_array.append(S_max)\n",
    "            break\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(fx_array))\n",
    "print(np.mean(fy_array))\n",
    "print(np.mean(Youngs_array))\n",
    "print(np.mean(compliance_array))\n",
    "print(np.mean(stress_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatStats(array):\n",
    "    print(\"min:\",np.min(array))\n",
    "    print(\"mean:\",np.mean(array))\n",
    "    print(\"max:\",np.max(array))\n",
    "\n",
    "    print(\"Normalizing\")\n",
    "    # Forces\n",
    "    # ar2 = np.array(array) \n",
    "    # ar2_mean = np.mean(np.abs(ar2))\n",
    "    # ar2 = ar2 / ar2_mean\n",
    "    ar2 = np.array(array) \n",
    "    ar2_mean = np.mean(ar2)\n",
    "    ar2 = ar2 / ar2_mean\n",
    "    print(\"min:\",np.min(ar2))\n",
    "    print(\"mean:\",np.mean(ar2))\n",
    "    print(\"max:\",np.max(ar2))\n",
    "\n",
    "    print(ar2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatStats(stress_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence class will hold each problem statement as a sequence of all the iterations.\n",
    "It will hold the load conditions as well as each iteration.\n",
    "<ol>\n",
    "<li>When training the model, some fraction of the sequences will be called and the model will train on them.</li>\n",
    "<li>The model will then be given some other fraction of seqences to predict.</li>\n",
    "<li>These predictions will be maped to their original inputs and will be re-outputed as new problem statment iterations.</li>\n",
    "<li>The new(Model predicted) datapoints can then be inputed back into the model for training.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(dataPointsToGrab:int):\n",
    "\n",
    "    # Constants of interest\n",
    "    # DATA_FILE_PATH = path to agent files\n",
    "    # dir_List = all agent files\n",
    "    # max_data_points = total number of datapoints\n",
    "\n",
    "    dataPointsToGrab = min(dataPointsToGrab,max_data_points)\n",
    "\n",
    "    #randomize the data grabed so that the first thee datapoints aren't always in the data.\n",
    "    indexList = np.arange(max_data_points,dtype='int32')\n",
    "    np.random.shuffle(indexList)\n",
    "    nonConvergedCounter = 0\n",
    "\n",
    "    sequenceData = []\n",
    "    print(\"Retreiving {} Datapoints.\".format(dataPointsToGrab))\n",
    "\n",
    "    for i in range(dataPointsToGrab):\n",
    "        print(\"{:.2f}%\\t\\t\".format((100*(i/dataPointsToGrab))),end='\\r')\n",
    "        try:\n",
    "            #join the data file path to a random sorted member within the data directory\n",
    "            pathToAgent = os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])\n",
    "            formated,x_array,derivatives_array,objectives_array,markName = getData(pathToAgent)\n",
    "            \n",
    "        except:\n",
    "            #if an exception occurs list it and move forward\n",
    "            print(\"Exception Occured at file '{}'.\".format(os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])))\n",
    "            continue\n",
    "        else:\n",
    "            cvrg = True\n",
    "            if('NotConverged' in markName):\n",
    "                print(\"file {} has not converged.\".format(dir_list[indexList[i]]))\n",
    "                nonConvergedCounter += 1\n",
    "                cvrg = False\n",
    "            else:\n",
    "                #if no error occured append that data to the data list\n",
    "                sequenceData.append(TopOptSequence(i,formated,x_array,len(x_array),cvrg))\n",
    "\n",
    "    print(\"100%\\t\\t\")\n",
    "    print(f\"Out of {dataPointsToGrab} data points gathered, {100*(nonConvergedCounter/dataPointsToGrab)}% had not converged for a total of {nonConvergedCounter}\")\n",
    "    return sequenceData\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = buildDataSet(5)\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadCondtionsImage = Data[0].formatLoadCondtions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Train Split\n",
    "\"\"\"\n",
    "By performing the test train split we can get a training data set and a testing dataset to get the metrics for out model\n",
    "By performing the split a second time we can get a validataion dataset that the model will never see that we can use to get out own accuracy score out of\n",
    "\"\"\"\n",
    "Data_train, Data_test = train_test_split(Data, test_size=0.5)\n",
    "print(\"Train: {}\".format(len(Data_train)))\n",
    "print(\"\\nTest: {}\".format(len(Data_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Information</h1>\n",
    "\n",
    "Below are the models that will be used to attempt to learn the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setUp modelSaving\n",
    "\n",
    "def getModel():\n",
    "    modelNum = 9\n",
    "    model = Model_m9(101,51)\n",
    "    fileSaveName = \"Model_m{}\".format(modelNum)\n",
    "    \n",
    "    \n",
    "\n",
    "    modelPath = os.path.join(os.getcwd(),'ModelSave',fileSaveName)\n",
    "    \n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=os.path.join(modelPath,fileSaveName),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    if(os.path.isdir(modelPath)):\n",
    "        try:\n",
    "            \n",
    "            model.load_weights(os.path.join(modelPath,fileSaveName))\n",
    "        except:\n",
    "            print(\"Model weights could not be loaded.\")\n",
    "        else:\n",
    "            print(\"Model weights Loaded\")\n",
    "    else:\n",
    "        os.mkdir(modelPath)\n",
    "        print(\"Model path created\")\n",
    "\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=100000,decay_rate=0.9,staircase=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(  optimizer=optimizer,\n",
    "                    loss= keras.losses.BinaryCrossentropy())\n",
    "    return model,cp_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentModelNumber = 9 #change this one\n",
    "model,callBack = getModel()\n",
    "print()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "w1 = []\n",
    "for w in weights:\n",
    "    w1.append(np.ravel(w))\n",
    "\n",
    "w2 = []\n",
    "for w in w1:\n",
    "    w2.append(np.ravel(w))\n",
    "\n",
    "weights = np.concatenate(w2)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(weights))\n",
    "print(np.std(weights))\n",
    "print(np.min(weights))\n",
    "print(np.max(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(hist):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_title(\"\")\n",
    "    keys = hist.history.keys()\n",
    "    y = np.arange(1,hist.params['epochs']+1)\n",
    "    for key in keys:\n",
    "        if('output' in key):\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            if(minVal != maxVal):\n",
    "                print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "                #ax.plot(y,hist.history[key],linewidth=0.5,label=key)\n",
    "        else:\n",
    "            ax.plot(y,hist.history[key],label=key)\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plotHistory_lite(hist):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_title(\"\")\n",
    "    keys = hist.keys()\n",
    "    for key in keys:\n",
    "        if('output' in key):\n",
    "            minVal = min(hist[key])\n",
    "            meanVal = np.mean(hist[key])\n",
    "            maxVal = max(hist[key])\n",
    "            if(minVal != maxVal):\n",
    "                print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "                #ax.plot(y,hist.history[key],linewidth=0.5,label=key)\n",
    "        else:\n",
    "            y = np.arange(1,len(hist[key])+1)\n",
    "            ax.plot(y,hist[key],label=key)\n",
    "            minVal = min(hist[key])\n",
    "            meanVal = np.mean(hist[key])\n",
    "            maxVal = max(hist[key])\n",
    "            print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preTrainModelOverFirstIteration(iterationJump:int=5):\n",
    "    def createDataset(data):\n",
    "        loadCondtions = []\n",
    "        parts = []\n",
    "        outputs = []\n",
    "        for i in range(len(data)):\n",
    "            StartingBlock,formattedImage,outputParts = data[i].dispenceFirstIteration(5,iterationJump)\n",
    "            loadCondtions.append(formattedImage)\n",
    "            parts.append(StartingBlock)\n",
    "            outputArrays = []\n",
    "            for outputBlock in outputParts:\n",
    "                outputArrays.append(outputBlock)\n",
    "            outputs.append(outputArrays)\n",
    "        \n",
    "        loadCondtions = np.array(loadCondtions)\n",
    "        parts = np.array(parts)\n",
    "        outputs = np.array(outputs)\n",
    "        return loadCondtions,parts,outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    format_array,x_array,outputs_array = createDataset(Data_train)\n",
    "\n",
    "    x1 = outputs_array[:,0,:,:,:]\n",
    "    x2 = outputs_array[:,1,:,:,:]\n",
    "    x3 = outputs_array[:,2,:,:,:]\n",
    "    x4 = outputs_array[:,3,:,:,:]\n",
    "    x5 = outputs_array[:,4,:,:,:]\n",
    "\n",
    "    print(\"format_array.shape:\",format_array.shape)\n",
    "    print(\"x_array.shape:\",x_array.shape)\n",
    "    print(\"outputs_array.shape:\",outputs_array.shape)\n",
    "    print(\"x1.shape:\",x1.shape)\n",
    "    print(\"x5.shape:\",x5.shape)\n",
    "    numEpochs = 5\n",
    "    BatchSize = 32 # default tensorflow batchsize\n",
    "    numBatches = len(x_array) // BatchSize\n",
    "    BatchesPerEpoch = numBatches// numEpochs\n",
    "    print(\"Pretraining model over {} epochs.\\n\\tnumSamples: {}\\n\\tnumBatches: {}\\n\\tBatches per Epoch:{}\\n\".format(numEpochs,len(x_array),numBatches,BatchesPerEpoch))\n",
    "    \n",
    "    history1 = model.fit(\n",
    "        x={'x':x_array,'loadConditions':format_array},\n",
    "        y=(x1,x2,x3,x4,x5),\n",
    "        validation_split = 0.1,\n",
    "        epochs=numEpochs,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch = BatchesPerEpoch,\n",
    "        callbacks = [callBack])\n",
    "\n",
    "    return history1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = preTrainModelOverFirstIteration(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(iterationJump:int=5):\n",
    "    def createDataset(data):\n",
    "        loadCondtions = []\n",
    "        parts = []\n",
    "        outputs = []\n",
    "        for i in range(len(data)):\n",
    "            if(data[i].converged):\n",
    "                for j in range(data[i].numIterations):\n",
    "                    StartingBlock,formattedImage,outputParts = data[i].dispenceIteration(j,5,iterationJump)\n",
    "                    loadCondtions.append(formattedImage)\n",
    "                    parts.append(StartingBlock)\n",
    "                    outputArrays = []\n",
    "                    for outputBlock in outputParts:\n",
    "                        outputArrays.append(outputBlock)\n",
    "                    outputs.append(outputArrays)\n",
    "        \n",
    "        loadCondtions = np.array(loadCondtions)\n",
    "        parts = np.array(parts)\n",
    "        outputs = np.array(outputs)\n",
    "        return loadCondtions,parts,outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    format_array,x_array,outputs_array = createDataset(Data)\n",
    "\n",
    "    x1 = outputs_array[:,0,:,:,:]\n",
    "    x2 = outputs_array[:,1,:,:,:]\n",
    "    x3 = outputs_array[:,2,:,:,:]\n",
    "    x4 = outputs_array[:,3,:,:,:]\n",
    "    x5 = outputs_array[:,4,:,:,:]\n",
    "\n",
    "    print(\"format_array.shape:\",format_array.shape)\n",
    "    print(\"x_array.shape:\",x_array.shape)\n",
    "    print(\"outputs_array.shape:\",outputs_array.shape)\n",
    "    print(\"x1.shape:\",x1.shape)\n",
    "    print(\"x5.shape:\",x5.shape)\n",
    "    numEpochs = 20\n",
    "    BatchSize = 7 # default tensorflow batchsize\n",
    "    numBatches = max(1,len(x_array) // BatchSize)\n",
    "    BatchesPerEpoch = max(1,numBatches// numEpochs)\n",
    "    print(\"Pretraining model over {} epochs.\\n\\tnumSamples: {}\\n\\tnumBatches: {}\\n\\tBatches per Epoch:{}\\n\".format(numEpochs,len(x_array),numBatches,BatchesPerEpoch))\n",
    "    \n",
    "    history1 = model.fit(\n",
    "        x={'x':x_array,'loadConditions':format_array},\n",
    "        y=(x1,x2,x3,x4,x5),\n",
    "        validation_split = 0.1,\n",
    "        epochs=numEpochs,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch = BatchesPerEpoch)#,\n",
    "        #callbacks = [callBack])\n",
    "\n",
    "    return history1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model_m10(tf.keras.Model):\n",
    "    def __init__(self,nelx:int=101,nely:int=51):\n",
    "\n",
    "        super(Model_m10, self).__init__()\n",
    "\n",
    "        self.model = buildModel_m9((nelx,nely,1),(nelx,nely,6))\n",
    "\n",
    "        \n",
    "\n",
    "    def call(self,data,training = False):\n",
    "        #part = data['x']\n",
    "        #forces = data['forces']\n",
    "        #supports = data['supports']\n",
    "        #filled = data['filled']\n",
    "        #print(1)\n",
    "        if(training):\n",
    "            \n",
    "            out1 = self.model(data)\n",
    "            data['x'] = out1\n",
    "            \n",
    "            #print(2)\n",
    "            out2 = self.model(data)\n",
    "            data['x'] = out2\n",
    "\n",
    "            out3 = self.model(data)\n",
    "            data['x'] = out3\n",
    "        \n",
    "            out4 = self.model(data)\n",
    "            data['x'] = out4\n",
    "\n",
    "            out5 = self.model(data)\n",
    "            #print(6)\n",
    "            return out1,out2,out3,out4,out5\n",
    "        else:\n",
    "            return self.model(data)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        partsArray = x['x']\n",
    "        formatImage = x['loadConditions']\n",
    "\n",
    "        y1 = y[0]\n",
    "        y2 = y[1]\n",
    "        y3 = y[2]\n",
    "        y4 = y[3]\n",
    "        y5 = y[4]\n",
    "\n",
    "        print('x:',partsArray.shape)\n",
    "        print('formatImage:',formatImage.shape)\n",
    "\n",
    "        #axis 0 = image batches\n",
    "        #axis 1 = image x axis\n",
    "        #axis 2 = image y axis\n",
    "        #axis 4 = image pixel value\n",
    "        part_flipLR = tf.image.flip_left_right(partsArray)\n",
    "        part_flipUD = tf.image.flip_up_down(partsArray)\n",
    "        part_flipDiag = tf.image.flip_left_right(part_flipUD)\n",
    "\n",
    "        #axis 0 = image batches\n",
    "        #axis 1 = image x axis\n",
    "        #axis 2 = image y axis\n",
    "        #axis 4 = image pixel value(0=circles, 1=forcesx, 2= forceY, 3=youngs, 4= compliance, 5=stress)\n",
    "\n",
    "        format_flipLR = tf.image.flip_left_right(formatImage)\n",
    "        format_flipUD = tf.image.flip_up_down(formatImage)\n",
    "        format_flipDiag = tf.image.flip_left_right(format_flipUD)\n",
    "        print('format_flipLR',format_flipLR.shape)\n",
    "\n",
    "        forceAxis_1_Multiply = np.ones(shape=formatImage.shape[1:])\n",
    "        forceAxis_1_Multiply[:,:,1] *= -1\n",
    "        forceAxis_1_Multiply = tf.convert_to_tensor(forceAxis_1_Multiply,dtype='float32')\n",
    "\n",
    "        forceAxis_2_Multiply = np.ones(shape=formatImage.shape[1:])\n",
    "        forceAxis_2_Multiply[:,:,2] *= -1\n",
    "        forceAxis_2_Multiply = tf.convert_to_tensor(forceAxis_2_Multiply,dtype='float32')\n",
    "\n",
    "        format_flipLR = tf.math.multiply(format_flipLR,forceAxis_1_Multiply)\n",
    "        format_flipUD = tf.math.multiply(format_flipUD,forceAxis_2_Multiply)\n",
    "\n",
    "        format_flipDiag = tf.math.multiply(format_flipDiag,forceAxis_1_Multiply)\n",
    "        format_flipDiag = tf.math.multiply(format_flipDiag,forceAxis_2_Multiply)\n",
    "\n",
    "        y1_flipLR = tf.image.flip_left_right(y1)\n",
    "        y1_flipUD = tf.image.flip_up_down(y1)\n",
    "        y1_flipDiag = tf.image.flip_left_right(y1_flipUD)\n",
    "\n",
    "        y2_flipLR = tf.image.flip_left_right(y2)\n",
    "        y2_flipUD = tf.image.flip_up_down(y2)\n",
    "        y2_flipDiag = tf.image.flip_left_right(y2_flipUD)\n",
    "\n",
    "        y3_flipLR = tf.image.flip_left_right(y3)\n",
    "        y3_flipUD = tf.image.flip_up_down(y3)\n",
    "        y3_flipDiag = tf.image.flip_left_right(y3_flipUD)\n",
    "\n",
    "        y4_flipLR = tf.image.flip_left_right(y4)\n",
    "        y4_flipUD = tf.image.flip_up_down(y4)\n",
    "        y4_flipDiag = tf.image.flip_left_right(y4_flipUD)\n",
    "\n",
    "        y5_flipLR = tf.image.flip_left_right(y5)\n",
    "        y5_flipUD = tf.image.flip_up_down(y5)\n",
    "        y5_flipDiag = tf.image.flip_left_right(y5_flipUD)\n",
    "\n",
    "        x_array = tf.concat([partsArray,part_flipLR,part_flipUD,part_flipDiag],axis=0)\n",
    "        format_array = tf.concat([formatImage,format_flipLR,format_flipUD,format_flipDiag],axis=0)\n",
    "\n",
    "        y1 = tf.concat([y1,y1_flipLR,y1_flipUD,y1_flipDiag],axis=0)\n",
    "        y2 = tf.concat([y2,y2_flipLR,y2_flipUD,y2_flipDiag],axis=0)\n",
    "        y3 = tf.concat([y3,y3_flipLR,y3_flipUD,y3_flipDiag],axis=0)\n",
    "        y4 = tf.concat([y4,y4_flipLR,y4_flipUD,y4_flipDiag],axis=0)\n",
    "        y5 = tf.concat([y5,y5_flipLR,y5_flipUD,y5_flipDiag],axis=0)\n",
    "\n",
    "        y_true = (y1,y2,y3,y4,y5)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self({'x':x_array,'loadConditions':format_array}, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y_true, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "model = Model_m10(101,51)\n",
    "fileSaveName = \"Model_m{}\".format(9)\n",
    "modelPath = os.path.join(os.getcwd(),'ModelSave',fileSaveName)\n",
    "model.load_weights(os.path.join(modelPath,fileSaveName))\n",
    "\n",
    "model.compile(  optimizer='Adam',\n",
    "                    loss= keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = TrainModel(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory(h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = TrainModel(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = json.load(open(r\"C:\\Users\\Nate\\Documents\\GitHub\\SOundstuff\\Top-Op\\trainHistory_0\",'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHistory_lite(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EditImage(image):\n",
    "\n",
    "    #noise = np.random.normal(loc=0,scale=.05,size=image.shape)\n",
    "    #return image + noise\n",
    "\n",
    "    return np.flip(image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawModelIteratins(num):\n",
    "    sequenceToStart:TopOptSequence = Data_test[num]\n",
    "    numImages = sequenceToStart.numIterations\n",
    "    imagesToShow = 5\n",
    "    #print(numImages)\n",
    "    fig,ax = plt.subplots(2,imagesToShow)\n",
    "\n",
    "    nelx = 100\n",
    "    nely = 50\n",
    "    StartingBlock,formattedImage,_ = sequenceToStart.dispenceFirstIteration(1,1)\n",
    "\n",
    "    formattedImage = np.array(formattedImage)\n",
    "    ImageToPredict = np.array(StartingBlock)\n",
    "    PredictedImages = [ImageToPredict]\n",
    "\n",
    "    start = time()\n",
    "    for i in range(numImages):\n",
    "        \n",
    "        output = model.predict({'x':ImageToPredict,'loadConditions':formattedImage},verbose = 0)\n",
    "        ImageToPredict = output#[0]\n",
    "        PredictedImages.append(ImageToPredict)\n",
    "    end = time()\n",
    "    print(\"{} iterations took {:.2f} seconds or about {:.5f} seconds per iteration.\".format(numImages,end-start,(end-start)/numImages))\n",
    "    imagesToJump = numImages // imagesToShow\n",
    "\n",
    "    for i in range(0,imagesToShow-1):\n",
    "        ax[0,i].imshow(np.reshape(sequenceToStart.xPhys_array[:,:,i*imagesToJump],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[0,i].get_xaxis().set_visible(False)\n",
    "        ax[0,i].get_yaxis().set_visible(False)\n",
    "\n",
    "        #ax[1,i].set_title(\"Pred\")#:{}\".format(finalBit(Y_score_finished[rnd[i]])))\n",
    "        ax[1,i].imshow(np.reshape(PredictedImages[i*imagesToJump],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[1,i].get_xaxis().set_visible(False)\n",
    "        ax[1,i].get_yaxis().set_visible(False)\n",
    "    ax[0,-1].imshow(np.reshape(sequenceToStart.xPhys_array[:,:,-1],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[0,-1].get_xaxis().set_visible(False)\n",
    "    ax[0,-1].get_yaxis().set_visible(False)\n",
    "\n",
    "    #ax[1,i].set_title(\"Pred\")#:{}\".format(finalBit(Y_score_finished[rnd[i]])))\n",
    "    ax[1,-1].imshow(np.reshape(PredictedImages[-1],(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[1,-1].get_xaxis().set_visible(False)\n",
    "    ax[1,-1].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[0,0].set_title(\"True\")\n",
    "    ax[1,0].set_title(\"Pred\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0#np.random.randint(0,len(Data_score)-1)\n",
    "print(n)\n",
    "DrawModelIteratins(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = np.random.randint(0,len(Data_score)-1)\n",
    "n = np.arange(len(Data_score),dtype='int32')\n",
    "np.random.shuffle(n)\n",
    "print(len(n))\n",
    "for i in range(10):\n",
    "    DrawModelIteratins(n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverIterations(index,data):\n",
    "    sequenceToStart = data[index]\n",
    "    numImages = 20#sequenceToStart.numIterations\n",
    "    imagesToShow = numImages\n",
    "    #print(numImages)\n",
    "    nelx = 101\n",
    "    nely = 51\n",
    "    StartingBlock,formattedImage,_ = sequenceToStart.dispenceFirstIteration(1,1)\n",
    "\n",
    "    formattedImage = np.reshape(formattedImage,(1,nelx,nely,6))\n",
    "    ImageToPredict = np.reshape(StartingBlock,(1,nelx,nely,1))\n",
    "    PredictedImages = [ImageToPredict]\n",
    "\n",
    "    start = time()\n",
    "    for i in range(numImages):\n",
    "        \n",
    "        output = m2.predict({'x':ImageToPredict,'loadConditions':formattedImage},verbose = 0)\n",
    "        ImageToPredict = output#[0]\n",
    "        PredictedImages.append(ImageToPredict)\n",
    "    end = time()\n",
    "    print(\"{} iterations took {:.2f} seconds or about {:.5f} seconds per iteration.\".format(numImages,end-start,(end-start)/numImages))\n",
    "    imagesToJump = 1\n",
    "    imageArray = []\n",
    "\n",
    "    for i,image in enumerate(PredictedImages):\n",
    "        fig,ax = plt.subplots(1,1)\n",
    "        \n",
    "        if(i == 0):\n",
    "            ax.set_title(\"Iteration: {}\".format(i))\n",
    "        else:\n",
    "            im1 = np.reshape(PredictedImages[i],(nelx*nely))\n",
    "            im2 = np.reshape(PredictedImages[i-1],(nelx*nely))\n",
    "            ax.set_title(\"Iteration: {}, Change: {:.5f}\".format(i,np.linalg.norm(im1-im2,ord=np.inf)))\n",
    "        ax.imshow(np.reshape(image,(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        #img_buf = io.BytesIO()\n",
    "        #plt.savefig(img_buf, format='png')\n",
    "\n",
    "        #im = Image.open(img_buf)\n",
    "        #imageArray.append(im)\n",
    "\n",
    "        plt.show()\n",
    "    #return imageArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotOverIterations(1,Data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesToSave = 10\n",
    "rnd = np.arange(imagesToSave,dtype='int32')\n",
    "np.random.shuffle(rnd)\n",
    "for i in range(10):\n",
    "    im_array = plotOverIterations(rnd[i],Data)\n",
    "    im = im_array[0]\n",
    "    im_array.pop(0)\n",
    "    im.save(\"out{}.gif\".format(i),save_all=True,append_images = im_array,optimize=False,loop=0)\n",
    "    im.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thoughts</h1>\n",
    "\n",
    "M6: Looking at the model at work shows progress when loads are grouped up. Failures seem to occur with spaced load conditions and low volfrac."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Unet, in order to be able to capture multiple resolutions we need to allow the downsampling to resolve issues with resolutions that are not multiples of 2.\n",
    "\n",
    "This can be solved with the following setup\n",
    "Take the resolution when a down sample occurs. Take the ceil of the down sample(5/2 = 3). Pass this to the next layer and continue untill upscaleing. When we upscale there is a possibility of the tensors that must be concatenated not being the same resolution. We take the resolution with the higher dimensions(this will always be the upsampled part, not the skip connection) and crop it by one along the nessesary dimension.\n",
    "\n",
    "Some times the cropping will be nessesary sometimes it will not be nessesary but it will always be a cropping by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Model_m9\"\n",
    "\n",
    "modelPath = os.path.join(os.getcwd(),'ModelSave',modelName)\n",
    "m2.load_weights(os.path.join(modelPath,modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.set_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(m2.model.layers):\n",
    "#layer = model.model.layers[1].output\n",
    "    name = layer.name\n",
    "    #if('conv2d' in name):\n",
    "    print(i,name)\n",
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,layer in enumerate(model.model.layers):\n",
    "#layer = model.model.layers[1].output\n",
    "    name = layer.name\n",
    "    #if('conv2d' in name):\n",
    "    print(i,name)\n",
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchups = [[11,4],[14,7],[15,8],[18,11],[19,12],[22,15],[23,16],[24,17],[27,20],[28,21],[29,22],[32,25],[33,26],[34,27],[36,29],[37,30],[39,31]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m2_layer in enumerate(m2.model.layers):\n",
    "    for j,m1_layer in enumerate(model.model.layers):\n",
    "        for iMatch,jMatch in matchups:\n",
    "            if(i==iMatch and j == jMatch):\n",
    "                print(f\"Getting weights from {i}:{m2_layer.name} and putting them in {j}:{m1_layer.name}\")\n",
    "                m1_layer.set_weights(m2_layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"Model_m9\"\n",
    "\n",
    "modelPath = os.path.join(os.getcwd(),'ModelSave',modelName)\n",
    "model.save_weights(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel_m10(x_inputShape = (101,51,1),LoadConditionsImage = (101,51,6),activation='relu',kernalSize:int=3):\n",
    "    partInput = tf.keras.layers.Input(shape=x_inputShape,name=\"x\")\n",
    "    loadsInput = tf.keras.layers.Input(shape=LoadConditionsImage,name=\"loadConditions\")\n",
    "\n",
    "    concatenatedStartLayer = tf.keras.layers.Concatenate()([partInput,loadsInput])\n",
    "\n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = tf.keras.layers.Conv2D(filters= 16, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(concatenatedStartLayer)\n",
    "    conv_128_64 = tf.keras.layers.Conv2D(filters= 16, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_128_64)\n",
    "    conv_64_32 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),padding='same')(conv_128_64)\n",
    "    conv_64_32 = tf.keras.layers.GaussianNoise(stddev=0.1)(conv_64_32)\n",
    "    conv_64_32 = tf.keras.layers.Dropout(rate=0.01)(conv_64_32)\n",
    "\n",
    "    #Second convolution Layer\n",
    "    conv_64_32 = tf.keras.layers.Conv2D(filters= 32, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_64_32 = tf.keras.layers.Conv2D(filters= 32, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_64_32)\n",
    "    conv_32_16 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),padding='same')(conv_64_32)\n",
    "    conv_32_16 = tf.keras.layers.GaussianNoise(stddev=0.1)(conv_32_16)\n",
    "    conv_32_16 = tf.keras.layers.Dropout(rate=0.01)(conv_32_16)\n",
    "\n",
    "    conv_32_16 = tf.keras.layers.Conv2D(filters= 64, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_32_16 = tf.keras.layers.Conv2D(filters= 64, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_32_16)\n",
    "    conv_16_8 = tf.keras.layers.MaxPooling2D(pool_size=(2,2),padding='same')(conv_32_16)\n",
    "    conv_16_8 = tf.keras.layers.GaussianNoise(stddev=0.1)(conv_16_8)\n",
    "    conv_16_8 = tf.keras.layers.Dropout(rate=0.01)(conv_16_8)\n",
    "\n",
    "    conv_16_8 = tf.keras.layers.Conv2D(filters= 128, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_16_8)\n",
    "    conv_16_8 = tf.keras.layers.Conv2D(filters= 128, kernel_size=(kernalSize,kernalSize),padding='same',activation=activation)(conv_16_8)\n",
    "\n",
    "    #upscaleLayer\n",
    "    #upscaling is performed by convolution transpose where stride=2 < kernalsize\n",
    "    convUpscale_32_16 = tf.keras.layers.Conv2DTranspose(filters= 64, kernel_size=(kernalSize,kernalSize),strides=2,padding='same',activation=activation)(conv_16_8)\n",
    "    convUpscale_32_16 = tf.keras.layers.GaussianNoise(stddev=0.1)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = ConcatAndCrop(convUpscale_32_16.shape,conv_32_16.shape)(convUpscale_32_16,conv_32_16)\n",
    "    convUpscale_32_16 = tf.keras.layers.Conv2D(filters = 64, kernel_size=(kernalSize,kernalSize),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = tf.keras.layers.Conv2D(filters = 64, kernel_size=(kernalSize,kernalSize),strides=1,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_32_16 = tf.keras.layers.Dropout(rate=0.01)(convUpscale_32_16)\n",
    "\n",
    "    convUpscale_64_32 = tf.keras.layers.Conv2DTranspose(filters= 32, kernel_size=(kernalSize,kernalSize),strides=2,padding='same',activation=activation)(convUpscale_32_16)\n",
    "    convUpscale_64_32 = tf.keras.layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = ConcatAndCrop(convUpscale_64_32.shape,conv_64_32.shape)(convUpscale_64_32,conv_64_32)\n",
    "    convUpscale_64_32 = tf.keras.layers.Conv2D(filters = 32, kernel_size=(kernalSize,kernalSize),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = tf.keras.layers.Conv2D(filters = 32, kernel_size=(kernalSize,kernalSize),strides=1,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = tf.keras.layers.Dropout(rate=0.01)(convUpscale_64_32)\n",
    "\n",
    "    convUpscale_128_64 = tf.keras.layers.Conv2DTranspose(filters= 64, kernel_size=(kernalSize,kernalSize),strides=2,padding='same',activation=activation)(convUpscale_64_32)\n",
    "    convUpscale_64_32 = tf.keras.layers.GaussianNoise(stddev=0.1)(convUpscale_64_32)\n",
    "    convUpscale_128_64 = ConcatAndCrop(convUpscale_128_64.shape,conv_128_64.shape)(convUpscale_128_64,conv_128_64)\n",
    "    convUpscale_128_64 = tf.keras.layers.Conv2D(filters = 16, kernel_size=(kernalSize,kernalSize),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "    convUpscale_128_64 = tf.keras.layers.Conv2D(filters = 16, kernel_size=(kernalSize,kernalSize),strides=1,padding='same',activation=activation)(convUpscale_128_64)\n",
    "\n",
    "    output_part = tf.keras.layers.Conv2D(filters= 1, kernel_size=(1,1),padding='same',activation='hard_sigmoid', name=\"x_out\")(convUpscale_128_64)\n",
    "    \"\"\"\n",
    "    The hard sigmoid activation, defined as:\n",
    "        if x < -2.5: return 0\n",
    "        if x > 2.5: return 1\n",
    "        if -2.5 <= x <= 2.5: return 0.2 * x + 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.keras.Model(inputs= [partInput,loadsInput],outputs=[output_part])#,finishedOutput])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = buildModel_m10(kernalSize=7)\n",
    "m.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d91d6363c0adb958ed116842d9c2fc7faebb1fa3beaff0888078e0808098095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
