{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetAgentData import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A latent diffusion model takes three primary steps\n",
    "<ol>\n",
    "    <li>An auto encoder to reduce the dimensionality of the input space and a decoder to upscale the reduced dimensionality back to its true scale.</li>\n",
    "    <li>An image denoising model. Idealy this will learn to denoise the encoded space. By using cross attention, a second key vector can guide the denoising process</li>\n",
    "    <li>Finally a preprocessing layer to encode the key vector that will be used for denoising.</li>\n",
    "</ol>\n",
    "\n",
    "The auto encoder is trival to train. Give it all data points and let it encode then decode the images.\n",
    "\n",
    "The text preprocessor is less clear how this will work with our problem statement. Ideally we do not need one as our problem statement is finite thus may only require light preprocessing.\n",
    "\n",
    "The image denoiser is the complicated part. From what I can find the image denoiser will be a modified U-net with cross atention layers before each downsample and at each concatenation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image auto encoder is simple enough to create and train. just train on the image dataset to create an encoder and decoder. We will then use the encoded latent space for all denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overview of the data\n",
    "#dataDirectory = os.path.join(os.getcwd(),'data')\n",
    "dataDirectory = r\"E:\\TopoptGAfileSaves\\ComplianceMinimization\\Agents\"\n",
    "DATA_FILE_PATH = os.path.join(dataDirectory,'100_50')\n",
    "\n",
    "dir_list = os.listdir(DATA_FILE_PATH)\n",
    "max_data_points = len(dir_list)\n",
    "print(\"Number of data points: {}\".format(len(dir_list)))\n",
    "print(dir_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopOptSequence:\n",
    "    def __init__(self,ID,forces,dof,passive,x,numIterations):\n",
    "        self.ID = ID\n",
    "        self.forceImage = forces\n",
    "        self.anchorImage = dof\n",
    "        self.filledAreaImage = passive\n",
    "        self.xPhys_array = x\n",
    "        self.numIterations = numIterations\n",
    "        self.iterationJumpTracker = []\n",
    "    \n",
    "    def dispenceData(self,iterationJump:int=5):\n",
    "        \"\"\"\n",
    "        When called creates list of numpy arrays filled with the data needed to train the model\n",
    "\n",
    "        returns:\n",
    "            forces_array\n",
    "            support_array\n",
    "            filled_array\n",
    "            x_array\n",
    "            x_optimized_array\n",
    "            finished_array\n",
    "        \"\"\"\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        self.iterationJumpTracker = []\n",
    "        for j in range(self.numIterations-iterationJump):\n",
    "                dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,j]])\n",
    "                v = 0.0\n",
    "                f= 'unfinished'\n",
    "                if(j+iterationJump >= self.numIterations - 1):\n",
    "                    v = 1.0\n",
    "                    f = 'finished'\n",
    "                dataY.append([self.xPhys_array[:,:,j+iterationJump],np.array([v])])\n",
    "                self.iterationJumpTracker.append([j,j+iterationJump])\n",
    "\n",
    "                #print(\"Adding itter: {} -> {}:{}\".format(j,j+iterationJump,f))\n",
    "\n",
    "        for j in range(1,min(iterationJump,self.numIterations)):\n",
    "            # add the last iterations(dataY has True)\n",
    "            dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,-j -1]])\n",
    "            dataY.append([self.xPhys_array[:,:,self.numIterations-1],np.array([1.])])\n",
    "            self.iterationJumpTracker.append([-j,self.numIterations-1])\n",
    "\n",
    "            #print(\"Adding itter: {} -> {}:finished\".format(numIterations-j-1,numIterations-1))\n",
    "\n",
    "        # add the optimal Stoping point data, input = output\n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,self.numIterations-1]])\n",
    "        dataY.append([self.xPhys_array[:,:,self.numIterations-1],np.array([1.])])\n",
    "        self.iterationJumpTracker.append([self.numIterations-1,self.numIterations-1])\n",
    "\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for x,finished in dataY:\n",
    "            x_optimized_array.append(x)\n",
    "            finished_array.append(finished)\n",
    "\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "\n",
    "    def findNextIterations(self,i):\n",
    "        \"\"\"\n",
    "        Given and index for a predicted image, find it's next optimized output.\n",
    "        each output image will have a order that matches the order of it's input.\n",
    "        By knowing the input image we can find what image the prediction was supposed to be.\n",
    "        By knowing what the image was supposed to be, we can find next step in the iteration.\n",
    "\n",
    "\n",
    "        Process works like this:\n",
    "            - iterationTracker stores the input and output image,\n",
    "            - We know the input image so we use this to find the correct output image\n",
    "            - We then search the tracker for where the output image was used as input to get the correct next iteration.\n",
    "        \"\"\"\n",
    "        def searchTracker(index):\n",
    "            \"\"\"given an input index find the matching output index\"\"\"\n",
    "            for inputIndex,outputIndex in self.iterationJumpTracker:\n",
    "                if(inputIndex == index):\n",
    "                    return outputIndex\n",
    "            return -1\n",
    "        \n",
    "        correctOutput = searchTracker(i)\n",
    "        nextIteration = searchTracker(correctOutput)\n",
    "        return nextIteration\n",
    "\n",
    "    def formatPredictedData(self,predicted_x_array):\n",
    "        \"\"\"\n",
    "        Given an array of images and the finished array, reformate the images with their respective inputs to create a new dataset that has as input the predicted x and as output the correct next iteration.\n",
    "\n",
    "        The iteration Jump tracker has stored what iterations were jumped so by following the tracker list we can find the correct next output\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(predicted_x_array)\n",
    "        if(n ==  self.numIterations):#check if correct number of iterations has been recieved\n",
    "            forces_array = []\n",
    "            support_array = []\n",
    "            filled_array = []\n",
    "            x_array = []\n",
    "            x_optimized_array = []\n",
    "            finished_array = []\n",
    "            for i in range(n):\n",
    "                correctOutput = self.findNextIterations(i)\n",
    "                if(correctOutput <= 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    forces_array.append(self.forceImage.copy())\n",
    "                    support_array.append(self.anchorImage.copy())\n",
    "                    filled_array.append(self.filledAreaImage.copy())\n",
    "\n",
    "                    x_array.append(predicted_x_array[i].copy())\n",
    "                    v = 0.0\n",
    "                    if(correctOutput >= n):\n",
    "                        correctOutput = n-1\n",
    "                        v = 1.0\n",
    "                    x_optimized_array.append(self.xPhys_array[:,:,correctOutput])\n",
    "                    finished_array.append([v])\n",
    "                    \n",
    "            return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Data given is not of the correct format. required iterations: {}. iterations recieved: {}\".format(self.numIterations,n))\n",
    "\n",
    "    def dispenceFirstIterationData(self):\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        \n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,0]])\n",
    "        dataY.append([self.xPhys_array[:,:,1],np.array([0])])\n",
    "\n",
    "        dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,0]])\n",
    "        dataY.append([self.xPhys_array[:,:,2],np.array([0])])\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for x,finished in dataY:\n",
    "            x_optimized_array.append(x)\n",
    "            finished_array.append(finished)\n",
    "\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "        \n",
    "    def dispenceM7modelData(self,iterationJump:int=5,predictionDepth:int=5):\n",
    "        \"\"\"\n",
    "        When called creates list of numpy arrays filled with the data needed to train the model\n",
    "        Designed to return the inital input but with an array of output representing the folowing iterations.\n",
    "\n",
    "        This will allow the model to train on its own predictions\n",
    "        \"\"\"\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for j in range(self.numIterations-(iterationJump//2)):\n",
    "            dataX.append([self.forceImage.copy(),self.anchorImage.copy(),self.filledAreaImage.copy(),self.xPhys_array[:,:,j]])\n",
    "            \n",
    "            dataY.append([])\n",
    "            for i in range(predictionDepth):\n",
    "                currentIteration = min(j + i*iterationJump,self.numIterations-1)\n",
    "                v = 0.0\n",
    "                if(currentIteration >= self.numIterations - 1):\n",
    "                    v = 1.0\n",
    "                dataY[j].append([self.xPhys_array[:,:,currentIteration],np.array([v])])\n",
    "\n",
    "\n",
    "        forces_array = []\n",
    "        support_array = []\n",
    "        filled_array = []\n",
    "        x_array = []\n",
    "        for forces,support,filled,x in dataX:\n",
    "\n",
    "            forces_array.append(forces)\n",
    "            support_array.append(support)\n",
    "            filled_array.append(filled)\n",
    "            x_array.append(x)\n",
    "\n",
    "        x_optimized_array = []\n",
    "        finished_array = []\n",
    "        for i in range(len(dataY)):\n",
    "            x_optimized_array.append([])\n",
    "            finished_array.append([])\n",
    "            for x,finished in dataY[i]:\n",
    "                x_optimized_array[i].append(x)\n",
    "                finished_array[i].append(finished)\n",
    "\n",
    "\n",
    "        return forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array\n",
    "\n",
    "    def dispenceIterationsOnly(self):\n",
    "        x_array = []\n",
    "        for i in range(self.numIterations):\n",
    "            x_array.append(self.xPhys_array[:,:,i])\n",
    "            \n",
    "        return x_array\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataSet(dataPointsToGrab:int):\n",
    "\n",
    "    # Constants of interest\n",
    "    # DATA_FILE_PATH = path to agent files\n",
    "    # dir_List = all agent files\n",
    "    # max_data_points = total number of datapoints\n",
    "\n",
    "    dataPointsToGrab = min(dataPointsToGrab,max_data_points)\n",
    "\n",
    "    #randomize the data grabed so that the first thee datapoints aren't always in the data.\n",
    "    indexList = np.arange(max_data_points,dtype='int32')\n",
    "    np.random.shuffle(indexList)\n",
    "\n",
    "    sequenceData = []\n",
    "    print(\"Retreiving {} Datapoints.\".format(dataPointsToGrab))\n",
    "\n",
    "    for i in range(dataPointsToGrab):\n",
    "        print(\"{:.2f}%\\t\\t\".format((100*(i/dataPointsToGrab))),end='\\r')\n",
    "        try:\n",
    "            #join the data file path to a random sorted member within the data directory\n",
    "            pathToAgent = os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])\n",
    "            forces,dof,passive,x,numIterations = formatIterativeModelDataSet(pathToAgent)\n",
    "        except:\n",
    "            #if an exception occurs list it and move forward\n",
    "            print(\"Exception Occured at file '{}'.\".format(os.path.join(DATA_FILE_PATH,dir_list[indexList[i]])))\n",
    "            continue\n",
    "        else:\n",
    "            #if no error occured append that data to the data list\n",
    "            sequenceData.append(TopOptSequence(i,forces,dof,passive,x,numIterations))\n",
    "\n",
    "    print(\"100%\\t\\t\")\n",
    "    return sequenceData\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = buildDataSet(5000)\n",
    "print(len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the data output is correct\n",
    "def checkArrays(arraysOfValues):\n",
    "    correct = True\n",
    "    for i in range(1,len(arraysOfValues)):\n",
    "        a = np.equal(arraysOfValues[i-1],arraysOfValues[i]).sum()\n",
    "        numberofValues = np.prod(arraysOfValues[i-1].shape)\n",
    "        #print(a,numberofValues)\n",
    "        if(a != numberofValues):\n",
    "            print(\"iteration {} is not the same as iteration {}, {} != {}.\".format(i-1,i,a,numberofValues))\n",
    "            correct = False\n",
    "        #print(a,forceValues)\n",
    "\n",
    "    return correct\n",
    "            \n",
    "def plotIteration(input_array,output_array,finished_array):\n",
    "    numIterations = len(input_array)\n",
    "    for i in range(numIterations):\n",
    "        fig,ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(input_array[i].T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[0].get_xaxis().set_visible(False)\n",
    "        ax[0].get_yaxis().set_visible(False)\n",
    "        ax[0].set_title(\"Input\")\n",
    "\n",
    "        ax[1].imshow(output_array[i].T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "        ax[1].get_xaxis().set_visible(False)\n",
    "        ax[1].get_yaxis().set_visible(False)\n",
    "        ax[1].set_title(\"Output, Finished:{}\".format(finished_array[i]))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def checkData(indexToUse):\n",
    "    currentSum = 0\n",
    "    for sequence in Data:\n",
    "        currentSum += sequence.numIterations\n",
    "    \n",
    "    print(\"With {} problem statements there are {} sample datapoints.\".format(len(Data),currentSum))\n",
    "\n",
    "    forces_array,support_array,filled_array,x_array,x_optimized_array,finished_array = Data[indexToUse].dispenceFirstIterationData()\n",
    "    #check forces\n",
    "    print(\"check Forces:\")\n",
    "    if(checkArrays(forces_array)):\n",
    "        print(\"\\tOk.\")\n",
    "\n",
    "    print(\"check Supports:\")\n",
    "    if(checkArrays(support_array)):\n",
    "        print(\"\\tOk.\")\n",
    "    \n",
    "    print(\"check Filled area:\")\n",
    "    if(checkArrays(filled_array)):\n",
    "        print(\"\\tOk.\")\n",
    "    \n",
    "    numIterations = Data[indexToUse].numIterations\n",
    "    print(\"Iterations:\",numIterations )\n",
    "    plotIteration(x_array,x_optimized_array,finished_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkData(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_train, Data_test = train_test_split(Data, test_size=0.05)\n",
    "print(\"Train: {}\".format(len(Data_train)))\n",
    "print(\"\\nTest: {}\".format(len(Data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#universal parameters\n",
    "activation = 'relu'\n",
    "uniformRandomInitalizer = tf.random_uniform_initializer(minval=0.3, maxval=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildEncoder(x_inputShape = (100,50,1)):\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "    \n",
    "    partInput_resize = layers.Resizing(height=128,width=64)(partInput)\n",
    "    \n",
    "    #First Convolution Layer\n",
    "    conv_128_64 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(partInput_resize)\n",
    "    conv_128_64 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(conv_128_64)\n",
    "\n",
    "    conv_64_32 = layers.MaxPooling2D((2, 2), padding=\"same\")(conv_128_64)\n",
    "    conv_64_32 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv_64_32)\n",
    "    conv_64_32 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(conv_64_32)\n",
    "\n",
    "    conv_32_16 = layers.MaxPooling2D((2, 2), padding=\"same\")(conv_64_32)\n",
    "\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1), padding='same', activation=\"hard_sigmoid\", name=\"x_out\")(conv_32_16)\n",
    "\n",
    "    \n",
    "    return keras.Model(inputs= [partInput],outputs=[output_part])\n",
    "\n",
    "def buildDecoder(x_inputShape = (32,16,1)):\n",
    "    partInput = keras.Input(shape=x_inputShape,name=\"x\")\n",
    "\n",
    "    conv_32_16 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(partInput)\n",
    "\n",
    "    conv_64_32 = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(conv_32_16)\n",
    "    conv_64_32 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv_64_32)\n",
    "\n",
    "    conv_128_64 = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(conv_64_32)\n",
    "    conv_128_64 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv_128_64)\n",
    "\n",
    "    output_resize =layers.Resizing(height=100,width=50)(conv_128_64)\n",
    "    output_part = layers.Conv2D(filters= 1, kernel_size=(1,1), padding='same', activation='hard_sigmoid', name=\"x_out\")(output_resize)\n",
    "\n",
    "    return keras.Model(inputs= [partInput],outputs=[output_part])\n",
    "\n",
    "class AutoEncoder(keras.Model):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = buildEncoder()\n",
    "        self.decoder = buildDecoder()\n",
    "        \n",
    "    def call(self,data):\n",
    "        x = self.encoder(data)\n",
    "        y = self.decoder(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetUpOptimizer(variant):\n",
    "    \"\"\"\n",
    "    Builds a keras optmizer based of default parameters\n",
    "    \n",
    "    Accepts:\n",
    "        1:adam\n",
    "        2:adadelta\n",
    "        3:adafactor\n",
    "        4:adagrad\n",
    "        5:adamax\n",
    "        6:ftrl\n",
    "        7:nadam\n",
    "        8:rmsprop\n",
    "    \"\"\"\n",
    "    if(variant == 1 or variant == 'adam'):\n",
    "        print(\"Optimizer: Adam\")\n",
    "        return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam') \n",
    "    elif(variant == 2 or variant == 'adadelta'):\n",
    "        print(\"Optimizer: AdaDelta\")\n",
    "        return keras.optimizers.experimental.Adadelta(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        rho=0.95,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adadelta'\n",
    "                                                    )\n",
    "    elif(variant == 3 or variant == 'adafactor'):\n",
    "        print(\"Optimizer: AdaFactor\")\n",
    "        return keras.optimizers.experimental.Adafactor(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        beta_2_decay=-0.8,\n",
    "                                                        epsilon_1=1e-30,\n",
    "                                                        epsilon_2=0.001,\n",
    "                                                        clip_threshold=1.0,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adafactor'\n",
    "                                                    )\n",
    "    elif(variant == 4 or variant == 'adagrad'):\n",
    "        print(\"Optimizer: AdaGrad\")\n",
    "        return keras.optimizers.experimental.Adagrad(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        initial_accumulator_value=0.1,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adagrad'\n",
    "                                                    )\n",
    "    elif(variant == 5 or variant == 'adamax'):\n",
    "        print(\"Optimizer: AdaMax\")\n",
    "        return keras.optimizers.experimental.Adamax(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        beta_1=0.9,\n",
    "                                                        beta_2=0.999,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        name='Adamax'\n",
    "                                                    )\n",
    "    elif(variant == 6 or variant == 'ftrl'):\n",
    "        print(\"Optimizer: FTRL\")\n",
    "        return keras.optimizers.experimental.Ftrl(\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    learning_rate_power=-0.5,\n",
    "                                                    initial_accumulator_value=0.1,\n",
    "                                                    l1_regularization_strength=0.0,\n",
    "                                                    l2_regularization_strength=0.0,\n",
    "                                                    l2_shrinkage_regularization_strength=0.0,\n",
    "                                                    beta=0.0,\n",
    "                                                    ema_momentum=0.99,\n",
    "                                                    name='Ftrl'\n",
    "                                                )\n",
    "    elif(variant == 7 or variant == 'nadam'):\n",
    "        print(\"Optimizer: Nadam\")\n",
    "        return keras.optimizers.experimental.Nadam(\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    beta_1=0.9,\n",
    "                                                    beta_2=0.999,\n",
    "                                                    epsilon=1e-07,\n",
    "                                                    ema_momentum=0.99,\n",
    "                                                    name='Nadam'\n",
    "                                                )\n",
    "    elif(variant == 8 or variant == 'rmsprop'):\n",
    "        print(\"Optimizer: RMSprop\")\n",
    "        return keras.optimizers.experimental.RMSprop(\n",
    "                                                        learning_rate=0.001,\n",
    "                                                        rho=0.9,\n",
    "                                                        momentum=0.0,\n",
    "                                                        epsilon=1e-07,\n",
    "                                                        ema_momentum=0.99,\n",
    "                                                        ema_overwrite_frequency=100,\n",
    "                                                        name='RMSprop'\n",
    "                                                    )\n",
    "    else:\n",
    "        print(\"Optimizer: Adam\")\n",
    "        return keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setUp modelSaving\n",
    "\n",
    "def getModel(modelNumber,optimizerVarient:int = 1,LoadWeights:bool=True):\n",
    "\n",
    "    if(modelNumber == 9):\n",
    "        model = AutoEncoder()\n",
    "        fileSaveName = \"Model_m9\"\n",
    "    else:\n",
    "        raise Exception(\"No model identified, model {} DNE.\".format(modelNumber))\n",
    "    \n",
    "\n",
    "    modelPath = os.path.join(os.getcwd(),'ModelSave',fileSaveName)\n",
    "    \n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(filepath=os.path.join(modelPath,fileSaveName),\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    if(os.path.isdir(modelPath)):\n",
    "        if(LoadWeights):\n",
    "            try:\n",
    "                \n",
    "                model.load_weights(os.path.join(modelPath,fileSaveName))\n",
    "            except:\n",
    "                print(\"Model weights could not be loaded.\")\n",
    "            else:\n",
    "                print(\"Model weights Loaded\")\n",
    "        else:\n",
    "            print(\"Model weights were not loaded.\")\n",
    "    else:\n",
    "        os.mkdir(modelPath)\n",
    "        print(\"Model path created\")\n",
    "\n",
    "\n",
    "    model.compile(  optimizer=SetUpOptimizer(optimizerVarient),\n",
    "                    loss= keras.losses.BinaryCrossentropy()\n",
    "                    )\n",
    "    return model,cp_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentModelNumber = 9 #change this one\n",
    "model,callBack = getModel(currentModelNumber)#,LoadWeights=False)\n",
    "print()\n",
    "model.encoder.summary()\n",
    "model.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(hist):\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_title(\"\")\n",
    "    keys = hist.history.keys()\n",
    "    y = np.arange(1,hist.params['epochs']+1)\n",
    "    for key in keys:\n",
    "        if('output' in key):\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            if(minVal != maxVal):\n",
    "                print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "                #ax.plot(y,hist.history[key],linewidth=0.5,label=key)\n",
    "        else:\n",
    "            ax.plot(y,hist.history[key],label=key)\n",
    "            minVal = min(hist.history[key])\n",
    "            meanVal = np.mean(hist.history[key])\n",
    "            maxVal = max(hist.history[key])\n",
    "            print(\"{}:\\n\\tmin:{}\\n\\tmean:{}\\n\\tmax:{}\".format(key,minVal,meanVal,maxVal))\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 3000*6*6 \n",
    "numEpochs = 50\n",
    "BatchSize = 32 # default tensorflow batchsize\n",
    "numBatches = samples // BatchSize\n",
    "BatchesPerEpoch = numBatches// numEpochs\n",
    "print(\"Pretraining model over {} epochs.\\n\\tnumSamples: {}\\n\\tnumBatches: {}\\n\\tBatches per Epoch:{}\\n\".format(numEpochs,samples,numBatches,BatchesPerEpoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AutoEncoder_TrainingSet(data,extraIterations):\n",
    "    x_array = []\n",
    "\n",
    "    def addNoise(image):\n",
    "        noise = np.random.normal(loc=0.5,scale=.05,size=xValtoAdd.shape)\n",
    "        return np.clip(image + noise,0,1)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x = data[i].dispenceIterationsOnly()\n",
    "        #add the last completed part\n",
    "        xValtoAdd = x[-1]\n",
    "        x_array.append(xValtoAdd)\n",
    "\n",
    "        #add some noise to the image to increase the training set size\n",
    "        x_array.append(addNoise(xValtoAdd))\n",
    "\n",
    "        #flip the imagesSome to add more data\n",
    "        x_array.append(np.fliplr(xValtoAdd))#flip left right\n",
    "        x_array.append(np.fliplr(addNoise(xValtoAdd)))\n",
    "        x_array.append(np.flipud(xValtoAdd))#flip up Down\n",
    "        x_array.append(np.flipud(addNoise(xValtoAdd)))\n",
    "\n",
    "\n",
    "        #add optional extra parts\n",
    "        choice = np.random.choice(data[i].numIterations-1,size=extraIterations,replace=False).astype('int32')\n",
    "        for j in range(extraIterations):\n",
    "            xValtoAdd = x[choice[j]]\n",
    "            x_array.append(xValtoAdd)\n",
    "\n",
    "            #add some noise to the image to increase the training set size\n",
    "            x_array.append(addNoise(xValtoAdd))\n",
    "\n",
    "            #flip the imagesSome to add more data\n",
    "            x_array.append(np.fliplr(xValtoAdd))\n",
    "            x_array.append(np.fliplr(addNoise(xValtoAdd)))\n",
    "            x_array.append(np.flipud(xValtoAdd))\n",
    "            x_array.append(np.flipud(addNoise(xValtoAdd)))\n",
    "\n",
    "\n",
    "    x_array = np.array(x_array)\n",
    "\n",
    "    return x_array\n",
    "\n",
    "\n",
    "\n",
    "def AutoEncoderTraining():\n",
    "\n",
    "    x_array = create_AutoEncoder_TrainingSet(Data_train,5)\n",
    "\n",
    "    numEpochs = 50\n",
    "    BatchSize = 32 # default tensorflow batchsize\n",
    "    numBatches = len(x_array) // BatchSize\n",
    "    BatchesPerEpoch = numBatches// numEpochs\n",
    "    print(\"Training AutoEncoder over {} epochs.\\n\\tnumSamples: {}\\n\\tnumBatches: {}\\n\\tBatches per Epoch:{}\\n\".format(numEpochs,len(x_array),numBatches,BatchesPerEpoch))\n",
    "    \n",
    "    history1 = model.fit(\n",
    "        x={'x':x_array},\n",
    "        y=(x_array),\n",
    "        batch_size=BatchSize,\n",
    "        validation_split = 0.1,\n",
    "        epochs=numEpochs,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch = BatchesPerEpoch,\n",
    "        callbacks = [callBack])\n",
    "    \n",
    "    return history1\n",
    "    \n",
    "    \n",
    "h2 = AutoEncoderTraining()\n",
    "plotHistory(h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawModelEncodings(num):\n",
    "    sequenceToStart = Data_test[num]\n",
    "    #print(numImages)\n",
    "    fig,ax = plt.subplots(3)\n",
    "\n",
    "\n",
    "    nelx = 100\n",
    "    nely = 50\n",
    "\n",
    "    \n",
    "    ImageToPredict = np.array([sequenceToStart.xPhys_array[:,:,-1]])\n",
    "    PredictedImages = [ImageToPredict]\n",
    "\n",
    "    start = time()\n",
    "    \n",
    "    encodedImage = model.encoder.predict({'x':ImageToPredict},verbose = 0)\n",
    "    DecodedImage = model.decoder.predict({'x':encodedImage},verbose = 0)\n",
    "\n",
    "    end = time()\n",
    "    print(\"Encoding took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "\n",
    "    \n",
    "    ax[0].imshow(np.reshape(ImageToPredict,(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[1].imshow(np.reshape(encodedImage,(32,16)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[1].get_xaxis().set_visible(False)\n",
    "    ax[1].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[2].imshow(np.reshape(DecodedImage,(nelx,nely)).T,cmap='gray_r',norm=colors.Normalize(vmin=0,vmax=1))\n",
    "    ax[2].get_xaxis().set_visible(False)\n",
    "    ax[2].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "    ax[0].set_title(\"Start\")\n",
    "    ax[1].set_title(\"Encoded\")\n",
    "    ax[2].set_title(\"Decoded\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = np.random.randint(0,len(Data_test)-1)\n",
    "#DrawModelEncodings(n)\n",
    "\n",
    "\n",
    "n = np.arange(len(Data_test),dtype='int32')\n",
    "np.random.shuffle(n)\n",
    "print(len(n))\n",
    "for i in range(5):\n",
    "    DrawModelEncodings(n[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
